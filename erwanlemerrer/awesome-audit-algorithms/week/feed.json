{
  "version": "https://jsonfeed.org/version/1",
  "icon": "https://www.trackawesomelist.com/icon.png",
  "favicon": "https://www.trackawesomelist.com/favicon.ico",
  "language": "en",
  "title": "Track Awesome Audit Algorithms Updates Weekly",
  "_seo_title": "Track Awesome Audit Algorithms (erwanlemerrer/awesome-audit-algorithms) Updates Weekly - Track Awesome List",
  "_site_title": "Track Awesome List",
  "description": "A curated list of algorithms and papers for auditing black-box algorithms.",
  "home_page_url": "https://www.trackawesomelist.com/erwanlemerrer/awesome-audit-algorithms/week/",
  "feed_url": "https://www.trackawesomelist.com/erwanlemerrer/awesome-audit-algorithms/week/feed.json",
  "items": [
    {
      "id": "https://www.trackawesomelist.com/2025/25/",
      "title": "Awesome Audit Algorithms Updates on Jun 23 - Jun 29, 2025",
      "_short_title": "Jun 23 - Jun 29, 2025",
      "_slug": "2025/25/",
      "summary": "2 awesome projects updated on Jun 23 - Jun 29, 2025",
      "_filepath": "/content/2025/25/README.md",
      "url": "https://www.trackawesomelist.com/2025/25/",
      "date_published": "2025-06-19T07:00:51.000Z",
      "date_modified": "2025-06-19T07:00:51.000Z",
      "content_text": "\n\n### Papers / 2025\n\n*   [P2NIA: Privacy-Preserving Non-Iterative Auditing](https://arxiv.org/abs/2504.00874) - (ECAI) *Proposes a mutually beneficial collaboration for both the auditor and the platform: a privacy-preserving and non-iterative audit scheme that enhances fairness assessments using synthetic or local data, avoiding the challenges associated with traditional API-based audits.*\n*   [The Fair Game: Auditing & debiasing AI algorithms overtime](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/9E8408C67F7CE30505122DD1586D9FA2/S3033373325000080a.pdf/the-fair-game-auditing-and-debiasing-ai-algorithms-over-time.pdf) - (Cambridge Forum on AI: Law and Governance) *Aims to simulate the evolution of ethical and legal frameworks in the society by creating an auditor which sends feedback to a debiasing algorithm deployed around an ML system.*",
      "content_html": "<h3><p>Papers / 2025</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2504.00874\" rel=\"noopener noreferrer\">P2NIA: Privacy-Preserving Non-Iterative Auditing</a> - (ECAI) <em>Proposes a mutually beneficial collaboration for both the auditor and the platform: a privacy-preserving and non-iterative audit scheme that enhances fairness assessments using synthetic or local data, avoiding the challenges associated with traditional API-based audits.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.cambridge.org/core/services/aop-cambridge-core/content/view/9E8408C67F7CE30505122DD1586D9FA2/S3033373325000080a.pdf/the-fair-game-auditing-and-debiasing-ai-algorithms-over-time.pdf\" rel=\"noopener noreferrer\">The Fair Game: Auditing &amp; debiasing AI algorithms overtime</a> - (Cambridge Forum on AI: Law and Governance) <em>Aims to simulate the evolution of ethical and legal frameworks in the society by creating an auditor which sends feedback to a debiasing algorithm deployed around an ML system.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2025/20/",
      "title": "Awesome Audit Algorithms Updates on May 19 - May 25, 2025",
      "_short_title": "May 19 - May 25, 2025",
      "_slug": "2025/20/",
      "summary": "3 awesome projects updated on May 19 - May 25, 2025",
      "_filepath": "/content/2025/20/README.md",
      "url": "https://www.trackawesomelist.com/2025/20/",
      "date_published": "2025-05-13T07:44:41.000Z",
      "date_modified": "2025-05-16T09:12:48.000Z",
      "content_text": "\n\n### Papers / 2025\n\n*   [Robust ML Auditing using Prior Knowledge](https://arxiv.org/pdf/2505.04796) - (ICML) *Formally establishes the conditions under which an auditor can prevent audit manipulations using prior knowledge about the ground truth.*\n\n### Papers / 2024\n\n*   [Hardware and software platform inference](https://arxiv.org/pdf/2411.05197) - (arXiv) *A method for identifying the underlying GPU architecture and software stack of a black-box machine learning model solely based on its input-output behavior.*\n\n### Related Events / 2025\n\n*   [AIMLAI at ECML/PKDD 2025](https://project.inria.fr/aimlai/)",
      "content_html": "<h3><p>Papers / 2025</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2505.04796\" rel=\"noopener noreferrer\">Robust ML Auditing using Prior Knowledge</a> - (ICML) <em>Formally establishes the conditions under which an auditor can prevent audit manipulations using prior knowledge about the ground truth.</em></li>\n</ul>\n<h3><p>Papers / 2024</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2411.05197\" rel=\"noopener noreferrer\">Hardware and software platform inference</a> - (arXiv) <em>A method for identifying the underlying GPU architecture and software stack of a black-box machine learning model solely based on its input-output behavior.</em></li>\n</ul>\n<h3><p>Related Events / 2025</p>\n</h3>\n<ul>\n<li><a href=\"https://project.inria.fr/aimlai/\" rel=\"noopener noreferrer\">AIMLAI at ECML/PKDD 2025</a></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2025/4/",
      "title": "Awesome Audit Algorithms Updates on Jan 27 - Feb 02, 2025",
      "_short_title": "Jan 27 - Feb 02, 2025",
      "_slug": "2025/4/",
      "summary": "3 awesome projects updated on Jan 27 - Feb 02, 2025",
      "_filepath": "/content/2025/4/README.md",
      "url": "https://www.trackawesomelist.com/2025/4/",
      "date_published": "2025-01-22T08:22:35.000Z",
      "date_modified": "2025-01-22T08:26:55.000Z",
      "content_text": "\n\n### Papers / 2025\n\n*   [CALM: Curiosity-Driven Auditing for Large Language Models](https://arxiv.org/abs/2501.02997) - (AAAI) *Auditing as a black-box optimization problem where the goal is to automatically uncover input-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe behaviors.*\n*   [Queries, Representation & Detection: The Next 100 Model Fingerprinting Schemes](https://arxiv.org/abs/2412.13021) - (AAAI) *Divides model fingerprinting into three core components, to identify ∼100 previously unexplored combinations of these and gain insights into their performance.*\n\n### Related Events / 2025\n\n*   [AAAI workshop on AI Governance: Alignment, Morality, and Law](https://aaai.org/conference/aaai/aaai-25/workshop-list/#ws06)",
      "content_html": "<h3><p>Papers / 2025</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2501.02997\" rel=\"noopener noreferrer\">CALM: Curiosity-Driven Auditing for Large Language Models</a> - (AAAI) <em>Auditing as a black-box optimization problem where the goal is to automatically uncover input-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe behaviors.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/2412.13021\" rel=\"noopener noreferrer\">Queries, Representation &amp; Detection: The Next 100 Model Fingerprinting Schemes</a> - (AAAI) <em>Divides model fingerprinting into three core components, to identify ∼100 previously unexplored combinations of these and gain insights into their performance.</em></li>\n</ul>\n<h3><p>Related Events / 2025</p>\n</h3>\n<ul>\n<li><a href=\"https://aaai.org/conference/aaai/aaai-25/workshop-list/#ws06\" rel=\"noopener noreferrer\">AAAI workshop on AI Governance: Alignment, Morality, and Law</a></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2024/43/",
      "title": "Awesome Audit Algorithms Updates on Oct 21 - Oct 27, 2024",
      "_short_title": "Oct 21 - Oct 27, 2024",
      "_slug": "2024/43/",
      "summary": "1 awesome projects updated on Oct 21 - Oct 27, 2024",
      "_filepath": "/content/2024/43/README.md",
      "url": "https://www.trackawesomelist.com/2024/43/",
      "date_published": "2024-10-22T07:28:50.000Z",
      "date_modified": "2024-10-22T07:28:50.000Z",
      "content_text": "\n\n### Papers / 2024\n\n*   [Auditing Local Explanations is Hard](https://arxiv.org/abs/2407.13281) - (NeurIPS) *Gives the (prohibitive) query complexity of auditing explanations.*",
      "content_html": "<h3><p>Papers / 2024</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2407.13281\" rel=\"noopener noreferrer\">Auditing Local Explanations is Hard</a> - (NeurIPS) <em>Gives the (prohibitive) query complexity of auditing explanations.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2024/40/",
      "title": "Awesome Audit Algorithms Updates on Sep 30 - Oct 06, 2024",
      "_short_title": "Sep 30 - Oct 06, 2024",
      "_slug": "2024/40/",
      "summary": "2 awesome projects updated on Sep 30 - Oct 06, 2024",
      "_filepath": "/content/2024/40/README.md",
      "url": "https://www.trackawesomelist.com/2024/40/",
      "date_published": "2024-10-02T09:13:59.000Z",
      "date_modified": "2024-10-04T10:42:36.000Z",
      "content_text": "\n\n### Papers / 2024\n\n*   [LLMs hallucinate graphs too: a structural perspective](https://arxiv.org/abs/2409.00159) - (complex networks) *Queries LLMs for known graphs and studies topological hallucinations. Proposes a structural hallucination rank.*\n\n### Papers / 2023\n\n*   [Auditing fairness under unawareness through counterfactual reasoning](https://www.sciencedirect.com/science/article/pii/S0306457322003259) - (Information Processing & Management) *Shows how to unveil whether a black-box model, complying with the regulations, is still biased or not.*",
      "content_html": "<h3><p>Papers / 2024</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2409.00159\" rel=\"noopener noreferrer\">LLMs hallucinate graphs too: a structural perspective</a> - (complex networks) <em>Queries LLMs for known graphs and studies topological hallucinations. Proposes a structural hallucination rank.</em></li>\n</ul>\n<h3><p>Papers / 2023</p>\n</h3>\n<ul>\n<li><a href=\"https://www.sciencedirect.com/science/article/pii/S0306457322003259\" rel=\"noopener noreferrer\">Auditing fairness under unawareness through counterfactual reasoning</a> - (Information Processing &amp; Management) <em>Shows how to unveil whether a black-box model, complying with the regulations, is still biased or not.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2024/38/",
      "title": "Awesome Audit Algorithms Updates on Sep 16 - Sep 22, 2024",
      "_short_title": "Sep 16 - Sep 22, 2024",
      "_slug": "2024/38/",
      "summary": "3 awesome projects updated on Sep 16 - Sep 22, 2024",
      "_filepath": "/content/2024/38/README.md",
      "url": "https://www.trackawesomelist.com/2024/38/",
      "date_published": "2024-09-19T07:44:59.000Z",
      "date_modified": "2024-09-19T07:44:59.000Z",
      "content_text": "\n\n### Related Events / 2024\n\n*   [1st International Conference on Auditing and Artificial Intelligence](https://www.ircg.msm.uni-due.de/ai/)\n*   [Regulatable ML Workshop (RegML'24)](https://regulatableml.github.io/)\n\n### Related Events / 2023\n\n*   [Supporting User Engagement in Testing, Auditing, and Contesting AI (CSCW User AI Auditing)](https://cscw-user-ai-auditing.github.io/)",
      "content_html": "<h3><p>Related Events / 2024</p>\n</h3>\n<ul>\n<li><a href=\"https://www.ircg.msm.uni-due.de/ai/\" rel=\"noopener noreferrer\">1st International Conference on Auditing and Artificial Intelligence</a></li>\n</ul>\n\n<ul>\n<li><a href=\"https://regulatableml.github.io/\" rel=\"noopener noreferrer\">Regulatable ML Workshop (RegML'24)</a></li>\n</ul>\n<h3><p>Related Events / 2023</p>\n</h3>\n<ul>\n<li><a href=\"https://cscw-user-ai-auditing.github.io/\" rel=\"noopener noreferrer\">Supporting User Engagement in Testing, Auditing, and Contesting AI (CSCW User AI Auditing)</a></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2024/35/",
      "title": "Awesome Audit Algorithms Updates on Aug 26 - Sep 01, 2024",
      "_short_title": "Aug 26 - Sep 01, 2024",
      "_slug": "2024/35/",
      "summary": "1 awesome projects updated on Aug 26 - Sep 01, 2024",
      "_filepath": "/content/2024/35/README.md",
      "url": "https://www.trackawesomelist.com/2024/35/",
      "date_published": "2024-08-26T14:47:48.000Z",
      "date_modified": "2024-08-26T14:47:48.000Z",
      "content_text": "\n\n### Papers / 2023\n\n*   [Privacy Auditing with One (1) Training Run](https://neurips.cc/virtual/2023/poster/70925) - (NeurIPS - best paper) *A scheme for auditing differentially private machine learning systems with a single training run.*",
      "content_html": "<h3><p>Papers / 2023</p>\n</h3>\n<ul>\n<li><a href=\"https://neurips.cc/virtual/2023/poster/70925\" rel=\"noopener noreferrer\">Privacy Auditing with One (1) Training Run</a> - (NeurIPS - best paper) <em>A scheme for auditing differentially private machine learning systems with a single training run.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2024/29/",
      "title": "Awesome Audit Algorithms Updates on Jul 15 - Jul 21, 2024",
      "_short_title": "Jul 15 - Jul 21, 2024",
      "_slug": "2024/29/",
      "summary": "4 awesome projects updated on Jul 15 - Jul 21, 2024",
      "_filepath": "/content/2024/29/README.md",
      "url": "https://www.trackawesomelist.com/2024/29/",
      "date_published": "2024-07-17T07:17:14.000Z",
      "date_modified": "2024-07-17T07:19:49.000Z",
      "content_text": "\n\n### Papers / 2024\n\n*   [Fairness Auditing with Multi-Agent Collaboration](https://arxiv.org/pdf/2402.08522) - (ECAI) *Considers multiple\n    agents working together, each auditing the same platform for different tasks.*\n*   [Mapping the Field of Algorithm Auditing: A Systematic Literature Review\n    Identifying Research Trends, Linguistic and Geographical Disparities](https://arxiv.org/pdf/2401.11194) - (Arxiv) *Systematic review of algorithm\n    auditing studies and identification of trends in their methodological approaches.*\n*   [FairProof: Confidential and Certifiable Fairness for Neural Networks](https://arxiv.org/pdf/2402.12572v1.pdf) - (Arxiv) *Proposes an alternative paradigm to traditional auditing using crytographic tools like Zero-Knowledge Proofs; gives a system called FairProof for verifying fairness of small neural networks.*\n\n### Papers / 2023\n\n*   [Keeping Up with the Language Models: Robustness-Bias Interplay in NLI Data and Models](https://arxiv.org/pdf/2305.12620.pdf) - (Arxiv) *Proposes a way to extend the shelf-life of auditing datasets by using language models themselves; also finds problems with the current bias auditing metrics and proposes alternatives -- these alternatives highlight that model brittleness superficially increased the previous bias scores.*",
      "content_html": "<h3><p>Papers / 2024</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2402.08522\" rel=\"noopener noreferrer\">Fairness Auditing with Multi-Agent Collaboration</a> - (ECAI) <em>Considers multiple\nagents working together, each auditing the same platform for different tasks.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2401.11194\" rel=\"noopener noreferrer\">Mapping the Field of Algorithm Auditing: A Systematic Literature Review\nIdentifying Research Trends, Linguistic and Geographical Disparities</a> - (Arxiv) <em>Systematic review of algorithm\nauditing studies and identification of trends in their methodological approaches.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2402.12572v1.pdf\" rel=\"noopener noreferrer\">FairProof: Confidential and Certifiable Fairness for Neural Networks</a> - (Arxiv) <em>Proposes an alternative paradigm to traditional auditing using crytographic tools like Zero-Knowledge Proofs; gives a system called FairProof for verifying fairness of small neural networks.</em></li>\n</ul>\n<h3><p>Papers / 2023</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2305.12620.pdf\" rel=\"noopener noreferrer\">Keeping Up with the Language Models: Robustness-Bias Interplay in NLI Data and Models</a> - (Arxiv) <em>Proposes a way to extend the shelf-life of auditing datasets by using language models themselves; also finds problems with the current bias auditing metrics and proposes alternatives -- these alternatives highlight that model brittleness superficially increased the previous bias scores.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2024/11/",
      "title": "Awesome Audit Algorithms Updates on Mar 11 - Mar 17, 2024",
      "_short_title": "Mar 11 - Mar 17, 2024",
      "_slug": "2024/11/",
      "summary": "1 awesome projects updated on Mar 11 - Mar 17, 2024",
      "_filepath": "/content/2024/11/README.md",
      "url": "https://www.trackawesomelist.com/2024/11/",
      "date_published": "2024-03-11T16:30:39.000Z",
      "date_modified": "2024-03-11T16:30:39.000Z",
      "content_text": "\n\n### Papers / 2023\n\n*   [XAudit : A Theoretical Look at Auditing with Explanations](https://arxiv.org/pdf/2206.04740.pdf) - (Arxiv) *Formalizes the role of explanations in auditing and investigates if and how model explanations\n    can help audits.*",
      "content_html": "<h3><p>Papers / 2023</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2206.04740.pdf\" rel=\"noopener noreferrer\">XAudit : A Theoretical Look at Auditing with Explanations</a> - (Arxiv) <em>Formalizes the role of explanations in auditing and investigates if and how model explanations\ncan help audits.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2024/9/",
      "title": "Awesome Audit Algorithms Updates on Feb 26 - Mar 03, 2024",
      "_short_title": "Feb 26 - Mar 03, 2024",
      "_slug": "2024/9/",
      "summary": "1 awesome projects updated on Feb 26 - Mar 03, 2024",
      "_filepath": "/content/2024/9/README.md",
      "url": "https://www.trackawesomelist.com/2024/9/",
      "date_published": "2024-02-28T13:48:41.000Z",
      "date_modified": "2024-02-28T13:48:41.000Z",
      "content_text": "\n\n### Related Events / 2023\n\n*   [Regulatable ML Workshop (RegML'23)](https://regulatableml.github.io/)",
      "content_html": "<h3><p>Related Events / 2023</p>\n</h3>\n<ul>\n<li><a href=\"https://regulatableml.github.io/\" rel=\"noopener noreferrer\">Regulatable ML Workshop (RegML'23)</a></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2024/7/",
      "title": "Awesome Audit Algorithms Updates on Feb 12 - Feb 18, 2024",
      "_short_title": "Feb 12 - Feb 18, 2024",
      "_slug": "2024/7/",
      "summary": "3 awesome projects updated on Feb 12 - Feb 18, 2024",
      "_filepath": "/content/2024/7/README.md",
      "url": "https://www.trackawesomelist.com/2024/7/",
      "date_published": "2024-02-14T08:31:38.000Z",
      "date_modified": "2024-02-16T08:54:28.000Z",
      "content_text": "\n\n### Papers / 2024\n\n*   [Under manipulations, are some AI models harder to audit?](https://grodino.github.io/projects/manipulated-audits/preprint.pdf) - (SATML) *Relates the difficulty of black-box audits\n    to the capacity of the targeted models, using the Rademacher complexity.*\n*   [Improved Membership Inference Attacks Against Language Classification Models](https://arxiv.org/pdf/2310.07219.pdf) - (ICLR) *Presents a framework for running membership inference attacks against classifier, in audit mode.*\n\n### Papers / 2023\n\n*   [Online Fairness Auditing through Iterative Refinement](https://dl.acm.org/doi/pdf/10.1145/3580305.3599454) - (KDD) *Provides an adaptive process that automates the inference of probabilistic guarantees associated with estimating fairness metrics.*",
      "content_html": "<h3><p>Papers / 2024</p>\n</h3>\n<ul>\n<li><a href=\"https://grodino.github.io/projects/manipulated-audits/preprint.pdf\" rel=\"noopener noreferrer\">Under manipulations, are some AI models harder to audit?</a> - (SATML) <em>Relates the difficulty of black-box audits\nto the capacity of the targeted models, using the Rademacher complexity.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2310.07219.pdf\" rel=\"noopener noreferrer\">Improved Membership Inference Attacks Against Language Classification Models</a> - (ICLR) <em>Presents a framework for running membership inference attacks against classifier, in audit mode.</em></li>\n</ul>\n<h3><p>Papers / 2023</p>\n</h3>\n<ul>\n<li><a href=\"https://dl.acm.org/doi/pdf/10.1145/3580305.3599454\" rel=\"noopener noreferrer\">Online Fairness Auditing through Iterative Refinement</a> - (KDD) <em>Provides an adaptive process that automates the inference of probabilistic guarantees associated with estimating fairness metrics.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2024/4/",
      "title": "Awesome Audit Algorithms Updates on Jan 22 - Jan 28, 2024",
      "_short_title": "Jan 22 - Jan 28, 2024",
      "_slug": "2024/4/",
      "summary": "1 awesome projects updated on Jan 22 - Jan 28, 2024",
      "_filepath": "/content/2024/4/README.md",
      "url": "https://www.trackawesomelist.com/2024/4/",
      "date_published": "2024-01-23T13:54:20.000Z",
      "date_modified": "2024-01-23T13:54:20.000Z",
      "content_text": "\n\n### Papers / 2024\n\n*   [Auditing Fairness by Betting](https://arxiv.org/pdf/2305.17570.pdf) - (Neurips) [\\[Code\\] (⭐5)](https://github.com/bchugg/auditing-fairness) *Sequential methods that allows for the continuous monitoring of incoming data from a black-box classifier or regressor.*",
      "content_html": "<h3><p>Papers / 2024</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2305.17570.pdf\" rel=\"noopener noreferrer\">Auditing Fairness by Betting</a> - (Neurips) <a href=\"https://github.com/bchugg/auditing-fairness\" rel=\"noopener noreferrer\">[Code] (⭐5)</a> <em>Sequential methods that allows for the continuous monitoring of incoming data from a black-box classifier or regressor.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2024/2/",
      "title": "Awesome Audit Algorithms Updates on Jan 08 - Jan 14, 2024",
      "_short_title": "Jan 08 - Jan 14, 2024",
      "_slug": "2024/2/",
      "summary": "1 awesome projects updated on Jan 08 - Jan 14, 2024",
      "_filepath": "/content/2024/2/README.md",
      "url": "https://www.trackawesomelist.com/2024/2/",
      "date_published": "2024-01-09T15:06:28.000Z",
      "date_modified": "2024-01-09T15:06:28.000Z",
      "content_text": "\n\n### Papers / 2023\n\n*   [Stealing the Decoding Algorithms of Language Models](https://people.cs.umass.edu/~amir/papers/CCS23-LM-stealing.pdf) - (CCS) *Steal the type and hyperparameters of the decoding algorithms of a LLM.*",
      "content_html": "<h3><p>Papers / 2023</p>\n</h3>\n<ul>\n<li><a href=\"https://people.cs.umass.edu/~amir/papers/CCS23-LM-stealing.pdf\" rel=\"noopener noreferrer\">Stealing the Decoding Algorithms of Language Models</a> - (CCS) <em>Steal the type and hyperparameters of the decoding algorithms of a LLM.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2023/37/",
      "title": "Awesome Audit Algorithms Updates on Sep 11 - Sep 17, 2023",
      "_short_title": "Sep 11 - Sep 17, 2023",
      "_slug": "2023/37/",
      "summary": "3 awesome projects updated on Sep 11 - Sep 17, 2023",
      "_filepath": "/content/2023/37/README.md",
      "url": "https://www.trackawesomelist.com/2023/37/",
      "date_published": "2023-09-15T08:07:43.000Z",
      "date_modified": "2023-09-15T08:07:43.000Z",
      "content_text": "\n\n### Papers / 2023\n\n*   [Modeling rabbit‑holes on YouTube](https://link.springer.com/epdf/10.1007/s13278-023-01105-9?sharing_token=h-O-asHI49VUWS9FxN1Gsve4RwlQNchNByi7wbcMAY6I98PKW1PqhFQJ_JqQyk3TrB05qDb3LUzMDmKOgrupccQliViDle-rwKEi2MZ8xBViaAQhyN41oZBKLLeXchoeIW2kklVHC094I5KD8pxja4-if6-iB0uAI1FnqnYoxjU%3D) - (SNAM) *Models the trapping dynamics of users in rabbit holes in YouTube, and provides a measure of this enclosure.*\n*   [Auditing YouTube’s Recommendation Algorithm for Misinformation Filter Bubbles](https://dl.acm.org/doi/full/10.1145/3568392) - (Transactions on Recommender Systems) *What it takes to “burst the bubble,” i.e., revert the bubble enclosure from recommendations.*\n\n### Papers / 2020\n\n*   [Auditing radicalization pathways on ](https://dl.acm.org/doi/pdf/10.1145/3351095.3372879) - (FAT\\*) *Studies the reachability of radical channels from each others, using random walks on static channel recommendations.*",
      "content_html": "<h3><p>Papers / 2023</p>\n</h3>\n<ul>\n<li><a href=\"https://link.springer.com/epdf/10.1007/s13278-023-01105-9?sharing_token=h-O-asHI49VUWS9FxN1Gsve4RwlQNchNByi7wbcMAY6I98PKW1PqhFQJ_JqQyk3TrB05qDb3LUzMDmKOgrupccQliViDle-rwKEi2MZ8xBViaAQhyN41oZBKLLeXchoeIW2kklVHC094I5KD8pxja4-if6-iB0uAI1FnqnYoxjU%3D\" rel=\"noopener noreferrer\">Modeling rabbit‑holes on YouTube</a> - (SNAM) <em>Models the trapping dynamics of users in rabbit holes in YouTube, and provides a measure of this enclosure.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://dl.acm.org/doi/full/10.1145/3568392\" rel=\"noopener noreferrer\">Auditing YouTube’s Recommendation Algorithm for Misinformation Filter Bubbles</a> - (Transactions on Recommender Systems) <em>What it takes to “burst the bubble,” i.e., revert the bubble enclosure from recommendations.</em></li>\n</ul>\n<h3><p>Papers / 2020</p>\n</h3>\n<ul>\n<li><a href=\"https://dl.acm.org/doi/pdf/10.1145/3351095.3372879\" rel=\"noopener noreferrer\">Auditing radicalization pathways on </a> - (FAT*) <em>Studies the reachability of radical channels from each others, using random walks on static channel recommendations.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2023/32/",
      "title": "Awesome Audit Algorithms Updates on Aug 07 - Aug 13, 2023",
      "_short_title": "Aug 07 - Aug 13, 2023",
      "_slug": "2023/32/",
      "summary": "2 awesome projects updated on Aug 07 - Aug 13, 2023",
      "_filepath": "/content/2023/32/README.md",
      "url": "https://www.trackawesomelist.com/2023/32/",
      "date_published": "2023-08-10T07:51:25.000Z",
      "date_modified": "2023-08-10T08:11:17.000Z",
      "content_text": "\n\n### Papers / 2023\n\n*   [Auditing Yelp’s Business Ranking and Review Recommendation Through the Lens of Fairness](https://arxiv.org/pdf/2308.02129.pdf) - (Arxiv) *Audits the fairness of Yelp’s business\n    ranking and review recommendation systems, with demographic parity, exposure, and statistical tests such as quantile linear and logistic regression.*\n\n### Papers / 2021\n\n*   [When the Umpire is also a Player: Bias in Private Label Product Recommendations on E-commerce Marketplaces](https://arxiv.org/pdf/2102.00141.pdf) - (FAccT) *Do Amazon private label products get an unfair share of recommendations and are therefore advantaged compared to 3rd party products?*",
      "content_html": "<h3><p>Papers / 2023</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2308.02129.pdf\" rel=\"noopener noreferrer\">Auditing Yelp’s Business Ranking and Review Recommendation Through the Lens of Fairness</a> - (Arxiv) <em>Audits the fairness of Yelp’s business\nranking and review recommendation systems, with demographic parity, exposure, and statistical tests such as quantile linear and logistic regression.</em></li>\n</ul>\n<h3><p>Papers / 2021</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2102.00141.pdf\" rel=\"noopener noreferrer\">When the Umpire is also a Player: Bias in Private Label Product Recommendations on E-commerce Marketplaces</a> - (FAccT) <em>Do Amazon private label products get an unfair share of recommendations and are therefore advantaged compared to 3rd party products?</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2023/21/",
      "title": "Awesome Audit Algorithms Updates on May 22 - May 28, 2023",
      "_short_title": "May 22 - May 28, 2023",
      "_slug": "2023/21/",
      "summary": "1 awesome projects updated on May 22 - May 28, 2023",
      "_filepath": "/content/2023/21/README.md",
      "url": "https://www.trackawesomelist.com/2023/21/",
      "date_published": "2023-05-22T15:31:08.000Z",
      "date_modified": "2023-05-22T15:31:08.000Z",
      "content_text": "\n\n### Papers / 2023\n\n*   [SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency](https://arxiv.org/pdf/2302.03251.pdf) - (ICLR) *Considers backdoor detection under the black-box setting in machine learning as a service (MLaaS) applications.*",
      "content_html": "<h3><p>Papers / 2023</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2302.03251.pdf\" rel=\"noopener noreferrer\">SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency</a> - (ICLR) <em>Considers backdoor detection under the black-box setting in machine learning as a service (MLaaS) applications.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2023/16/",
      "title": "Awesome Audit Algorithms Updates on Apr 17 - Apr 23, 2023",
      "_short_title": "Apr 17 - Apr 23, 2023",
      "_slug": "2023/16/",
      "summary": "1 awesome projects updated on Apr 17 - Apr 23, 2023",
      "_filepath": "/content/2023/16/README.md",
      "url": "https://www.trackawesomelist.com/2023/16/",
      "date_published": "2023-04-20T14:46:34.000Z",
      "date_modified": "2023-04-20T14:46:34.000Z",
      "content_text": "\n\n### Papers / 2023\n\n*   [Confidential-PROFITT: Confidential PROof of FaIr Training of Trees](https://openreview.net/pdf?id=iIfDQVyuFD) - (ICLR) *Proposes fair decision tree learning algorithms along with zero-knowledge proof protocols to obtain a proof of fairness on the audited server.*",
      "content_html": "<h3><p>Papers / 2023</p>\n</h3>\n<ul>\n<li><a href=\"https://openreview.net/pdf?id=iIfDQVyuFD\" rel=\"noopener noreferrer\">Confidential-PROFITT: Confidential PROof of FaIr Training of Trees</a> - (ICLR) <em>Proposes fair decision tree learning algorithms along with zero-knowledge proof protocols to obtain a proof of fairness on the audited server.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2023/11/",
      "title": "Awesome Audit Algorithms Updates on Mar 13 - Mar 19, 2023",
      "_short_title": "Mar 13 - Mar 19, 2023",
      "_slug": "2023/11/",
      "summary": "6 awesome projects updated on Mar 13 - Mar 19, 2023",
      "_filepath": "/content/2023/11/README.md",
      "url": "https://www.trackawesomelist.com/2023/11/",
      "date_published": "2023-03-14T17:20:40.000Z",
      "date_modified": "2023-03-14T17:20:40.000Z",
      "content_text": "\n\n### Papers / 2018\n\n*   [Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR](https://arxiv.org/abs/1711.00399) - (Harvard Journal of Law & Technology) *To explain a decision on x, find a conterfactual: the closest point to x that changes the decision.*\n\n### Papers / 2016\n\n*   [Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems](https://www.andrew.cmu.edu/user/danupam/datta-sen-zick-oakland16.pdf) - (IEEE S\\&P) *Evaluate the individual, joint and marginal influence of features on a model using shapley values.*\n*   [Auditing Black-Box Models for Indirect Influence](https://arxiv.org/abs/1602.07043) - (ICDM) *Evaluate the influence of a variable on a black-box model by \"cleverly\" removing it from the dataset and looking at the accuracy gap*\n*   [Iterative Orthogonal Feature Projection for Diagnosing Bias in Black-Box Models](https://arxiv.org/abs/1611.04967) - (FATML Workshop) *Performs feature ranking to analyse black-box models*\n\n### Papers / 2015\n\n*   [Certifying and Removing Disparate Impact](https://arxiv.org/abs/1412.3756) - (SIGKDD) *Proposes SVM-based methods to certify absence of bias and methods to remove biases from a dataset.*\n\n### Papers / 2014\n\n*   [A peek into the black box: exploring classifiers by randomization](https://github.com/erwanlemerrer/awesome-audit-algorithms/blob/main/README.md/) - (Data Mining and Knowledge Discovery journal) ([code (⭐2)](https://github.com/tsabsch/goldeneye)) *Finds groups of features that can be permuted without changing the output label of predicted samples*",
      "content_html": "<h3><p>Papers / 2018</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1711.00399\" rel=\"noopener noreferrer\">Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR</a> - (Harvard Journal of Law &amp; Technology) <em>To explain a decision on x, find a conterfactual: the closest point to x that changes the decision.</em></li>\n</ul>\n<h3><p>Papers / 2016</p>\n</h3>\n<ul>\n<li><a href=\"https://www.andrew.cmu.edu/user/danupam/datta-sen-zick-oakland16.pdf\" rel=\"noopener noreferrer\">Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems</a> - (IEEE S&amp;P) <em>Evaluate the individual, joint and marginal influence of features on a model using shapley values.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1602.07043\" rel=\"noopener noreferrer\">Auditing Black-Box Models for Indirect Influence</a> - (ICDM) <em>Evaluate the influence of a variable on a black-box model by \"cleverly\" removing it from the dataset and looking at the accuracy gap</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1611.04967\" rel=\"noopener noreferrer\">Iterative Orthogonal Feature Projection for Diagnosing Bias in Black-Box Models</a> - (FATML Workshop) <em>Performs feature ranking to analyse black-box models</em></li>\n</ul>\n<h3><p>Papers / 2015</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1412.3756\" rel=\"noopener noreferrer\">Certifying and Removing Disparate Impact</a> - (SIGKDD) <em>Proposes SVM-based methods to certify absence of bias and methods to remove biases from a dataset.</em></li>\n</ul>\n<h3><p>Papers / 2014</p>\n</h3>\n<ul>\n<li><a href=\"https://github.com/erwanlemerrer/awesome-audit-algorithms/blob/main/README.md/\" rel=\"noopener noreferrer\">A peek into the black box: exploring classifiers by randomization</a> - (Data Mining and Knowledge Discovery journal) (<a href=\"https://github.com/tsabsch/goldeneye\" rel=\"noopener noreferrer\">code (⭐2)</a>) <em>Finds groups of features that can be permuted without changing the output label of predicted samples</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2023/9/",
      "title": "Awesome Audit Algorithms Updates on Feb 27 - Mar 05, 2023",
      "_short_title": "Feb 27 - Mar 05, 2023",
      "_slug": "2023/9/",
      "summary": "1 awesome projects updated on Feb 27 - Mar 05, 2023",
      "_filepath": "/content/2023/9/README.md",
      "url": "https://www.trackawesomelist.com/2023/9/",
      "date_published": "2023-03-03T10:04:47.000Z",
      "date_modified": "2023-03-03T10:04:47.000Z",
      "content_text": "\n\n### Papers / 2022\n\n*   [Two-Face: Adversarial Audit of Commercial Face Recognition Systems](https://ojs.aaai.org/index.php/ICWSM/article/view/19300/19072) - (ICWSM) *Performs an adversarial audit on multiple systems APIs and datasets, making a number of concerning observations.*",
      "content_html": "<h3><p>Papers / 2022</p>\n</h3>\n<ul>\n<li><a href=\"https://ojs.aaai.org/index.php/ICWSM/article/view/19300/19072\" rel=\"noopener noreferrer\">Two-Face: Adversarial Audit of Commercial Face Recognition Systems</a> - (ICWSM) <em>Performs an adversarial audit on multiple systems APIs and datasets, making a number of concerning observations.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2023/5/",
      "title": "Awesome Audit Algorithms Updates on Jan 30 - Feb 05, 2023",
      "_short_title": "Jan 30 - Feb 05, 2023",
      "_slug": "2023/5/",
      "summary": "1 awesome projects updated on Jan 30 - Feb 05, 2023",
      "_filepath": "/content/2023/5/README.md",
      "url": "https://www.trackawesomelist.com/2023/5/",
      "date_published": "2023-01-30T11:29:16.000Z",
      "date_modified": "2023-01-30T11:29:16.000Z",
      "content_text": "\n\n### Papers / 2022\n\n*   [Scaling up search engine audits: Practical insights for algorithm auditing](https://journals.sagepub.com/doi/10.1177/01655515221093029) - (Journal of Information Science) [(Code) (⭐31)](https://github.com/gesiscss/WebBot) *Audits multiple search engines using simulated browsing behavior with virtual agents.*",
      "content_html": "<h3><p>Papers / 2022</p>\n</h3>\n<ul>\n<li><a href=\"https://journals.sagepub.com/doi/10.1177/01655515221093029\" rel=\"noopener noreferrer\">Scaling up search engine audits: Practical insights for algorithm auditing</a> - (Journal of Information Science) <a href=\"https://github.com/gesiscss/WebBot\" rel=\"noopener noreferrer\">(Code) (⭐31)</a> <em>Audits multiple search engines using simulated browsing behavior with virtual agents.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2023/4/",
      "title": "Awesome Audit Algorithms Updates on Jan 23 - Jan 29, 2023",
      "_short_title": "Jan 23 - Jan 29, 2023",
      "_slug": "2023/4/",
      "summary": "1 awesome projects updated on Jan 23 - Jan 29, 2023",
      "_filepath": "/content/2023/4/README.md",
      "url": "https://www.trackawesomelist.com/2023/4/",
      "date_published": "2023-01-23T09:33:49.000Z",
      "date_modified": "2023-01-23T09:33:49.000Z",
      "content_text": "\n\n### Papers / 2022\n\n*   [A zest of lime: towards architecture-independent model distances](https://openreview.net/pdf?id=OUz_9TiTv9j) - (ICLR) *Measures the distance between two remote models using LIME.*",
      "content_html": "<h3><p>Papers / 2022</p>\n</h3>\n<ul>\n<li><a href=\"https://openreview.net/pdf?id=OUz_9TiTv9j\" rel=\"noopener noreferrer\">A zest of lime: towards architecture-independent model distances</a> - (ICLR) <em>Measures the distance between two remote models using LIME.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2023/3/",
      "title": "Awesome Audit Algorithms Updates on Jan 16 - Jan 22, 2023",
      "_short_title": "Jan 16 - Jan 22, 2023",
      "_slug": "2023/3/",
      "summary": "1 awesome projects updated on Jan 16 - Jan 22, 2023",
      "_filepath": "/content/2023/3/README.md",
      "url": "https://www.trackawesomelist.com/2023/3/",
      "date_published": "2023-01-20T15:44:32.000Z",
      "date_modified": "2023-01-20T15:44:32.000Z",
      "content_text": "\n\n### Related Events / 2023\n\n*   [Workshop on Algorithmic Audits of Algorithms (WAAA)](https://algorithmic-audits.github.io)",
      "content_html": "<h3><p>Related Events / 2023</p>\n</h3>\n<ul>\n<li><a href=\"https://algorithmic-audits.github.io\" rel=\"noopener noreferrer\">Workshop on Algorithmic Audits of Algorithms (WAAA)</a></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2022/41/",
      "title": "Awesome Audit Algorithms Updates on Oct 10 - Oct 16, 2022",
      "_short_title": "Oct 10 - Oct 16, 2022",
      "_slug": "2022/41/",
      "summary": "1 awesome projects updated on Oct 10 - Oct 16, 2022",
      "_filepath": "/content/2022/41/README.md",
      "url": "https://www.trackawesomelist.com/2022/41/",
      "date_published": "2022-10-11T08:14:17.000Z",
      "date_modified": "2022-10-11T08:14:17.000Z",
      "content_text": "\n\n### Papers / 2013\n\n*   [Auditing: Active Learning with Outcome-Dependent Query Costs](https://www.cs.bgu.ac.il/~sabatos/papers/SabatoSarwate13.pdf) - (NIPS) *Learns from a binary classifier paying only for negative labels.*",
      "content_html": "<h3><p>Papers / 2013</p>\n</h3>\n<ul>\n<li><a href=\"https://www.cs.bgu.ac.il/~sabatos/papers/SabatoSarwate13.pdf\" rel=\"noopener noreferrer\">Auditing: Active Learning with Outcome-Dependent Query Costs</a> - (NIPS) <em>Learns from a binary classifier paying only for negative labels.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2022/36/",
      "title": "Awesome Audit Algorithms Updates on Sep 05 - Sep 11, 2022",
      "_short_title": "Sep 05 - Sep 11, 2022",
      "_slug": "2022/36/",
      "summary": "2 awesome projects updated on Sep 05 - Sep 11, 2022",
      "_filepath": "/content/2022/36/README.md",
      "url": "https://www.trackawesomelist.com/2022/36/",
      "date_published": "2022-09-09T13:25:06.000Z",
      "date_modified": "2022-09-09T13:25:06.000Z",
      "content_text": "\n\n### Papers / 2018\n\n*   [Auditing the Personalization and Composition of Politically-Related Search Engine Results Pages](https://dl.acm.org/doi/10.1145/3178876.3186143) - (WWW) *A Chrome extension to survey participants and collect the Search Engine Results Pages (SERPs) and autocomplete suggestions, for studying personalization and composition.*\n\n### Papers / 2016\n\n*   [An Empirical Analysis of Algorithmic Pricing on Amazon Marketplace](https://mislove.org/publications/Amazon-WWW.pdf) - (WWW) [(Code)](http://personalization.ccs.neu.edu) *Develops a methodology for detecting algorithmic pricing, and use it empirically to analyze their prevalence and behavior on Amazon Marketplace.*",
      "content_html": "<h3><p>Papers / 2018</p>\n</h3>\n<ul>\n<li><a href=\"https://dl.acm.org/doi/10.1145/3178876.3186143\" rel=\"noopener noreferrer\">Auditing the Personalization and Composition of Politically-Related Search Engine Results Pages</a> - (WWW) <em>A Chrome extension to survey participants and collect the Search Engine Results Pages (SERPs) and autocomplete suggestions, for studying personalization and composition.</em></li>\n</ul>\n<h3><p>Papers / 2016</p>\n</h3>\n<ul>\n<li><a href=\"https://mislove.org/publications/Amazon-WWW.pdf\" rel=\"noopener noreferrer\">An Empirical Analysis of Algorithmic Pricing on Amazon Marketplace</a> - (WWW) <a href=\"http://personalization.ccs.neu.edu\" rel=\"noopener noreferrer\">(Code)</a> <em>Develops a methodology for detecting algorithmic pricing, and use it empirically to analyze their prevalence and behavior on Amazon Marketplace.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2022/35/",
      "title": "Awesome Audit Algorithms Updates on Aug 29 - Sep 04, 2022",
      "_short_title": "Aug 29 - Sep 04, 2022",
      "_slug": "2022/35/",
      "summary": "1 awesome projects updated on Aug 29 - Sep 04, 2022",
      "_filepath": "/content/2022/35/README.md",
      "url": "https://www.trackawesomelist.com/2022/35/",
      "date_published": "2022-09-01T09:25:45.000Z",
      "date_modified": "2022-09-01T09:25:45.000Z",
      "content_text": "\n\n### Papers / 2021\n\n*   [Everyday Algorithm Auditing: Understanding the Power of Everyday Users in Surfacing Harmful Algorithmic Behaviors](https://arxiv.org/pdf/2105.02980.pdf) - (CHI) *Makes the case for \"everyday algorithmic auditing\" by users.*",
      "content_html": "<h3><p>Papers / 2021</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2105.02980.pdf\" rel=\"noopener noreferrer\">Everyday Algorithm Auditing: Understanding the Power of Everyday Users in Surfacing Harmful Algorithmic Behaviors</a> - (CHI) <em>Makes the case for \"everyday algorithmic auditing\" by users.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2022/32/",
      "title": "Awesome Audit Algorithms Updates on Aug 08 - Aug 14, 2022",
      "_short_title": "Aug 08 - Aug 14, 2022",
      "_slug": "2022/32/",
      "summary": "3 awesome projects updated on Aug 08 - Aug 14, 2022",
      "_filepath": "/content/2022/32/README.md",
      "url": "https://www.trackawesomelist.com/2022/32/",
      "date_published": "2022-08-10T12:15:38.000Z",
      "date_modified": "2022-08-12T08:01:02.000Z",
      "content_text": "\n\n### Papers / 2022\n\n*   [Active Fairness Auditing](https://proceedings.mlr.press/v162/yan22c/yan22c.pdf) - (ICML) *Studies of query-based auditing algorithms that can estimate the demographic parity of ML models in a query-efficient manner.*\n\n### Papers / 2021\n\n*   [Extracting Training Data from Large Language Models](https://arxiv.org/pdf/2012.07805.pdf) - (USENIX Security) *Extract verbatim text sequences from the GPT-2 model’s training data.*\n\n### Papers / 2018\n\n*   [Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation](https://arxiv.org/abs/1710.06169) - (AIES) *Treats black box models as teachers, training transparent student models to mimic the risk scores assigned by black-box models.*",
      "content_html": "<h3><p>Papers / 2022</p>\n</h3>\n<ul>\n<li><a href=\"https://proceedings.mlr.press/v162/yan22c/yan22c.pdf\" rel=\"noopener noreferrer\">Active Fairness Auditing</a> - (ICML) <em>Studies of query-based auditing algorithms that can estimate the demographic parity of ML models in a query-efficient manner.</em></li>\n</ul>\n<h3><p>Papers / 2021</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2012.07805.pdf\" rel=\"noopener noreferrer\">Extracting Training Data from Large Language Models</a> - (USENIX Security) <em>Extract verbatim text sequences from the GPT-2 model’s training data.</em></li>\n</ul>\n<h3><p>Papers / 2018</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1710.06169\" rel=\"noopener noreferrer\">Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation</a> - (AIES) <em>Treats black box models as teachers, training transparent student models to mimic the risk scores assigned by black-box models.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2022/30/",
      "title": "Awesome Audit Algorithms Updates on Jul 25 - Jul 31, 2022",
      "_short_title": "Jul 25 - Jul 31, 2022",
      "_slug": "2022/30/",
      "summary": "1 awesome projects updated on Jul 25 - Jul 31, 2022",
      "_filepath": "/content/2022/30/README.md",
      "url": "https://www.trackawesomelist.com/2022/30/",
      "date_published": "2022-07-26T07:20:55.000Z",
      "date_modified": "2022-07-26T07:20:55.000Z",
      "content_text": "\n\n### Papers / 2020\n\n*   [Black-Box Ripper: Copying black-box models using generative evolutionary algorithms](https://proceedings.neurips.cc/paper/2020/file/e8d66338fab3727e34a9179ed8804f64-Paper.pdf) - (NeurIPS) *Replicates the functionality of a black-box neural model, yet with no limit on the amount of queries (via a teacher/student scheme and an evolutionary search).*",
      "content_html": "<h3><p>Papers / 2020</p>\n</h3>\n<ul>\n<li><a href=\"https://proceedings.neurips.cc/paper/2020/file/e8d66338fab3727e34a9179ed8804f64-Paper.pdf\" rel=\"noopener noreferrer\">Black-Box Ripper: Copying black-box models using generative evolutionary algorithms</a> - (NeurIPS) <em>Replicates the functionality of a black-box neural model, yet with no limit on the amount of queries (via a teacher/student scheme and an evolutionary search).</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2022/22/",
      "title": "Awesome Audit Algorithms Updates on May 30 - Jun 05, 2022",
      "_short_title": "May 30 - Jun 05, 2022",
      "_slug": "2022/22/",
      "summary": "1 awesome projects updated on May 30 - Jun 05, 2022",
      "_filepath": "/content/2022/22/README.md",
      "url": "https://www.trackawesomelist.com/2022/22/",
      "date_published": "2022-06-03T13:45:38.000Z",
      "date_modified": "2022-06-03T13:45:38.000Z",
      "content_text": "\n\n### Papers / 2022\n\n*   [Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis](https://proceedings.neurips.cc/paper/2021/file/da94cbeff56cfda50785df477941308b-Paper.pdf) - (NeurIPS) *Sobol indices provide an efficient way to capture higher-order interactions between image regions and their contributions to a (black box) neural network’s prediction through the lens of variance.*",
      "content_html": "<h3><p>Papers / 2022</p>\n</h3>\n<ul>\n<li><a href=\"https://proceedings.neurips.cc/paper/2021/file/da94cbeff56cfda50785df477941308b-Paper.pdf\" rel=\"noopener noreferrer\">Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis</a> - (NeurIPS) <em>Sobol indices provide an efficient way to capture higher-order interactions between image regions and their contributions to a (black box) neural network’s prediction through the lens of variance.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2022/18/",
      "title": "Awesome Audit Algorithms Updates on May 02 - May 08, 2022",
      "_short_title": "May 02 - May 08, 2022",
      "_slug": "2022/18/",
      "summary": "1 awesome projects updated on May 02 - May 08, 2022",
      "_filepath": "/content/2022/18/README.md",
      "url": "https://www.trackawesomelist.com/2022/18/",
      "date_published": "2022-05-06T06:59:57.000Z",
      "date_modified": "2022-05-06T06:59:57.000Z",
      "content_text": "\n\n### Papers / 2022\n\n*   [Your Echos are Heard: Tracking, Profiling, and Ad Targeting in the Amazon Smart Speaker Ecosystem](https://arxiv.org/pdf/2204.10920.pdf) - (arxiv) *Infers a link between the Amazon Echo system and the ad targeting algorithm.*",
      "content_html": "<h3><p>Papers / 2022</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2204.10920.pdf\" rel=\"noopener noreferrer\">Your Echos are Heard: Tracking, Profiling, and Ad Targeting in the Amazon Smart Speaker Ecosystem</a> - (arxiv) <em>Infers a link between the Amazon Echo system and the ad targeting algorithm.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2022/14/",
      "title": "Awesome Audit Algorithms Updates on Apr 04 - Apr 10, 2022",
      "_short_title": "Apr 04 - Apr 10, 2022",
      "_slug": "2022/14/",
      "summary": "1 awesome projects updated on Apr 04 - Apr 10, 2022",
      "_filepath": "/content/2022/14/README.md",
      "url": "https://www.trackawesomelist.com/2022/14/",
      "date_published": "2022-04-04T14:13:00.000Z",
      "date_modified": "2022-04-04T14:13:00.000Z",
      "content_text": "\n\n### Papers / 2021\n\n*   [Auditing Black-Box Prediction Models for Data Minimization Compliance](https://www.cs.bu.edu/faculty/crovella/paper-archive/minimization-audit-Neurips21.pdf) - (NeurIPS) *Measures the level of data minimization satisfied by the prediction model using a limited number of queries.*",
      "content_html": "<h3><p>Papers / 2021</p>\n</h3>\n<ul>\n<li><a href=\"https://www.cs.bu.edu/faculty/crovella/paper-archive/minimization-audit-Neurips21.pdf\" rel=\"noopener noreferrer\">Auditing Black-Box Prediction Models for Data Minimization Compliance</a> - (NeurIPS) <em>Measures the level of data minimization satisfied by the prediction model using a limited number of queries.</em></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2022/7/",
      "title": "Awesome Audit Algorithms Updates on Feb 14 - Feb 20, 2022",
      "_short_title": "Feb 14 - Feb 20, 2022",
      "_slug": "2022/7/",
      "summary": "42 awesome projects updated on Feb 14 - Feb 20, 2022",
      "_filepath": "/content/2022/7/README.md",
      "url": "https://www.trackawesomelist.com/2022/7/",
      "date_published": "2022-02-16T12:52:28.000Z",
      "date_modified": "2022-02-18T10:53:10.000Z",
      "content_text": "\n\n### Papers / 2021\n\n*   [Setting the Record Straighter on Shadow Banning](https://arxiv.org/abs/2012.05101) - (INFOCOM)  [(Code)](https://gitlab.enseeiht.fr/bmorgan/infocom-2021) *Considers the possibility of shadow banning in Twitter (ie, the moderation black-box algorithm), and measures the probability of several hypothesis.*\n*   [FairLens: Auditing black-box clinical decision support systems](https://www.sciencedirect.com/science/article/pii/S030645732100145X?casa_token=oyjFKij269MAAAAA:w_ohScpMPNMnkDdzBqAIod5QfBgQlq5Ht9mMRSOydZpOgNG-i1yuqEmBjWN__38gOGmjNL7dVT0) - (Information Processing & Management) *Presents a pipeline to detect and explain potential fairness issues in Clinical DSS, by comparing different multi-label classification disparity measures.*\n*   [Auditing Algorithmic Bias on Twitter](https://dl.acm.org/doi/abs/10.1145/3447535.3462491) - (WebSci).\n*   [Bayesian Algorithm Execution: Estimating Computable Properties of Black-box Functions Using Mutual Information](https://proceedings.mlr.press/v139/neiswanger21a.html) - (ICML) *A budget constrained and Bayesian optimization procedure to extract properties out of a black-box algorithm.*\n\n### Papers / 2020\n\n*   [Adversarial Model Extraction on Graph Neural Networks](https://arxiv.org/abs/1912.07721) - (AAAI Workshop on Deep Learning on Graphs: Methodologies and Applications) *Introduces GNN model extraction and presents a preliminary approach for this.*\n*   [Remote Explainability faces the bouncer problem](https://rdcu.be/b6qB4) - (Nature Machine Intelligence volume 2, pages529–539)  [(Code) (⭐4)](https://github.com/erwanlemerrer/bouncer_problem) *Shows the impossibility (with one request) or the difficulty to spot lies on the explanations of a remote AI decision.*\n*   [GeoDA: a geometric framework for black-box adversarial attacks](https://openaccess.thecvf.com/content_CVPR_2020/papers/Rahmati_GeoDA_A_Geometric_Framework_for_Black-Box_Adversarial_Attacks_CVPR_2020_paper.pdf) - (CVPR)  [(Code) (⭐34)](https://github.com/thisisalirah/GeoDA) *Crafts adversarial examples to fool models, in a pure blackbox setup (no gradients, inferred class only).*\n*   [The Imitation Game: Algorithm Selectionby Exploiting Black-Box Recommender (⭐2)](https://github.com/erwanlemerrer/erwanlemerrer.github.io/raw/master/files/imitation_blackbox_recommenders_netys-2020.pdf) - (Netys)  [(Code) (⭐1)](https://github.com/gdamaskinos/RecRank) *Parametrize a local recommendation algorithm by imitating the decision of a remote and better trained one.*\n*   [Auditing News Curation Systems:A Case Study Examining Algorithmic and Editorial Logic in Apple News](https://ojs.aaai.org/index.php/ICWSM/article/view/7277) - (ICWSM) *Audit study of Apple News as a sociotechnical news curation system (trending stories section).*\n*   [Auditing Algorithms:  On Lessons Learned and the Risks of DataMinimization](https://dl.acm.org/doi/pdf/10.1145/3375627.3375852) - (AIES) *A practical audit for a well-being recommendation app developed by Telefónica (mostly on bias).*\n*   [Extracting Training Data from Large Language Models](https://arxiv.org/pdf/2012.07805) - (arxiv) *Performs a training data extraction attack to recover individual training examples by querying the language model.*\n\n### Papers / 2019\n\n*   [Adversarial Frontier Stitching for Remote Neural Network Watermarking](https://arxiv.org/abs/1711.01894) - (Neural Computing and Applications) [(Alternative implementation) (⭐25)](https://github.com/dunky11/adversarial-frontier-stitching) *Check if a remote machine learning model is a \"leaked\" one: through standard API requests to a remote model, extract (or not) a zero-bit watermark, that was inserted to watermark valuable models (eg, large deep neural networks).*\n*   [Knockoff Nets: Stealing Functionality of Black-Box Models](https://arxiv.org/abs/1812.02766.pdf) - (CVPR) *Ask to what extent can an adversary steal functionality of such \"victim\" models based solely on blackbox interactions: image in, predictions out.*\n*   [Opening Up the Black Box:Auditing Google's Top Stories Algorithm](https://par.nsf.gov/servlets/purl/10101277) - (Flairs-32) *Audit of the Google's Top stories panel that pro-vides insights into its algorithmic choices for selectingand ranking news publisher*\n*   [Making targeted black-box evasion attacks effective andefficient](https://arxiv.org/pdf/1906.03397.pdf) - (arXiv) *Investigates how an adversary can optimally use its query budget for targeted evasion attacks against deep neural networks.*\n*   [Online Learning for Measuring Incentive Compatibility in Ad Auctions](https://research.fb.com/wp-content/uploads/2019/05/Online-Learning-for-Measuring-Incentive-Compatibility-in-Ad-Auctions.pdf) - (WWW) *Measures the incentive compatible- (IC) mechanisms (regret) of black-box auction platforms.*\n*   [TamperNN: Efficient Tampering Detection of Deployed Neural Nets](https://arxiv.org/abs/1903.00317) - (ISSRE) *Algorithms to craft inputs that can detect the tampering with a remotely executed classifier model.*\n*   [Neural Network Model Extraction Attacks in Edge Devicesby Hearing Architectural Hints](https://arxiv.org/pdf/1903.03916.pdf) - (arxiv) *Through the acquisition of memory access events from bus snooping, layer sequence identification bythe LSTM-CTC model, layer topology connection according to the memory access pattern, and layer dimension estimation under data volume constraints, it demonstrates one can accurately recover the a similar network architecture as the attack starting point*\n*   [Stealing Knowledge from Protected Deep Neural Networks Using Composite Unlabeled Data](https://ieeexplore.ieee.org/abstract/document/8851798) - (ICNN) *Composite method which can be used to attack and extract the knowledge ofa black box model even if it completely conceals its softmaxoutput.*\n*   [Neural Network Inversion in Adversarial Setting via Background Knowledge Alignment](https://dl.acm.org/citation.cfm?id=3354261) - (CCS) *Model inversion approach in the adversary setting based on training an inversion model that acts as aninverse of the original model. With no fullknowledge about the original training data, an accurate inversion is still possible by training the inversion model on auxiliary samplesdrawn from a more generic data distribution.*\n\n### Papers / 2018\n\n*   [Towards Reverse-Engineering Black-Box Neural Networks](https://arxiv.org/abs/1711.01768) - (ICLR) [(Code) (⭐55)](https://github.com/coallaoh/WhitenBlackBox) *Infer inner hyperparameters (eg number of layers, non-linear activation type) of a remote neural network model by analysing its response patterns to certain inputs.*\n*   [Data driven exploratory attacks on black box classifiers in adversarial domains](https://www.sciencedirect.com/science/article/pii/S092523121830136X) - (Neurocomputing) *Reverse engineers remote classifier models (e.g., for evading a CAPTCHA test).*\n*   [xGEMs: Generating Examplars to Explain Black-Box Models](https://arxiv.org/pdf/1806.08867.pdf) - (arXiv) *Searches bias in the black box model by training an unsupervised implicit generative model. Thensummarizes the black-box model behavior quantitatively by perturbing data samples along the data manifold.*\n*   [Learning Networks from Random Walk-Based Node Similarities](https://arxiv.org/pdf/1801.07386) - (NIPS) *Reversing graphs by observing some random walk commute times.*\n*   [Identifying the Machine Learning Family from Black-Box Models](https://rd.springer.com/chapter/10.1007/978-3-030-00374-6_6) - (CAEPIA) *Determines which kind of machine learning model is behind the returned predictions.*\n*   [Stealing Neural Networks via Timing Side Channels](https://arxiv.org/pdf/1812.11720.pdf) - (arXiv) *Stealing/approximating a model through timing attacks usin queries.*\n*   [Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data](https://arxiv.org/abs/1806.05476) - (IJCNN)  [(Code) (⭐27)](https://github.com/jeiks/Stealing_DL_Models) *Stealing black-box models (CNNs) knowledge by querying them with random natural images (ImageNet and Microsoft-COCO).*\n\n### Papers / 2017\n\n*   [Uncovering Influence Cookbooks : Reverse Engineering the Topological Impact in Peer Ranking Services](https://dl.acm.org/authorize.cfm?key=N21772) - (CSCW) *Aims at identifying which centrality metrics are in use in a peer ranking service.*\n*   [The topological face of recommendation: models and application to bias detection](https://arxiv.org/abs/1704.08991) - (Complex Networks) *Proposes a bias detection framework for items recommended to users.*\n*   [Membership Inference Attacks Against Machine Learning Models](http://ieeexplore.ieee.org/document/7958568/) - (Symposium on Security and Privacy) *Given a machine learning model and a record, determine whether this record was used as part of the model's training dataset or not.*\n*   [Practical Black-Box Attacks against Machine Learning](https://dl.acm.org/citation.cfm?id=3053009) - (Asia CCS) *Understand how vulnerable is a remote service to adversarial classification attacks.*\n\n### Papers / 2016\n\n*   [Bias in Online Freelance Marketplaces: Evidence from TaskRabbit](http://datworkshop.org/papers/dat16-final22.pdf) - (dat workshop) *Measures the TaskRabbit's search algorithm rank.*\n*   [Stealing Machine Learning Models via Prediction APIs](https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer) - (Usenix Security)  [(Code) (⭐350)](https://github.com/ftramer/Steal-ML) *Aims at extracting machine learning models in use by remote services.*\n*   [“Why Should I Trust You?”Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938v3.pdf) - (arXiv)  [(Code) (⭐318)](https://github.com/marcotcr/lime-experiments) *Explains a blackbox classifier model by sampling around data instances.*\n*   [Back in Black: Towards Formal, Black Box Analysis of Sanitizers and Filters](http://ieeexplore.ieee.org/document/7546497/) - (Security and Privacy) *Black-box analysis of sanitizers and filters.*\n*   [Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems](http://ieeexplore.ieee.org/document/7546525/) - (Security and Privacy) *Introduces measures that capture the degree of influence of inputs on outputs of the observed system.*\n\n### Papers / 2015\n\n*   [Peeking Beneath the Hood of Uber](https://dl.acm.org/citation.cfm?id=2815681) - (IMC) *Infer implementation details of Uber's surge price algorithm.*\n\n### Papers / 2014\n\n*   [XRay: Enhancing the Web's Transparency with Differential Correlation](https://www.usenix.org/node/184394) - (USENIX Security) *Audits which user profile data were used for targeting a particular ad, recommendation, or price.*\n\n### Papers / 2013\n\n*   [Measuring Personalization of Web Search](https://dl.acm.org/citation.cfm?id=2488435) - (WWW) *Develops a methodology for measuring personalization in Web search result.*\n\n### Papers / 2012\n\n*   [Query Strategies for Evading Convex-Inducing Classifiers](http://www.jmlr.org/papers/v13/nelson12a.html) - (JMLR) *Evasion methods for convex classifiers. Considers evasion complexity.*\n\n### Papers / 2008\n\n*   [Privacy Oracle: a System for Finding Application Leakswith Black Box Differential Testing](https://dl.acm.org/citation.cfm?id=1455806) - (CCS) *Privacy Oracle: a system that uncovers applications' leaks of personal information in transmissions to remoteservers.*\n\n### Papers / 2005\n\n*   [Adversarial Learning](https://dl.acm.org/citation.cfm?id=1081950) - (KDD) *Reverse engineering of remote linear classifiers, using membership queries.*",
      "content_html": "<h3><p>Papers / 2021</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2012.05101\" rel=\"noopener noreferrer\">Setting the Record Straighter on Shadow Banning</a> - (INFOCOM)  <a href=\"https://gitlab.enseeiht.fr/bmorgan/infocom-2021\" rel=\"noopener noreferrer\">(Code)</a> <em>Considers the possibility of shadow banning in Twitter (ie, the moderation black-box algorithm), and measures the probability of several hypothesis.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.sciencedirect.com/science/article/pii/S030645732100145X?casa_token=oyjFKij269MAAAAA:w_ohScpMPNMnkDdzBqAIod5QfBgQlq5Ht9mMRSOydZpOgNG-i1yuqEmBjWN__38gOGmjNL7dVT0\" rel=\"noopener noreferrer\">FairLens: Auditing black-box clinical decision support systems</a> - (Information Processing &amp; Management) <em>Presents a pipeline to detect and explain potential fairness issues in Clinical DSS, by comparing different multi-label classification disparity measures.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://dl.acm.org/doi/abs/10.1145/3447535.3462491\" rel=\"noopener noreferrer\">Auditing Algorithmic Bias on Twitter</a> - (WebSci).</li>\n</ul>\n\n<ul>\n<li><a href=\"https://proceedings.mlr.press/v139/neiswanger21a.html\" rel=\"noopener noreferrer\">Bayesian Algorithm Execution: Estimating Computable Properties of Black-box Functions Using Mutual Information</a> - (ICML) <em>A budget constrained and Bayesian optimization procedure to extract properties out of a black-box algorithm.</em></li>\n</ul>\n<h3><p>Papers / 2020</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1912.07721\" rel=\"noopener noreferrer\">Adversarial Model Extraction on Graph Neural Networks</a> - (AAAI Workshop on Deep Learning on Graphs: Methodologies and Applications) <em>Introduces GNN model extraction and presents a preliminary approach for this.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://rdcu.be/b6qB4\" rel=\"noopener noreferrer\">Remote Explainability faces the bouncer problem</a> - (Nature Machine Intelligence volume 2, pages529–539)  <a href=\"https://github.com/erwanlemerrer/bouncer_problem\" rel=\"noopener noreferrer\">(Code) (⭐4)</a> <em>Shows the impossibility (with one request) or the difficulty to spot lies on the explanations of a remote AI decision.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Rahmati_GeoDA_A_Geometric_Framework_for_Black-Box_Adversarial_Attacks_CVPR_2020_paper.pdf\" rel=\"noopener noreferrer\">GeoDA: a geometric framework for black-box adversarial attacks</a> - (CVPR)  <a href=\"https://github.com/thisisalirah/GeoDA\" rel=\"noopener noreferrer\">(Code) (⭐34)</a> <em>Crafts adversarial examples to fool models, in a pure blackbox setup (no gradients, inferred class only).</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://github.com/erwanlemerrer/erwanlemerrer.github.io/raw/master/files/imitation_blackbox_recommenders_netys-2020.pdf\" rel=\"noopener noreferrer\">The Imitation Game: Algorithm Selectionby Exploiting Black-Box Recommender (⭐2)</a> - (Netys)  <a href=\"https://github.com/gdamaskinos/RecRank\" rel=\"noopener noreferrer\">(Code) (⭐1)</a> <em>Parametrize a local recommendation algorithm by imitating the decision of a remote and better trained one.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://ojs.aaai.org/index.php/ICWSM/article/view/7277\" rel=\"noopener noreferrer\">Auditing News Curation Systems:A Case Study Examining Algorithmic and Editorial Logic in Apple News</a> - (ICWSM) <em>Audit study of Apple News as a sociotechnical news curation system (trending stories section).</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://dl.acm.org/doi/pdf/10.1145/3375627.3375852\" rel=\"noopener noreferrer\">Auditing Algorithms:  On Lessons Learned and the Risks of DataMinimization</a> - (AIES) <em>A practical audit for a well-being recommendation app developed by Telefónica (mostly on bias).</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2012.07805\" rel=\"noopener noreferrer\">Extracting Training Data from Large Language Models</a> - (arxiv) <em>Performs a training data extraction attack to recover individual training examples by querying the language model.</em></li>\n</ul>\n<h3><p>Papers / 2019</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1711.01894\" rel=\"noopener noreferrer\">Adversarial Frontier Stitching for Remote Neural Network Watermarking</a> - (Neural Computing and Applications) <a href=\"https://github.com/dunky11/adversarial-frontier-stitching\" rel=\"noopener noreferrer\">(Alternative implementation) (⭐25)</a> <em>Check if a remote machine learning model is a \"leaked\" one: through standard API requests to a remote model, extract (or not) a zero-bit watermark, that was inserted to watermark valuable models (eg, large deep neural networks).</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1812.02766.pdf\" rel=\"noopener noreferrer\">Knockoff Nets: Stealing Functionality of Black-Box Models</a> - (CVPR) <em>Ask to what extent can an adversary steal functionality of such \"victim\" models based solely on blackbox interactions: image in, predictions out.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://par.nsf.gov/servlets/purl/10101277\" rel=\"noopener noreferrer\">Opening Up the Black Box:Auditing Google's Top Stories Algorithm</a> - (Flairs-32) <em>Audit of the Google's Top stories panel that pro-vides insights into its algorithmic choices for selectingand ranking news publisher</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1906.03397.pdf\" rel=\"noopener noreferrer\">Making targeted black-box evasion attacks effective andefficient</a> - (arXiv) <em>Investigates how an adversary can optimally use its query budget for targeted evasion attacks against deep neural networks.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://research.fb.com/wp-content/uploads/2019/05/Online-Learning-for-Measuring-Incentive-Compatibility-in-Ad-Auctions.pdf\" rel=\"noopener noreferrer\">Online Learning for Measuring Incentive Compatibility in Ad Auctions</a> - (WWW) <em>Measures the incentive compatible- (IC) mechanisms (regret) of black-box auction platforms.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1903.00317\" rel=\"noopener noreferrer\">TamperNN: Efficient Tampering Detection of Deployed Neural Nets</a> - (ISSRE) <em>Algorithms to craft inputs that can detect the tampering with a remotely executed classifier model.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1903.03916.pdf\" rel=\"noopener noreferrer\">Neural Network Model Extraction Attacks in Edge Devicesby Hearing Architectural Hints</a> - (arxiv) <em>Through the acquisition of memory access events from bus snooping, layer sequence identification bythe LSTM-CTC model, layer topology connection according to the memory access pattern, and layer dimension estimation under data volume constraints, it demonstrates one can accurately recover the a similar network architecture as the attack starting point</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/abstract/document/8851798\" rel=\"noopener noreferrer\">Stealing Knowledge from Protected Deep Neural Networks Using Composite Unlabeled Data</a> - (ICNN) <em>Composite method which can be used to attack and extract the knowledge ofa black box model even if it completely conceals its softmaxoutput.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://dl.acm.org/citation.cfm?id=3354261\" rel=\"noopener noreferrer\">Neural Network Inversion in Adversarial Setting via Background Knowledge Alignment</a> - (CCS) <em>Model inversion approach in the adversary setting based on training an inversion model that acts as aninverse of the original model. With no fullknowledge about the original training data, an accurate inversion is still possible by training the inversion model on auxiliary samplesdrawn from a more generic data distribution.</em></li>\n</ul>\n<h3><p>Papers / 2018</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1711.01768\" rel=\"noopener noreferrer\">Towards Reverse-Engineering Black-Box Neural Networks</a> - (ICLR) <a href=\"https://github.com/coallaoh/WhitenBlackBox\" rel=\"noopener noreferrer\">(Code) (⭐55)</a> <em>Infer inner hyperparameters (eg number of layers, non-linear activation type) of a remote neural network model by analysing its response patterns to certain inputs.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.sciencedirect.com/science/article/pii/S092523121830136X\" rel=\"noopener noreferrer\">Data driven exploratory attacks on black box classifiers in adversarial domains</a> - (Neurocomputing) <em>Reverse engineers remote classifier models (e.g., for evading a CAPTCHA test).</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1806.08867.pdf\" rel=\"noopener noreferrer\">xGEMs: Generating Examplars to Explain Black-Box Models</a> - (arXiv) <em>Searches bias in the black box model by training an unsupervised implicit generative model. Thensummarizes the black-box model behavior quantitatively by perturbing data samples along the data manifold.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1801.07386\" rel=\"noopener noreferrer\">Learning Networks from Random Walk-Based Node Similarities</a> - (NIPS) <em>Reversing graphs by observing some random walk commute times.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://rd.springer.com/chapter/10.1007/978-3-030-00374-6_6\" rel=\"noopener noreferrer\">Identifying the Machine Learning Family from Black-Box Models</a> - (CAEPIA) <em>Determines which kind of machine learning model is behind the returned predictions.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1812.11720.pdf\" rel=\"noopener noreferrer\">Stealing Neural Networks via Timing Side Channels</a> - (arXiv) <em>Stealing/approximating a model through timing attacks usin queries.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1806.05476\" rel=\"noopener noreferrer\">Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data</a> - (IJCNN)  <a href=\"https://github.com/jeiks/Stealing_DL_Models\" rel=\"noopener noreferrer\">(Code) (⭐27)</a> <em>Stealing black-box models (CNNs) knowledge by querying them with random natural images (ImageNet and Microsoft-COCO).</em></li>\n</ul>\n<h3><p>Papers / 2017</p>\n</h3>\n<ul>\n<li><a href=\"https://dl.acm.org/authorize.cfm?key=N21772\" rel=\"noopener noreferrer\">Uncovering Influence Cookbooks : Reverse Engineering the Topological Impact in Peer Ranking Services</a> - (CSCW) <em>Aims at identifying which centrality metrics are in use in a peer ranking service.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1704.08991\" rel=\"noopener noreferrer\">The topological face of recommendation: models and application to bias detection</a> - (Complex Networks) <em>Proposes a bias detection framework for items recommended to users.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"http://ieeexplore.ieee.org/document/7958568/\" rel=\"noopener noreferrer\">Membership Inference Attacks Against Machine Learning Models</a> - (Symposium on Security and Privacy) <em>Given a machine learning model and a record, determine whether this record was used as part of the model's training dataset or not.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://dl.acm.org/citation.cfm?id=3053009\" rel=\"noopener noreferrer\">Practical Black-Box Attacks against Machine Learning</a> - (Asia CCS) <em>Understand how vulnerable is a remote service to adversarial classification attacks.</em></li>\n</ul>\n<h3><p>Papers / 2016</p>\n</h3>\n<ul>\n<li><a href=\"http://datworkshop.org/papers/dat16-final22.pdf\" rel=\"noopener noreferrer\">Bias in Online Freelance Marketplaces: Evidence from TaskRabbit</a> - (dat workshop) <em>Measures the TaskRabbit's search algorithm rank.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer\" rel=\"noopener noreferrer\">Stealing Machine Learning Models via Prediction APIs</a> - (Usenix Security)  <a href=\"https://github.com/ftramer/Steal-ML\" rel=\"noopener noreferrer\">(Code) (⭐350)</a> <em>Aims at extracting machine learning models in use by remote services.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1602.04938v3.pdf\" rel=\"noopener noreferrer\">“Why Should I Trust You?”Explaining the Predictions of Any Classifier</a> - (arXiv)  <a href=\"https://github.com/marcotcr/lime-experiments\" rel=\"noopener noreferrer\">(Code) (⭐318)</a> <em>Explains a blackbox classifier model by sampling around data instances.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"http://ieeexplore.ieee.org/document/7546497/\" rel=\"noopener noreferrer\">Back in Black: Towards Formal, Black Box Analysis of Sanitizers and Filters</a> - (Security and Privacy) <em>Black-box analysis of sanitizers and filters.</em></li>\n</ul>\n\n<ul>\n<li><a href=\"http://ieeexplore.ieee.org/document/7546525/\" rel=\"noopener noreferrer\">Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems</a> - (Security and Privacy) <em>Introduces measures that capture the degree of influence of inputs on outputs of the observed system.</em></li>\n</ul>\n<h3><p>Papers / 2015</p>\n</h3>\n<ul>\n<li><a href=\"https://dl.acm.org/citation.cfm?id=2815681\" rel=\"noopener noreferrer\">Peeking Beneath the Hood of Uber</a> - (IMC) <em>Infer implementation details of Uber's surge price algorithm.</em></li>\n</ul>\n<h3><p>Papers / 2014</p>\n</h3>\n<ul>\n<li><a href=\"https://www.usenix.org/node/184394\" rel=\"noopener noreferrer\">XRay: Enhancing the Web's Transparency with Differential Correlation</a> - (USENIX Security) <em>Audits which user profile data were used for targeting a particular ad, recommendation, or price.</em></li>\n</ul>\n<h3><p>Papers / 2013</p>\n</h3>\n<ul>\n<li><a href=\"https://dl.acm.org/citation.cfm?id=2488435\" rel=\"noopener noreferrer\">Measuring Personalization of Web Search</a> - (WWW) <em>Develops a methodology for measuring personalization in Web search result.</em></li>\n</ul>\n<h3><p>Papers / 2012</p>\n</h3>\n<ul>\n<li><a href=\"http://www.jmlr.org/papers/v13/nelson12a.html\" rel=\"noopener noreferrer\">Query Strategies for Evading Convex-Inducing Classifiers</a> - (JMLR) <em>Evasion methods for convex classifiers. Considers evasion complexity.</em></li>\n</ul>\n<h3><p>Papers / 2008</p>\n</h3>\n<ul>\n<li><a href=\"https://dl.acm.org/citation.cfm?id=1455806\" rel=\"noopener noreferrer\">Privacy Oracle: a System for Finding Application Leakswith Black Box Differential Testing</a> - (CCS) <em>Privacy Oracle: a system that uncovers applications' leaks of personal information in transmissions to remoteservers.</em></li>\n</ul>\n<h3><p>Papers / 2005</p>\n</h3>\n<ul>\n<li><a href=\"https://dl.acm.org/citation.cfm?id=1081950\" rel=\"noopener noreferrer\">Adversarial Learning</a> - (KDD) <em>Reverse engineering of remote linear classifiers, using membership queries.</em></li>\n</ul>\n"
    }
  ]
}
