<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <title>Track Awesome Deep Learning (ChristosChristofidis/awesome-deep-learning) Updates Weekly - Track Awesome List</title>
    <meta property="og:url" content="https://www.trackawesomelist.com/ChristosChristofidis/awesome-deep-learning/week/" />
    <meta property="og:type" content="summary" />
    <meta property="og:title" content="Track Awesome Deep Learning Updates Weekly" />
    <meta property="og:description" content="A curated list of awesome Deep Learning tutorials, projects and communities." />
    <meta property="og:site_name" content="Track Awesome List" />
    <style>
      main {
        max-width: 1024px;
        margin: 0 auto;
        padding: 0 0.5em;
      }
      :root,[data-color-mode=light][data-light-theme=light],[data-color-mode=dark][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}[data-color-mode=light][data-light-theme=dark],[data-color-mode=dark][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}.markdown-body{word-wrap:break-word;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;font-size:16px;line-height:1.5}.markdown-body:before{content:"";display:table}.markdown-body:after{clear:both;content:"";display:table}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:var(--color-danger-fg)}.markdown-body .anchor{float:left;margin-left:-20px;padding-right:4px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body p,.markdown-body blockquote,.markdown-body ul,.markdown-body ol,.markdown-body dl,.markdown-body table,.markdown-body pre,.markdown-body details{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;background-color:var(--color-border-default);border:0;margin:24px 0;padding:0}.markdown-body blockquote{color:var(--color-fg-muted);border-left:.25em solid var(--color-border-default);padding:0 1em}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:var(--color-fg-default);vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 tt,.markdown-body h1 code,.markdown-body h2 tt,.markdown-body h2 code,.markdown-body h3 tt,.markdown-body h3 code,.markdown-body h4 tt,.markdown-body h4 code,.markdown-body h5 tt,.markdown-body h5 code,.markdown-body h6 tt,.markdown-body h6 code{font-size:inherit;padding:0 .2em}.markdown-body h1{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:2em}.markdown-body h2{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{color:var(--color-fg-muted);font-size:.85em}.markdown-body summary h1,.markdown-body summary h2,.markdown-body summary h3,.markdown-body summary h4,.markdown-body summary h5,.markdown-body summary h6{display:inline-block}.markdown-body summary h1 .anchor,.markdown-body summary h2 .anchor,.markdown-body summary h3 .anchor,.markdown-body summary h4 .anchor,.markdown-body summary h5 .anchor,.markdown-body summary h6 .anchor{margin-left:-40px}.markdown-body summary h1,.markdown-body summary h2{border-bottom:0;padding-bottom:0}.markdown-body ul,.markdown-body ol{padding-left:2em}.markdown-body ul.no-list,.markdown-body ol.no-list{padding:0;list-style-type:none}.markdown-body ol[type="1"]{list-style-type:decimal}.markdown-body ol[type=a]{list-style-type:lower-alpha}.markdown-body ol[type=i]{list-style-type:lower-roman}.markdown-body div>ol:not([type]){list-style-type:decimal}.markdown-body ul ul,.markdown-body ul ol,.markdown-body ol ol,.markdown-body ol ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{margin-top:16px;padding:0;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{margin-bottom:16px;padding:0 16px}.markdown-body table{width:100%;width:-webkit-max-content;width:-webkit-max-content;width:max-content;max-width:100%;display:block;overflow:auto}.markdown-body table th{font-weight:600}.markdown-body table th,.markdown-body table td{border:1px solid var(--color-border-default);padding:6px 13px}.markdown-body table tr{background-color:var(--color-canvas-default);border-top:1px solid var(--color-border-muted)}.markdown-body table tr:nth-child(2n){background-color:var(--color-canvas-subtle)}.markdown-body table img{background-color:rgba(0,0,0,0)}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:var(--color-canvas-default)}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:rgba(0,0,0,0)}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{float:left;width:auto;border:1px solid var(--color-border-default);margin:13px 0 0;padding:7px;display:block;overflow:hidden}.markdown-body span.frame span img{float:left;display:block}.markdown-body span.frame span span{clear:both;color:var(--color-fg-default);padding:5px 0 0;display:block}.markdown-body span.align-center{clear:both;display:block;overflow:hidden}.markdown-body span.align-center>span{text-align:center;margin:13px auto 0;display:block;overflow:hidden}.markdown-body span.align-center span img{text-align:center;margin:0 auto}.markdown-body span.align-right{clear:both;display:block;overflow:hidden}.markdown-body span.align-right>span{text-align:right;margin:13px 0 0;display:block;overflow:hidden}.markdown-body span.align-right span img{text-align:right;margin:0}.markdown-body span.float-left{float:left;margin-right:13px;display:block;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{float:right;margin-left:13px;display:block;overflow:hidden}.markdown-body span.float-right>span{text-align:right;margin:13px auto 0;display:block;overflow:hidden}.markdown-body code,.markdown-body tt{background-color:var(--color-neutral-muted);border-radius:6px;margin:0;padding:.2em .4em;font-size:85%}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{-webkit-text-decoration:inherit;-webkit-text-decoration:inherit;text-decoration:inherit}.markdown-body samp{font-size:85%}.markdown-body pre{word-wrap:normal}.markdown-body pre code{font-size:100%}.markdown-body pre>code{word-break:normal;white-space:pre;background:0 0;border:0;margin:0;padding:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{word-break:normal;margin-bottom:0}.markdown-body .highlight pre,.markdown-body pre{background-color:var(--color-canvas-subtle);border-radius:6px;padding:16px;font-size:85%;line-height:1.45;overflow:auto}.markdown-body pre code,.markdown-body pre tt{max-width:auto;line-height:inherit;word-wrap:normal;background-color:rgba(0,0,0,0);border:0;margin:0;padding:0;display:inline;overflow:visible}.markdown-body .csv-data td,.markdown-body .csv-data th{text-align:left;white-space:nowrap;padding:5px;font-size:12px;line-height:1;overflow:hidden}.markdown-body .csv-data .blob-num{text-align:right;background:var(--color-canvas-default);border:0;padding:10px 8px 9px}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{background:var(--color-canvas-subtle);border-top:0;font-weight:600}.markdown-body [data-footnote-ref]:before{content:"["}.markdown-body [data-footnote-ref]:after{content:"]"}.markdown-body .footnotes{color:var(--color-fg-muted);border-top:1px solid var(--color-border-default);font-size:12px}.markdown-body .footnotes ol{padding-left:16px}.markdown-body .footnotes li{position:relative}.markdown-body .footnotes li:target:before{pointer-events:none;content:"";border:2px solid var(--color-accent-emphasis);border-radius:6px;position:absolute;top:-8px;bottom:-8px;left:-24px;right:-8px}.markdown-body .footnotes li:target{color:var(--color-fg-default)}.markdown-body .footnotes .data-footnote-backref g-emoji{font-family:monospace}.markdown-body{background-color:var(--color-canvas-default);color:var(--color-fg-default)}.markdown-body a{color:var(--color-accent-fg);text-decoration:none}.markdown-body a:hover{text-decoration:underline}.markdown-body iframe{background-color:#fff;border:0;margin-bottom:16px}.markdown-body svg.octicon{fill:currentColor}.markdown-body .anchor>.octicon{display:inline}.markdown-body .highlight .token.keyword,.gfm-highlight .token.keyword{color:var(--color-prettylights-syntax-keyword)}.markdown-body .highlight .token.tag .token.class-name,.markdown-body .highlight .token.tag .token.script .token.punctuation,.gfm-highlight .token.tag .token.class-name,.gfm-highlight .token.tag .token.script .token.punctuation{color:var(--color-prettylights-syntax-storage-modifier-import)}.markdown-body .highlight .token.operator,.markdown-body .highlight .token.number,.markdown-body .highlight .token.boolean,.markdown-body .highlight .token.tag .token.punctuation,.markdown-body .highlight .token.tag .token.script .token.script-punctuation,.markdown-body .highlight .token.tag .token.attr-name,.gfm-highlight .token.operator,.gfm-highlight .token.number,.gfm-highlight .token.boolean,.gfm-highlight .token.tag .token.punctuation,.gfm-highlight .token.tag .token.script .token.script-punctuation,.gfm-highlight .token.tag .token.attr-name{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.function,.gfm-highlight .token.function{color:var(--color-prettylights-syntax-entity)}.markdown-body .highlight .token.string,.gfm-highlight .token.string{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.comment,.gfm-highlight .token.comment{color:var(--color-prettylights-syntax-comment)}.markdown-body .highlight .token.class-name,.gfm-highlight .token.class-name{color:var(--color-prettylights-syntax-variable)}.markdown-body .highlight .token.regex,.gfm-highlight .token.regex{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.regex .regex-delimiter,.gfm-highlight .token.regex .regex-delimiter{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.tag .token.tag,.markdown-body .highlight .token.property,.gfm-highlight .token.tag .token.tag,.gfm-highlight .token.property{color:var(--color-prettylights-syntax-entity-tag)}
    </style>
  </head>
  <body>
    <main data-color-mode="light" data-light-theme="light" data-dark-theme="dark" class="markdown-body">
      <h1>Track Awesome Deep Learning Updates Weekly</h1>
<p>A curated list of awesome Deep Learning tutorials, projects and communities.</p>
<p><a href="/">üè† Home</a><span> ¬∑ </span><a href="https://www.trackawesomelist.com/search/">üîç Search</a><span> ¬∑ </span><a href="https://www.trackawesomelist.com/ChristosChristofidis/awesome-deep-learning/week/rss.xml">üî• Feed</a><span> ¬∑ </span><a href="https://trackawesomelist.us17.list-manage.com/subscribe?u=d2f0117aa829c83a63ec63c2f&id=36a103854c">üìÆ Subscribe</a><span> ¬∑ </span><a href="https://github.com/sponsors/theowenyoung">‚ù§Ô∏è  Sponsor</a><span> ¬∑ </span><a href="https://github.com/ChristosChristofidis/awesome-deep-learning">üò∫ ChristosChristofidis/awesome-deep-learning</a><span> ¬∑ </span><span>‚≠ê 26K</span><span> ¬∑ </span><span>üè∑Ô∏è Computer Science</span></p>
<p><span>[ </span><a href="/ChristosChristofidis/awesome-deep-learning/">Daily</a><span> / </span><span>Weekly</span><span> / </span><a href="/ChristosChristofidis/awesome-deep-learning/readme/">Overview</a><span> ]</span></p>

<h2><a href="https://www.trackawesomelist.com/2025/20/">May 19 - May 25, 2025</a></h2><h3><p>Researchers / Tools</p>
</h3>
<ul>
<li><a href="https://getmaxim.ai" rel="noopener noreferrer">Maxim AI</a> - Tool for AI Agent Simulation, Evaluation &amp; Observability.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2022/46/">Nov 14 - Nov 20, 2022</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="https://www.knowledgeisle.com/wp-content/uploads/2019/12/2-Aur%C3%A9lien-G%C3%A9ron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-O%E2%80%99Reilly-Media-2019.pdf" rel="noopener noreferrer">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</a> by Aur√©lien G√©ron  | Oct 15, 2019</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2022/34/">Aug 22 - Aug 28, 2022</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="https://www.manning.com/books/jax-in-action" rel="noopener noreferrer">Jax in Action</a> - by Grigory Sapunov</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2022/33/">Aug 15 - Aug 21, 2022</a></h2><h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="https://www.scaler.com/topics/what-is-deep-learning/" rel="noopener noreferrer">Understand what is Deep Learning</a></li>
</ul>
<h3><p>Researchers / Tools</p>
</h3>
<ul>
<li><a href="https://dvc.org/" rel="noopener noreferrer">DVC</a> - DVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code.</li>
</ul>

<ul>
<li><a href="https://cml.dev/" rel="noopener noreferrer">CML</a> - CML helps you bring your favorite DevOps tools to machine learning.</li>
</ul>

<ul>
<li><a href="https://mlem.ai/" rel="noopener noreferrer">MLEM</a> - MLEM is a tool to easily package, deploy and serve Machine Learning models. It seamlessly supports a variety of scenarios like real-time serving and batch processing.</li>
</ul>
<h3><p>Researchers / Miscellaneous</p>
</h3>
<ul>
<li><a href="http://on-demand-gtc.gputechconf.com/gtcnew/on-demand-gtc.php?searchByKeyword=shelhamer&amp;searchItems=&amp;sessionTopic=&amp;sessionEvent=4&amp;sessionYear=2014&amp;sessionFormat=&amp;submit=&amp;select=+" rel="noopener noreferrer">Caffe Webinar</a></li>
</ul>

<ul>
<li><a href="http://meta-guide.com/software-meta-guide/100-best-github-deep-learning/" rel="noopener noreferrer">100 Best Github Resources in Github for DL</a></li>
</ul>

<ul>
<li><a href="https://code.google.com/p/word2vec/" rel="noopener noreferrer">Word2Vec</a></li>
</ul>

<ul>
<li><a href="https://github.com/tleyden/docker/tree/master/caffe" rel="noopener noreferrer">Caffe DockerFile (‚≠ê81)</a></li>
</ul>

<ul>
<li><a href="https://github.com/TorontoDeepLearning/convnet" rel="noopener noreferrer">TorontoDeepLEarning convnet (‚≠ê506)</a></li>
</ul>

<ul>
<li><a href="https://github.com/clementfarabet/gfx.js" rel="noopener noreferrer">gfx.js (‚≠ê125)</a></li>
</ul>

<ul>
<li><a href="https://github.com/torch/torch7/wiki/Cheatsheet" rel="noopener noreferrer">Torch7 Cheat sheet (‚≠ê9.1k)</a></li>
</ul>

<ul>
<li><a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-864-advanced-natural-language-processing-fall-2005/" rel="noopener noreferrer">Misc from MIT's 'Advanced Natural Language Processing' course</a></li>
</ul>

<ul>
<li><a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/" rel="noopener noreferrer">Misc from MIT's 'Machine Learning' course</a></li>
</ul>

<ul>
<li><a href="http://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-520-a-networks-for-learning-regression-and-classification-spring-2001/" rel="noopener noreferrer">Misc from MIT's 'Networks for Learning: Regression and Classification' course</a></li>
</ul>

<ul>
<li><a href="http://ocw.mit.edu/courses/health-sciences-and-technology/hst-723j-neural-coding-and-perception-of-sound-spring-2005/index.htm" rel="noopener noreferrer">Misc from MIT's 'Neural Coding and Perception of Sound' course</a></li>
</ul>

<ul>
<li><a href="http://www.datasciencecentral.com/profiles/blogs/implementing-a-distributed-deep-learning-network-over-spark" rel="noopener noreferrer">Implementing a Distributed Deep Learning Network over Spark</a></li>
</ul>

<ul>
<li><a href="https://github.com/erikbern/deep-pink" rel="noopener noreferrer">A chess AI that learns to play chess using deep learning. (‚≠ê821)</a></li>
</ul>

<ul>
<li><a href="https://github.com/kristjankorjus/Replicating-DeepMind" rel="noopener noreferrer">Reproducing the results of "Playing Atari with Deep Reinforcement Learning" by DeepMind (‚≠ê654)</a></li>
</ul>

<ul>
<li><a href="https://github.com/idio/wiki2vec" rel="noopener noreferrer">Wiki2Vec. Getting Word2vec vectors for entities and word from Wikipedia Dumps (‚≠ê600)</a></li>
</ul>

<ul>
<li><a href="https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner" rel="noopener noreferrer">The original code from the DeepMind article + tweaks (‚≠ê1.8k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/google/deepdream" rel="noopener noreferrer">Google deepdream - Neural Network art (‚≠ê13k)</a></li>
</ul>

<ul>
<li><a href="https://gist.github.com/karpathy/587454dc0146a6ae21fc" rel="noopener noreferrer">An efficient, batched LSTM.</a></li>
</ul>

<ul>
<li><a href="https://github.com/hexahedria/biaxial-rnn-music-composition" rel="noopener noreferrer">A recurrent neural network designed to generate classical music. (‚≠ê1.9k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/facebook/MemNN" rel="noopener noreferrer">Memory Networks Implementations - Facebook (‚≠ê1.8k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/cmusatyalab/openface" rel="noopener noreferrer">Face recognition with Google's FaceNet deep neural network. (‚≠ê15k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/joeledenberg/DigitRecognition" rel="noopener noreferrer">Basic digit recognition neural network (‚≠ê96)</a></li>
</ul>

<ul>
<li><a href="https://www.projectoxford.ai/demo/emotion#detection" rel="noopener noreferrer">Emotion Recognition API Demo - Microsoft</a></li>
</ul>

<ul>
<li><a href="https://github.com/ethereon/caffe-tensorflow" rel="noopener noreferrer">Proof of concept for loading Caffe models in TensorFlow (‚≠ê2.8k)</a></li>
</ul>

<ul>
<li><a href="http://pjreddie.com/darknet/yolo/#webcam" rel="noopener noreferrer">YOLO: Real-Time Object Detection</a></li>
</ul>

<ul>
<li><a href="https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/" rel="noopener noreferrer">YOLO: Practical Implementation using Python</a></li>
</ul>

<ul>
<li><a href="https://github.com/Rochester-NRT/AlphaGo" rel="noopener noreferrer">AlphaGo - A replication of DeepMind's 2016 Nature publication, "Mastering the game of Go with deep neural networks and tree search"</a></li>
</ul>

<ul>
<li><a href="https://github.com/ZuzooVn/machine-learning-for-software-engineers" rel="noopener noreferrer">Machine Learning for Software Engineers (‚≠ê29k)</a></li>
</ul>

<ul>
<li><a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.oa4rzez3g" rel="noopener noreferrer">Machine Learning is Fun!</a></li>
</ul>

<ul>
<li><a href="https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A" rel="noopener noreferrer">Siraj Raval's Deep Learning tutorials</a></li>
</ul>

<ul>
<li><a href="https://github.com/natanielruiz/dockerface" rel="noopener noreferrer">Dockerface (‚≠ê191)</a> - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container.</li>
</ul>

<ul>
<li><a href="https://github.com/ybayle/awesome-deep-learning-music" rel="noopener noreferrer">Awesome Deep Learning Music (‚≠ê2.9k)</a> - Curated list of articles related to deep learning scientific research applied to music</li>
</ul>

<ul>
<li><a href="https://github.com/benedekrozemberczki/awesome-graph-embedding" rel="noopener noreferrer">Awesome Graph Embedding (‚≠ê4.8k)</a> - Curated list of articles related to deep learning scientific research on graph structured data at the graph level.</li>
</ul>

<ul>
<li><a href="https://github.com/chihming/awesome-network-embedding" rel="noopener noreferrer">Awesome Network Embedding (‚≠ê2.6k)</a> - Curated list of articles related to deep learning scientific research on graph structured data at the node level.</li>
</ul>

<ul>
<li><a href="https://github.com/Microsoft/Recommenders" rel="noopener noreferrer">Microsoft Recommenders (‚≠ê20k)</a> contains examples, utilities and best practices for building recommendation systems. Implementations of several state-of-the-art algorithms are provided for self-study and customization in your own applications.</li>
</ul>

<ul>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener noreferrer">The Unreasonable Effectiveness of Recurrent Neural Networks</a> - Andrej Karpathy blog post about using RNN for generating text.</li>
</ul>

<ul>
<li><a href="https://github.com/divamgupta/ladder_network_keras" rel="noopener noreferrer">Ladder Network (‚≠ê100)</a> - Keras Implementation of Ladder Network for Semi-Supervised Learning</li>
</ul>

<ul>
<li><a href="https://github.com/amitness/toolbox" rel="noopener noreferrer">toolbox: Curated list of ML libraries</a></li>
</ul>

<ul>
<li><a href="https://poloclub.github.io/cnn-explainer/" rel="noopener noreferrer">CNN Explainer</a></li>
</ul>

<ul>
<li><a href="https://github.com/AMAI-GmbH/AI-Expert-Roadmap" rel="noopener noreferrer">AI Expert Roadmap (‚≠ê30k)</a> - Roadmap to becoming an Artificial Intelligence Expert</li>
</ul>

<ul>
<li><a href="https://github.com/AstraZeneca/awesome-polipharmacy-side-effect-prediction/" rel="noopener noreferrer">Awesome Drug Interactions, Synergy, and Polypharmacy Prediction (‚≠ê96)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2022/30/">Jul 25 - Jul 31, 2022</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="https://www.manning.com/books/regularization-in-deep-learning" rel="noopener noreferrer">Regularization in Deep Learning</a> - by Liu Peng</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2022/28/">Jul 11 - Jul 17, 2022</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://aws.training/machinelearning" rel="noopener noreferrer">AWS Machine Learning</a> Machine Learning and Deep Learning Courses from Amazon's Machine Learning university</li>
</ul>
<h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="http://www.icg.tu-graz.ac.at/~schindler/Data" rel="noopener noreferrer">ICG Testhouse sequence</a> -  2 turntable sequences from different viewing heights, 36 images each, resolution 1000x750, color (Formats: PPM)</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2022/22/">May 30 - Jun 05, 2022</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="https://www.manning.com/books/deep-learning-with-r-second-edition" rel="noopener noreferrer">Deep Learning with R, Second Edition</a> - by Fran√ßois Chollet with Tomasz Kalinowski and J. J. Allaire</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2022/18/">May 02 - May 08, 2022</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="http://aishelf.org/category/ia/deep-learning/" rel="noopener noreferrer">Deep Learning A.I.Shelf</a></li>
</ul>
<h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="http://proceedings.mlr.press/v37/ioffe15.pdf" rel="noopener noreferrer">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
</ul>

<ul>
<li><a href="http://geometrylearning.com/paper/DeepFaceDrawing.pdf?fbclid=IwAR0colWFHPGBCB1APZq9JVsWeWtmeZd9oCTNQvR52T5PRUJP_dLOwB8pt0I" rel="noopener noreferrer">DeepFaceDrawing: Deep Generation of Face Images from Sketches</a></li>
</ul>
<h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="https://data.mendeley.com/datasets/57zpx667y9/2" rel="noopener noreferrer">SANAD: Single-Label Arabic News Articles Dataset for Automatic Text Categorization</a> - SANAD Dataset is a large collection of Arabic news articles that can be used in different Arabic NLP tasks such as Text Classification and Word Embedding. The articles were collected using Python scripts written specifically for three popular news websites: AlKhaleej, AlArabiya and Akhbarona.</li>
</ul>

<ul>
<li><a href="https://referit3d.github.io" rel="noopener noreferrer">Referit3D</a> - Two large-scale and complementary visio-linguistic datasets (aka Nr3D and Sr3D) for identifying fine-grained 3D objects in ScanNet scenes. Nr3D contains 41.5K natural, free-form utterances, and Sr3d contains 83.5K template-based utterances.</li>
</ul>

<ul>
<li><a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener noreferrer">SQuAD</a> - Stanford released ~100,000 English QA pairs and ~50,000 unanswerable questions</li>
</ul>

<ul>
<li><a href="https://fquad.illuin.tech/" rel="noopener noreferrer">FQuAD</a> - ~25,000 French QA pairs released by Illuin Technology</li>
</ul>

<ul>
<li><a href="https://www.deepset.ai/germanquad" rel="noopener noreferrer">GermanQuAD and GermanDPR</a> - deepset released ~14,000 German QA pairs</li>
</ul>

<ul>
<li><a href="https://github.com/annnyway/QA-for-Russian" rel="noopener noreferrer">SberQuAD (‚≠ê2)</a> - Sberbank released ~90,000 Russian QA pairs</li>
</ul>

<ul>
<li><a href="http://artemisdataset.org/" rel="noopener noreferrer">ArtEmis</a> - Contains 450K affective annotations of emotional responses and linguistic explanations for 80,000 artworks of WikiArt.</li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://haystack.deepset.ai/docs/intromd" rel="noopener noreferrer">haystack: an open-source neural search framework</a></li>
</ul>

<ul>
<li><a href="https://github.com/enlite-ai/maze" rel="noopener noreferrer">Maze (‚≠ê277)</a> - Application-oriented deep reinforcement learning framework addressing real-world decision problems.</li>
</ul>

<ul>
<li><a href="https://github.com/chncwang/InsNet" rel="noopener noreferrer">InsNet - A neural network library for building instance-dependent NLP models with padding-free dynamic batching (‚≠ê66)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2022/17/">Apr 25 - May 01, 2022</a></h2><h3><p>Researchers / Tools</p>
</h3>
<ul>
<li><a href="https://github.com/nebuly-ai/nebullvm" rel="noopener noreferrer">Nebullvm (‚≠ê8.4k)</a> - Easy-to-use library to boost deep learning inference leveraging multiple deep learning compilers.</li>
</ul>

<ul>
<li><a href="https://github.com/lutzroeder/netron" rel="noopener noreferrer">Netron (‚≠ê31k)</a> - Visualizer for deep learning and machine learning models</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2022/11/">Mar 14 - Mar 20, 2022</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/hpcaitech/ColossalAI" rel="noopener noreferrer">Colossal-AI - An Integrated Large-scale Model Training System with Efficient Parallelization Techniques (‚≠ê41k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2022/10/">Mar 07 - Mar 13, 2022</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="https://www.manning.com/books/engineering-deep-learning-platforms" rel="noopener noreferrer">Engineering Deep Learning Platforms</a> - by Chi Wang and Donald Szeto</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2022/5/">Jan 31 - Feb 06, 2022</a></h2><h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="https://allainews.com/" rel="noopener noreferrer">all AI news</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2022/2/">Jan 10 - Jan 16, 2022</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://cds.nyu.edu/deep-learning/" rel="noopener noreferrer">Yann LeCun‚Äôs Deep Learning Course at CDS</a> - DS-GA 1008 ¬∑ SPRING 2021</li>
</ul>

<ul>
<li><a href="https://webcms3.cse.unsw.edu.au/COMP9444/19T3/" rel="noopener noreferrer">Neural Networks and Deep Learning</a> - COMP9444 19T3</li>
</ul>
<h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="https://arxiv.org/pdf/2106.13112.pdf" rel="noopener noreferrer">VOLO: Vision Outlooker for Visual Recognition</a></li>
</ul>

<ul>
<li><a href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener noreferrer">ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2021/50/">Dec 13 - Dec 19, 2021</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="https://www.manning.com/books/evolutionary-deep-learning" rel="noopener noreferrer">Evolutionary Deep Learning</a> - by Micheal Lanham</li>
</ul>
<h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="https://youtu.be/LXWSE_9gHd0" rel="noopener noreferrer">What is Neural Structured Learning by Andrew Ferlitsch</a></li>
</ul>

<ul>
<li><a href="https://youtu.be/_DaviS6K0Vc" rel="noopener noreferrer">Deep Learning Design Patterns by Andrew Ferlitsch</a></li>
</ul>

<ul>
<li><a href="https://youtu.be/QCGSS3kyGo0" rel="noopener noreferrer">Architecture of a Modern CNN: the design pattern approach by Andrew Ferlitsch</a></li>
</ul>

<ul>
<li><a href="https://youtu.be/K1PLeggQ33I" rel="noopener noreferrer">Metaparameters in a CNN by Andrew Ferlitsch</a></li>
</ul>

<ul>
<li><a href="https://youtu.be/dH2nuI-1-qM" rel="noopener noreferrer">Multi-task CNN: a real-world example by Andrew Ferlitsch</a></li>
</ul>

<ul>
<li><a href="https://youtu.be/1FyAh07jh0o" rel="noopener noreferrer">A friendly introduction to deep reinforcement learning by Luis Serrano</a></li>
</ul>

<ul>
<li><a href="https://youtu.be/f6ivp84qFUc" rel="noopener noreferrer">What are GANs and how do they work? by Edward Raff</a></li>
</ul>

<ul>
<li><a href="https://youtu.be/7VRdaqMDalQ" rel="noopener noreferrer">Coding a basic WGAN in PyTorch by Edward Raff</a></li>
</ul>

<ul>
<li><a href="https://youtu.be/8TMT-gHlj_Q" rel="noopener noreferrer">Training a Reinforcement Learning Agent by Miguel Morales</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2021/43/">Oct 25 - Oct 31, 2021</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="https://www.manning.com/books/grokking-deep-learning-for-computer-vision" rel="noopener noreferrer">Grokking Deep Learning for Computer Vision</a></li>
</ul>

<ul>
<li><a href="https://www.manning.com/books/deep-learning-patterns-and-practices" rel="noopener noreferrer">Deep Learning Patterns and Practices</a> - by Andrew Ferlitsch</li>
</ul>

<ul>
<li><a href="https://www.manning.com/books/inside-deep-learning" rel="noopener noreferrer">Inside Deep Learning</a> - by Edward Raff</li>
</ul>

<ul>
<li><a href="https://www.manning.com/books/deep-learning-with-python-second-edition" rel="noopener noreferrer">Deep Learning with Python, Second Edition</a> - by Fran√ßois Chollet</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2021/38/">Sep 20 - Sep 26, 2021</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="https://www.manning.com/books/deep-learning-for-natural-language-processing" rel="noopener noreferrer">Deep Learning for Natural Language Processing</a> - by Stephan Raaijmakers</li>
</ul>
<h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="https://github.com/bupt-ai-cz/LLVIP" rel="noopener noreferrer">LLVIP (‚≠ê741)</a> - 15488 visible-infrared paired images (30976 images) for low-light vision research, <a href="https://bupt-ai-cz.github.io/LLVIP/" rel="noopener noreferrer">Project_Page</a></li>
</ul>

<ul>
<li><a href="https://github.com/bupt-ai-cz/Meta-SelfLearning" rel="noopener noreferrer">MSDA (‚≠ê203)</a> - Over over 5 million images from 5 different domains for multi-source ocr/text recognition DA research, <a href="https://bupt-ai-cz.github.io/Meta-SelfLearning/" rel="noopener noreferrer">Project_Page</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2021/35/">Aug 30 - Sep 05, 2021</a></h2><h3><p>Researchers / Tools</p>
</h3>
<ul>
<li><a href="https://neptune.ai/" rel="noopener noreferrer">Neptune</a> - Lightweight tool for experiment tracking and results visualization.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2021/31/">Aug 02 - Aug 08, 2021</a></h2><h3><p>Researchers / Tools</p>
</h3>
<ul>
<li><a href="https://www.microsoft.com/en-us/research/project/visual-studio-code-tools-ai/" rel="noopener noreferrer">Visual Studio Tools for AI</a> - Develop, debug and deploy deep learning and AI solutions</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2021/15/">Apr 12 - Apr 18, 2021</a></h2><h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://medium.com/sicara/keras-tutorial-content-based-image-retrieval-convolutional-denoising-autoencoder-dc91450cc511" rel="noopener noreferrer">Keras Tutorial: Content Based Image Retrieval Using a Convolutional Denoising Autoencoder</a></li>
</ul>
<h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="http://statistics.ucla.edu/" rel="noopener noreferrer">stat.ucla.edu</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2021/13/">Mar 29 - Apr 04, 2021</a></h2><h3><p>Researchers / Tools</p>
</h3>
<ul>
<li><a href="https://github.com/activeloopai/Hub" rel="noopener noreferrer">hub (‚≠ê8.7k)</a> - Fastest unstructured dataset management for TensorFlow/PyTorch by activeloop.ai. Stream &amp; version-control data. Converts large data into single     numpy-like array on the cloud, accessible on any machine.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/49/">Dec 07 - Dec 13, 2020</a></h2><h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://github.com/SauravMaheshkar/Trax-Examples/blob/main/NLP/NER%20using%20Reformer.ipynb" rel="noopener noreferrer">Named Entity Recognition using Reformers</a></li>
</ul>

<ul>
<li><a href="https://github.com/SauravMaheshkar/Trax-Examples/blob/main/NLP/Deep%20N-Gram.ipynb" rel="noopener noreferrer">Deep N-Gram Models on Shakespeare‚Äôs works</a></li>
</ul>

<ul>
<li><a href="https://github.com/SauravMaheshkar/Trax-Examples/blob/main/vision/illustrated-wideresnet.ipynb" rel="noopener noreferrer">Wide Residual Networks</a></li>
</ul>

<ul>
<li><a href="https://github.com/SauravMaheshkar/Flax-Examples" rel="noopener noreferrer">Fashion MNIST using Flax</a></li>
</ul>

<ul>
<li><a href="https://github.com/SauravMaheshkar/Fake-News-Classification" rel="noopener noreferrer">Fake News Classification (with streamlit deployment)</a></li>
</ul>

<ul>
<li><a href="https://github.com/SauravMaheshkar/CoxPH-Model-for-Primary-Biliary-Cirrhosis" rel="noopener noreferrer">Regression Analysis for Primary Biliary Cirrhosis</a></li>
</ul>

<ul>
<li><a href="https://github.com/SauravMaheshkar/Cross-Matching-Methods-for-Astronomical-Catalogs" rel="noopener noreferrer">Cross Matching Methods for Astronomical Catalogs</a></li>
</ul>

<ul>
<li><a href="https://github.com/SauravMaheshkar/Named-Entity-Recognition-" rel="noopener noreferrer">Named Entity Recognition using BiDirectional LSTMs</a></li>
</ul>

<ul>
<li><a href="https://github.com/SauravMaheshkar/Flutter_Image-Recognition" rel="noopener noreferrer">Image Recognition App using Tflite and Flutter</a></li>
</ul>
<h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="https://theepiccode.com/" rel="noopener noreferrer">The Epic Code</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/google/trax" rel="noopener noreferrer">Trax ‚Äî Deep Learning with Clear Code and Speed (‚≠ê8.2k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/google/flax" rel="noopener noreferrer">Flax - a neural network ecosystem for JAX that is designed for flexibility (‚≠ê6.7k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/Quick-AI/quickvision" rel="noopener noreferrer">QuickVision (‚≠ê52)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/48/">Nov 30 - Dec 06, 2020</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://www.udacity.com/course/deep-learning-pytorch--ud188" rel="noopener noreferrer">Intro to Deep Learning with PyTorch</a> - A great introductory course on Deep Learning by Udacity and Facebook AI</li>
</ul>

<ul>
<li><a href="https://www.kaggle.com/learn/deep-learning" rel="noopener noreferrer">Deep Learning by Kaggle</a> - Kaggle's  free course on Deep Learning</li>
</ul>
<h3><p>Researchers / Tools</p>
</h3>
<ul>
<li><a href="https://dagshub.com/" rel="noopener noreferrer">DAGsHub</a> - Community platform for Open Source ML ‚Äì Manage experiments, data &amp; models and create collaborative ML projects easily.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/45/">Nov 09 - Nov 15, 2020</a></h2><h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://www.manning.com/liveproject/semi-supervised-deep-learning-with-gans-for-melanoma-detection/" rel="noopener noreferrer">Semi-Supervised Deep Learning with GANs for Melanoma Detection</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/43/">Oct 26 - Nov 01, 2020</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/lightly-ai/lightly" rel="noopener noreferrer">lightly - A computer vision framework for self-supervised learning (‚≠ê3.5k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/42/">Oct 19 - Oct 25, 2020</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="https://www.manning.com/books/tensorflow-in-action" rel="noopener noreferrer">TensorFlow 2.0 in Action</a> - by Thushan Ganegedara</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/40/">Oct 05 - Oct 11, 2020</a></h2><h3><p>Researchers / Tutorials</p>
</h3>
<ul>
<li><a href="https://karpathy.ai/" rel="noopener noreferrer"> Andrej Karpathy </a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/39/">Sep 28 - Oct 04, 2020</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="https://www.manning.com/books/math-and-architectures-of-deep-learning" rel="noopener noreferrer">Math and Architectures of Deep Learning</a> - by Krishnendu Chaudhury</li>
</ul>
<h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU" rel="noopener noreferrer">Machine Learning CS 229</a> : End part focuses on deep learning By Andrew Ng</li>
</ul>
<h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="http://ahmedbesbes.com" rel="noopener noreferrer">ahmedbesbes.com</a></li>
</ul>

<ul>
<li><a href="https://www.catalyzeX.com" rel="noopener noreferrer">CatalyzeX: Machine Learning Hub for Builders and Makers</a></li>
</ul>
<h3><p>Researchers / Tools</p>
</h3>
<ul>
<li><a href="https://github.com/determined-ai/determined" rel="noopener noreferrer">Determined (‚≠ê3.2k)</a> - Deep learning training platform with integrated support for distributed training, hyperparameter tuning, smart GPU scheduling, experiment tracking, and a model registry.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/32/">Aug 10 - Aug 16, 2020</a></h2><h3><p>Researchers / Tools</p>
</h3>
<ul>
<li><a href="https://chrome.google.com/webstore/detail/code-finder-for-research/aikkeehnlfpamidigaffhfmgbkdeheil" rel="noopener noreferrer">CatalyzeX</a> - Browser extension (<a href="https://chrome.google.com/webstore/detail/code-finder-for-research/aikkeehnlfpamidigaffhfmgbkdeheil" rel="noopener noreferrer">Chrome</a> and <a href="https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/" rel="noopener noreferrer">Firefox</a>) that automatically finds and links to code implementations for ML papers anywhere online: Google, Twitter, Arxiv, Scholar, etc.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/30/">Jul 27 - Aug 02, 2020</a></h2><h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLheiZMDg_8ufxEx9cNVcOYXsT3BppJP4b" rel="noopener noreferrer">Medical Imaging with Deep Learning Tutorial</a>: This tutorial is styled as a graduate lecture about medical imaging with deep learning. This will cover the background of popular medical image domains (chest X-ray and histology) as well as methods to tackle multi-modality/view, segmentation, and counting tasks.</li>
</ul>
<h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="https://aihub.org/" rel="noopener noreferrer">AI Hub - supported by AAAI, NeurIPS</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/29/">Jul 20 - Jul 26, 2020</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/gojek/feast" rel="noopener noreferrer">PyTorch Geometric Temporal - Representation learning on dynamic graphs (‚≠ê6.2k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/27/">Jul 06 - Jul 12, 2020</a></h2><h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF" rel="noopener noreferrer">Deepmind x UCL Deeplearning</a>: 2020 version</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb" rel="noopener noreferrer">Deepmind x UCL Reinforcement Learning</a>: Deep Reinforcement Learning</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/playlist?list=PLp-0K3kfddPzCnS4CqKphh-zT3aDwybDe" rel="noopener noreferrer">CMU 11-785 Intro to Deep learning Spring 2020</a> Course: 11-785, Intro to Deep Learning by Bhiksha Raj</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/24/">Jun 15 - Jun 21, 2020</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="https://www.oreilly.com/library/view/practical-deep-learning/9781492034858/" rel="noopener noreferrer">Practical Deep Learning for Cloud, Mobile, and Edge</a> - A book for optimization techniques during production.</li>
</ul>
<h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://classpert.com/deep-learning" rel="noopener noreferrer">Deep Learning Online Course list at Classpert</a> List of Deep Learning online courses (some are free) from Classpert Online Course Search</li>
</ul>
<h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="https://arxiv.org/pdf/1503.03832.pdf" rel="noopener noreferrer">FaceNet: A Unified Embedding for Face Recognition and Clustering</a></li>
</ul>

<ul>
<li><a href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" rel="noopener noreferrer">Siamese Neural Networks for One-shot Image Recognition</a></li>
</ul>

<ul>
<li><a href="https://arxiv.org/pdf/2006.03511.pdf" rel="noopener noreferrer">Unsupervised Translation of Programming Languages</a></li>
</ul>

<ul>
<li><a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf" rel="noopener noreferrer">Matching Networks for One Shot Learning</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/22/">Jun 01 - Jun 07, 2020</a></h2><h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="https://vismod.media.mit.edu/vismod/imagery/VisionTexture/vistex.html" rel="noopener noreferrer">MIT Vision Texture</a> - Image archive (100+ images) (Formats: ppm)</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/12/">Mar 23 - Mar 29, 2020</a></h2><h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="https://theaisummer.com/" rel="noopener noreferrer">AI Summer</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/10/">Mar 09 - Mar 15, 2020</a></h2><h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/" rel="noopener noreferrer">The Illustrated Self-Supervised Learning</a></li>
</ul>

<ul>
<li><a href="https://amitness.com/2020/02/albert-visual-summary/" rel="noopener noreferrer">Visual Paper Summary: ALBERT (A Lite BERT)</a></li>
</ul>
<h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="https://amitness.com/" rel="noopener noreferrer">amitness.com</a></li>
</ul>
<h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="http://biolab.csr.unibo.it/home.asp" rel="noopener noreferrer">Biometric Systems Lab</a> - University of Bologna</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/9/">Mar 02 - Mar 08, 2020</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/logicalclocks/hopsworks" rel="noopener noreferrer">Hopsworks - A Feature Store for ML and Data-Intensive AI (‚≠ê1.2k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/gojek/feast" rel="noopener noreferrer">Feast - A Feature Store for ML for GCP by Gojek/Google (‚≠ê6.2k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/8/">Feb 24 - Mar 01, 2020</a></h2><h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="https://arxiv.org/pdf/1406.2661v1.pdf" rel="noopener noreferrer">Generative Adversarial Nets</a></li>
</ul>

<ul>
<li><a href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener noreferrer">Fast R-CNN</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/5/">Feb 03 - Feb 09, 2020</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="https://d2l.ai/" rel="noopener noreferrer">Dive into Deep Learning</a> - numpy based interactive Deep Learning book</li>
</ul>
<h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://www.manning.com/livevideo/machine-learning-for-mere-mortals" rel="noopener noreferrer">Machine Learning for Mere Mortals video course</a> by Nick Chase</li>
</ul>

<ul>
<li><a href="https://developers.google.com/machine-learning/crash-course/" rel="noopener noreferrer">Machine Learning Crash Course with TensorFlow APIs</a> -Google AI</li>
</ul>

<ul>
<li><a href="https://course.fast.ai/part2" rel="noopener noreferrer">Deep Learning from the Foundations</a> Jeremy Howard - Fast.ai</li>
</ul>

<ul>
<li><a href="https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893" rel="noopener noreferrer">Deep Reinforcement Learning (nanodegree) - Udacity</a> a 3-6 month Udacity nanodegree, spanning multiple courses (2018)</li>
</ul>

<ul>
<li><a href="https://www.manning.com/livevideo/grokking-deep-learning-in-motion" rel="noopener noreferrer">Grokking Deep Learning in Motion</a> by Beau Carnes (2018)</li>
</ul>

<ul>
<li><a href="https://www.udemy.com/share/1000gAA0QdcV9aQng=/" rel="noopener noreferrer">Face Detection with Computer Vision and Deep Learning</a> by Hakan Cebeci</li>
</ul>
<h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="https://www.manning.com/livevideo/deep-learning-with-r-in-motion" rel="noopener noreferrer">Deep Learning with R in Motion</a>: a live video course that teaches how to apply deep learning to text and images using the powerful Keras library and its R language interface.</li>
</ul>
<h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://github.com/MelAbgrall/HardwareforAI" rel="noopener noreferrer">Hardware for AI: Understanding computer hardware &amp; build your own computer (‚≠ê9)</a></li>
</ul>
<h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="https://hackr.io/tutorials/learn-artificial-intelligence-ai" rel="noopener noreferrer">Programming Community Curated Resources</a></li>
</ul>

<ul>
<li><a href="https://machinelearningmastery.com/blog/" rel="noopener noreferrer">Machine Learning Mastery blog</a></li>
</ul>

<ul>
<li><a href="https://ml-compiled.readthedocs.io/en/latest/" rel="noopener noreferrer">ML Compiled</a></li>
</ul>

<ul>
<li><a href="https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/" rel="noopener noreferrer">A Beginner's Guide To Understanding Convolutional Neural Networks</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/catalyst-team/catalyst" rel="noopener noreferrer">Catalyst: High-level utils for PyTorch DL &amp; RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing (‚≠ê3.4k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/rlworkgroup/garage" rel="noopener noreferrer">garage - A toolkit for reproducible reinforcement learning research (‚≠ê2k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/alankbi/detecto" rel="noopener noreferrer">Detecto - Train and run object detection models with 5-10 lines of code (‚≠ê620)</a></li>
</ul>

<ul>
<li><a href="https://github.com/benedekrozemberczki/karateclub" rel="noopener noreferrer">Karate Club - An unsupervised machine learning library for graph structured data (‚≠ê2.2k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/mrdimosthenis/Synapses" rel="noopener noreferrer">Synapses - A lightweight library for neural networks that runs anywhere (‚≠ê71)</a></li>
</ul>

<ul>
<li><a href="https://github.com/reinforceio/tensorforce" rel="noopener noreferrer">TensorForce - A TensorFlow library for applied reinforcement learning (‚≠ê3.3k)</a></li>
</ul>
<h3><p>Researchers / Tools</p>
</h3>
<ul>
<li><a href="https://github.com/ml-tooling/ml-workspace" rel="noopener noreferrer">ML Workspace (‚≠ê3.5k)</a> - All-in-one web-based IDE for machine learning and data science.</li>
</ul>

<ul>
<li><a href="https://github.com/rlworkgroup/dowel" rel="noopener noreferrer">dowel (‚≠ê33)</a> - A little logger for machine learning research. Log any object to the console, CSVs, TensorBoard, text log files, and more with just one call to <code>logger.log()</code></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2020/4/">Jan 27 - Feb 02, 2020</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLZSO_6-bSqHQHBCoGaObUljoXAyyqhpFW" rel="noopener noreferrer">Deep Learning - UC Berkeley | STAT-157</a> by Alex Smola and Mu Li (2019)</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2019/50/">Dec 16 - Dec 22, 2019</a></h2><h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="http://www.designrepository.org" rel="noopener noreferrer">National Design Repository</a> - Over 55,000 3D CAD and solid models of (mostly) mechanical/machined engineering designs. (Formats: gif,vrml,wrl,stp,sat)</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2019/47/">Nov 25 - Dec 01, 2019</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/apache/incubator-mxnet" rel="noopener noreferrer">MXnet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning framework (‚≠ê21k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2019/44/">Nov 04 - Nov 10, 2019</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://www.coursera.org/specializations/deep-learning" rel="noopener noreferrer">Deep Learning Specialization - Coursera</a> - Breaking into AI with the best course from Andrew NG.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2019/35/">Sep 02 - Sep 08, 2019</a></h2><h3><p>Researchers / Conferences</p>
</h3>
<ul>
<li><a href="https://montrealaisymposium.wordpress.com/" rel="noopener noreferrer">MAIS - Montreal AI Symposium</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/Neuraxio/Neuraxle" rel="noopener noreferrer">Neuraxle - A general-purpose ML pipelining framework (‚≠ê613)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2019/27/">Jul 08 - Jul 14, 2019</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://introtodeeplearning.com" rel="noopener noreferrer">MIT Intro to Deep Learning 7 day bootcamp</a> - A seven day bootcamp designed in MIT to introduce deep learning methods and applications (2019)</li>
</ul>

<ul>
<li><a href="https://mithi.github.io/deep-blueberry" rel="noopener noreferrer">Deep Blueberry: Deep Learning</a> - A free five-weekend plan to self-learners to learn the basics of deep-learning architectures like CNNs, LSTMs, RNNs, VAEs, GANs, DQN, A3C and more (2019)</li>
</ul>

<ul>
<li><a href="https://spinningup.openai.com/" rel="noopener noreferrer">Spinning Up in Deep Reinforcement Learning</a> - A free deep reinforcement learning course by OpenAI (2019)</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2019/23/">Jun 10 - Jun 16, 2019</a></h2><h3><p>Researchers / Tools</p>
</h3>
<ul>
<li><a href="https://github.com/microsoft/tensorwatch" rel="noopener noreferrer">TensorWatch (‚≠ê3.4k)</a> - Debugging and visualization for deep learning</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2019/10/">Mar 11 - Mar 17, 2019</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://www.deeplearning.ai/ai-for-everyone/" rel="noopener noreferrer">AI for Everyone</a> by Andrew Ng (2019)</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2019/9/">Mar 04 - Mar 10, 2019</a></h2><h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="http://vision.psych.umn.edu/users/kersten//kersten-lab/kersten-lab.html" rel="noopener noreferrer">Univerity of Minnesota Vision Lab</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2019/8/">Feb 25 - Mar 03, 2019</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="http://www.boente.eti.br/fuzzy/ebook-fuzzy-mitchell.pdf" rel="noopener noreferrer">An introduction to genetic algorithms</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2018/48/">Nov 26 - Dec 02, 2018</a></h2><h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="https://www.manning.com/livevideo/deep-learning-crash-course" rel="noopener noreferrer">Deep Learning Crash Course</a> By Oliver Zeigermann</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2018/43/">Oct 22 - Oct 28, 2018</a></h2><h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://ahmedbesbes.com/understanding-deep-convolutional-neural-networks-with-a-practical-use-case-in-tensorflow-and-keras.html" rel="noopener noreferrer">Understanding deep Convolutional Neural Networks with a practical use-case in Tensorflow and Keras</a></li>
</ul>

<ul>
<li><a href="https://ahmedbesbes.com/overview-and-benchmark-of-traditional-and-deep-learning-models-in-text-classification.html" rel="noopener noreferrer">Overview and benchmark of traditional and deep learning models in text classification</a></li>
</ul>
<h3><p>Researchers / Tutorials</p>
</h3>
<ul>
<li><a href="http://www.ayyucekizrak.com/" rel="noopener noreferrer">Merve Ayy√ºce Kƒ±zrak</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2018/39/">Sep 24 - Sep 30, 2018</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/albu/albumentations" rel="noopener noreferrer">albumentations - A fast and framework agnostic image augmentation library (‚≠ê15k)</a></li>
</ul>
<h3><p>Researchers / Tools</p>
</h3>
<ul>
<li><a href="http://jupyter.org" rel="noopener noreferrer">Jupyter Notebook</a> - Web-based notebook environment for interactive computing</li>
</ul>

<ul>
<li><a href="https://github.com/tensorflow/tensorboard" rel="noopener noreferrer">TensorBoard (‚≠ê6.9k)</a> - TensorFlow's Visualization Toolkit</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2018/38/">Sep 17 - Sep 23, 2018</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://tvm.ai/" rel="noopener noreferrer">TVM - End to End Deep Learning Compiler Stack for CPUs, GPUs and specialized accelerators</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2018/24/">Jun 11 - Jun 17, 2018</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/" rel="noopener noreferrer">Artificial intelligence and machine learning: Topic wise explanation</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2018/12/">Mar 19 - Mar 25, 2018</a></h2><h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" rel="noopener noreferrer">Efficient BackProp</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2018/10/">Mar 05 - Mar 11, 2018</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="http://research.microsoft.com/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf" rel="noopener noreferrer">Deep Learning</a> by Microsoft Research (2013)</li>
</ul>
<h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="http://vimeo.com/80821560" rel="noopener noreferrer">Making Sense of the World with Deep Learning</a> By Adam Coates</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/watch?v=wZfVBwOO0-k" rel="noopener noreferrer">Demystifying Unsupervised Feature Learning </a> By Adam Coates</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/watch?v=czLI3oLDe8M" rel="noopener noreferrer">Deep Learning: Intelligence from Big Data</a> by Steve Jurvetson (and panel) at VLAB in Stanford.</li>
</ul>
<h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="https://arxiv.org/pdf/1611.07004v1.pdf" rel="noopener noreferrer">Image-to-Image Translation with Conditional Adversarial Networks</a></li>
</ul>

<ul>
<li><a href="https://arxiv.org/pdf/1611.07004v1.pdf" rel="noopener noreferrer">Berkeley AI Research (BAIR) Laboratory</a></li>
</ul>
<h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="http://www.cs.washington.edu/research/imagedatabase/groundtruth/" rel="noopener noreferrer">Content-based image retrieval database</a> - 11 sets of color images for testing algorithms for content-based retrieval. Most sets have a description file with names of objects in each image. (Formats: jpg)</li>
</ul>

<ul>
<li><a href="http://www-rocq.inria.fr/~tarel/syntim/paires.html" rel="noopener noreferrer">INRIA's Syntim stereo databases</a> - 34 calibrated color stereo pairs (Formats: gif)</li>
</ul>

<ul>
<li><a href="http://www.ece.ncsu.edu/imaging" rel="noopener noreferrer">Image Analysis Laboratory</a></li>
</ul>

<ul>
<li><a href="http://www.mis.atr.co.jp/~mlyons/jaffe.html" rel="noopener noreferrer">JAFFE Facial Expression Image Database</a> - The JAFFE database consists of 213 images of Japanese female subjects posing 6 basic facial expressions as well as a neutral pose. Ratings on emotion adjectives are also available, free of charge, for research purposes. (Formats: TIFF Grayscale images.)</li>
</ul>

<ul>
<li><a href="ftp://ftp.vislist.com/IMAGERY/JISCT/" rel="noopener noreferrer">JISCT Stereo Evaluation</a> - 44 image pairs. These data have been used in an evaluation of stereo analysis, as described in the April 1993 ARPA Image Understanding Workshop paper ``The JISCT Stereo Evaluation'' by R.C.Bolles, H.H.Baker, and M.J.Hannah, 263--274 (Formats: SSI)</li>
</ul>

<ul>
<li><a href="ftp://whitechapel.media.mit.edu/pub/images" rel="noopener noreferrer">MIT face images and more</a> - hundreds of images (Formats: homebrew)</li>
</ul>

<ul>
<li><a href="ftp://ftp.cs.columbia.edu/jpeg/other/uuencoded" rel="noopener noreferrer">NIST Fingerprint data</a> - compressed multipart uuencoded tar file</li>
</ul>

<ul>
<li><a href="http://gicl.mcs.drexel.edu" rel="noopener noreferrer">Geometric &amp; Intelligent Computing Laboratory</a></li>
</ul>

<ul>
<li><a href="http://sampl.eng.ohio-state.edu/~sampl/database.htm" rel="noopener noreferrer">OSU/SAMPL Database: Range Images, 3D Models, Stills, Motion Sequences</a> - Over 1000 range images, 3D object models, still images and motion sequences (Formats: gif, ppm, vrml, homebrew)</li>
</ul>

<ul>
<li><a href="http://www.cs.otago.ac.nz/research/vision/index.html" rel="noopener noreferrer">Vision Research Group</a></li>
</ul>

<ul>
<li><a href="ftp://ftp.limsi.fr/pub/quenot/opflow/testdata/piv/" rel="noopener noreferrer">ftp://ftp.limsi.fr/pub/quenot/opflow/testdata/piv/</a> - Real and synthetic image sequences used for testing a Particle Image Velocimetry application. These images may be used for the test of optical flow and image matching algorithms. (Formats: pgm (raw))</li>
</ul>

<ul>
<li><a href="http://www.limsi.fr/Recherche/IMM/PageIMM.html" rel="noopener noreferrer">LIMSI-CNRS/CHM/IMM/vision</a></li>
</ul>

<ul>
<li><a href="http://www.taurusstudio.net/research/pmtexdb/index.htm" rel="noopener noreferrer">Photometric 3D Surface Texture Database</a> - This is the first 3D texture database which provides both full real surface rotations and registered photometric stereo data (30 textures, 1680 images). (Formats: TIFF)</li>
</ul>

<ul>
<li><a href="http://www.informatik.uni-stuttgart.de/ipvr/bv/bv_home_engl.html" rel="noopener noreferrer">Department Image Understanding</a></li>
</ul>

<ul>
<li><a href="http://www.ee.surrey.ac.uk/Research/CVSSP" rel="noopener noreferrer">Centre for Vision, Speech and Signal Processing</a></li>
</ul>

<ul>
<li><a href="http://i21www.ira.uka.de" rel="noopener noreferrer">IAKS/KOGS</a></li>
</ul>

<ul>
<li><a href="http://www.ee.oulu.fi/~olli/Projects/Lumber.Grading.html" rel="noopener noreferrer">U Oulu wood and knots database</a> - Includes classifications - 1000+ color images (Formats: ppm)</li>
</ul>

<ul>
<li><a href="http://vision.doc.ntu.ac.uk/datasets/UCID/ucid.html" rel="noopener noreferrer">UCID - an Uncompressed Colour Image Database</a> - a benchmark database for image retrieval with predefined ground truth. (Formats: tiff)</li>
</ul>

<ul>
<li><a href="http://cvc.yale.edu/projects/yalefacesB/yalefacesB.html" rel="noopener noreferrer">Yale Face Database B</a> - 5760 single light source images of 10 subjects each seen under 576 viewing conditions (9 poses x 64 illumination conditions). (Formats: PGM)</li>
</ul>

<ul>
<li><a href="https://github.com/zalandoresearch/fashion-mnist" rel="noopener noreferrer">Fashion-MNIST (‚≠ê12k)</a> - MNIST like fashion product dataset consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2018/7/">Feb 12 - Feb 18, 2018</a></h2><h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="https://github.com/several27/FakeNewsCorpus" rel="noopener noreferrer">FakeNewsCorpus (‚≠ê397)</a> - Contains about 10 million news articles classified using <a href="http://opensources.co" rel="noopener noreferrer">opensources.co</a> types</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2018/6/">Feb 05 - Feb 11, 2018</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/NervanaSystems/coach" rel="noopener noreferrer">Coach - Reinforcement Learning Coach by Intel¬Æ AI Lab (‚≠ê2.3k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2018/4/">Jan 22 - Jan 28, 2018</a></h2><h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=oS5fz_mHVz0&amp;list=PLWKotBjTDoLj3rXBL-nEIPRN9V3a9Cx07" rel="noopener noreferrer">Deep Learning Crash Course</a>: a series of mini-lectures by Leo Isikdogan on YouTube (2018)</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/51/">Dec 18 - Dec 24, 2017</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="http://www.deeplearningbook.org/" rel="noopener noreferrer">Deep Learning</a> by Yoshua Bengio, Ian Goodfellow and Aaron Courville  (05/07/2015)</li>
</ul>
<h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm" rel="noopener noreferrer">Designing, Visualizing and Understanding Deep Neural Networks-UC Berkeley</a></li>
</ul>

<ul>
<li><a href="http://uvadlc.github.io" rel="noopener noreferrer">UVA Deep Learning Course</a> MSc in Artificial Intelligence for the University of Amsterdam.</li>
</ul>

<ul>
<li><a href="http://selfdrivingcars.mit.edu/" rel="noopener noreferrer">MIT 6.S094: Deep Learning for Self-Driving Cars</a></li>
</ul>

<ul>
<li><a href="http://introtodeeplearning.com/" rel="noopener noreferrer">MIT 6.S191: Introduction to Deep Learning</a></li>
</ul>

<ul>
<li><a href="http://rll.berkeley.edu/deeprlcourse/" rel="noopener noreferrer">Berkeley CS 294: Deep Reinforcement Learning</a></li>
</ul>

<ul>
<li><a href="https://www.manning.com/livevideo/keras-in-motion" rel="noopener noreferrer">Keras in Motion video course</a></li>
</ul>

<ul>
<li><a href="http://course.fast.ai/" rel="noopener noreferrer">Practical Deep Learning For Coders</a> by Jeremy Howard - Fast.ai</li>
</ul>

<ul>
<li><a href="http://deeplearning.cs.cmu.edu/" rel="noopener noreferrer">Introduction to Deep Learning</a> by Prof. Bhiksha Raj (2017)</li>
</ul>
<h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://www.manning.com/books/deep-learning-with-python" rel="noopener noreferrer">Deep Learning with Python</a></li>
</ul>

<ul>
<li><a href="https://www.manning.com/books/grokking-deep-learning" rel="noopener noreferrer">Grokking Deep Learning</a></li>
</ul>

<ul>
<li><a href="https://www.manning.com/books/deep-learning-for-search" rel="noopener noreferrer">Deep Learning for Search</a></li>
</ul>

<ul>
<li><a href="https://github.com/yunjey/pytorch-tutorial" rel="noopener noreferrer">Pytorch Tutorial by Yunjey Choi (‚≠ê31k)</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/caffe2/caffe2" rel="noopener noreferrer">Caffe2 - A New Lightweight, Modular, and Scalable Deep Learning Framework (‚≠ê8.4k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/PAIR-code/deeplearnjs" rel="noopener noreferrer">deeplearn.js - Hardware-accelerated deep learning and linear algebra (NumPy) library for the web (‚≠ê8.5k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/49/">Dec 04 - Dec 10, 2017</a></h2><h3><p>Researchers / Conferences</p>
</h3>
<ul>
<li><a href="http://cvpr2018.thecvf.com" rel="noopener noreferrer">CVPR - IEEE Conference on Computer Vision and Pattern Recognition</a></li>
</ul>

<ul>
<li><a href="http://celweb.vuse.vanderbilt.edu/aamas18/" rel="noopener noreferrer">AAMAS - International Joint Conference on Autonomous Agents and Multiagent Systems</a></li>
</ul>

<ul>
<li><a href="https://www.ijcai-18.org/" rel="noopener noreferrer">IJCAI -     International Joint Conference on Artificial Intelligence</a></li>
</ul>

<ul>
<li><a href="https://icml.cc" rel="noopener noreferrer">ICML -     International Conference on Machine Learning</a></li>
</ul>

<ul>
<li><a href="http://www.ecmlpkdd2018.org" rel="noopener noreferrer">ECML - European Conference on Machine Learning</a></li>
</ul>

<ul>
<li><a href="http://www.kdd.org/kdd2018/" rel="noopener noreferrer">KDD - Knowledge Discovery and Data Mining</a></li>
</ul>

<ul>
<li><a href="https://nips.cc/Conferences/2018" rel="noopener noreferrer">NIPS - Neural Information Processing Systems</a></li>
</ul>

<ul>
<li><a href="https://conferences.oreilly.com/artificial-intelligence/ai-ny" rel="noopener noreferrer">O'Reilly AI Conference -     O'Reilly Artificial Intelligence Conference</a></li>
</ul>

<ul>
<li><a href="https://www.waset.org/conference/2018/07/istanbul/ICDM" rel="noopener noreferrer">ICDM - International Conference on Data Mining</a></li>
</ul>

<ul>
<li><a href="http://iccv2017.thecvf.com" rel="noopener noreferrer">ICCV - International Conference on Computer Vision</a></li>
</ul>

<ul>
<li><a href="https://www.aaai.org" rel="noopener noreferrer">AAAI - Association for the Advancement of Artificial Intelligence</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/44/">Oct 30 - Nov 05, 2017</a></h2><h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="https://arxiv.org/abs/1710.09829" rel="noopener noreferrer">Dynamic Routing Between Capsules</a></li>
</ul>

<ul>
<li><a href="https://openreview.net/pdf?id=HJWLfGWRb" rel="noopener noreferrer">Matrix Capsules With Em Routing</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/42/">Oct 16 - Oct 22, 2017</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="http://vision.stanford.edu/teaching/cs231n/syllabus.html" rel="noopener noreferrer">Convolutional Neural Networks for Visual Recognition - Stanford</a> by Fei-Fei Li, Andrej Karpathy (2017)</li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/SerpentAI/SerpentAI" rel="noopener noreferrer">Serpent.AI - Game agent framework: Use any video game as a deep learning sandbox (‚≠ê6.9k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/41/">Oct 09 - Oct 15, 2017</a></h2><h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="http://yerevann.com/a-guide-to-deep-learning/" rel="noopener noreferrer">Guide to Machine Learning</a></li>
</ul>

<ul>
<li><a href="https://spandan-madan.github.io/DeepLearningProject/" rel="noopener noreferrer">Deep Learning for Beginners</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/36/">Sep 04 - Sep 10, 2017</a></h2><h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit" rel="noopener noreferrer">Visual Object Classes Challenge 2012 (VOC2012)</a> - VOC2012 dataset containing 12k images with 20 annotated classes for object detection and segmentation.</li>
</ul>

<ul>
<li><a href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html" rel="noopener noreferrer">Large-scale Fashion (DeepFashion) Database</a> - Contains over 800,000 diverse fashion images.  Each image in this dataset is labeled with 50 categories, 1,000 descriptive attributes, bounding box and clothing landmarks</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/34/">Aug 21 - Aug 27, 2017</a></h2><h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="noopener noreferrer">Google - Sequence to Sequence  Learning with Neural Networks</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/33/">Aug 14 - Aug 20, 2017</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/Microsoft/CNTK" rel="noopener noreferrer">CNTK - Microsoft Cognitive Toolkit (‚≠ê18k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/30/">Jul 24 - Jul 30, 2017</a></h2><h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="https://arxiv.org/pdf/1512.03385v1.pdf" rel="noopener noreferrer">Residual Learning</a></li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1704.04861" rel="noopener noreferrer">MobileNets by Google</a></li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1706.05739" rel="noopener noreferrer">Cross Audio-Visual Recognition in the Wild Using Deep Learning</a></li>
</ul>
<h3><p>Researchers / Tutorials</p>
</h3>
<ul>
<li><a href="http://ai.stanford.edu/~wzou" rel="noopener noreferrer"> Youzhi (Will) Zou </a></li>
</ul>

<ul>
<li><a href="http://vision.stanford.edu/feifeili" rel="noopener noreferrer"> Fei-Fei Li </a></li>
</ul>

<ul>
<li><a href="https://research.google.com/pubs/105214.html" rel="noopener noreferrer"> Ian Goodfellow </a></li>
</ul>

<ul>
<li><a href="http://www.site.uottawa.ca/~laganier/" rel="noopener noreferrer"> Robert Lagani√®re </a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/28/">Jul 10 - Jul 16, 2017</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLHyI3Fbmv0SdzMHAy0aN59oYnLy5vyyTA" rel="noopener noreferrer">Graduate Summer School: Deep Learning, Feature Learning</a> by Geoffrey Hinton, Yoshua Bengio, Yann LeCun, Andrew Ng, Nando de Freitas and several others @ IPAM, UCLA (2012)</li>
</ul>
<h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="http://www.cs.toronto.edu/~hinton/deeprefs.html" rel="noopener noreferrer">Geoff Hinton's reading list (all papers)</a></li>
</ul>
<h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://github.com/josephmisiti/machine-learning-module" rel="noopener noreferrer">The Best Machine Learning Tutorials On The Web (‚≠ê469)</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/dnouri/nolearn" rel="noopener noreferrer">nolearn - wrappers and abstractions around existing neural network libraries, most notably Lasagne (‚≠ê953)</a></li>
</ul>

<ul>
<li><a href="https://github.com/deepmind/sonnet" rel="noopener noreferrer">Sonnet - a library for constructing neural networks by Google's DeepMind (‚≠ê9.9k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/pytorch/pytorch" rel="noopener noreferrer">PyTorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration (‚≠ê91k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/25/">Jun 19 - Jun 25, 2017</a></h2><h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://github.com/astorfi/TensorFlow-World" rel="noopener noreferrer">TensorFlow-World (‚≠ê4.5k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/14/">Apr 03 - Apr 09, 2017</a></h2><h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="https://web-beta.archive.org/web/20011216051535/vision.psych.umn.edu/www/kersten-lab/demos/digitalembryo.html" rel="noopener noreferrer">Digital Embryos</a> - Digital embryos are novel objects which may be used to develop and test object recognition systems. They have an organic appearance. (Formats: various formats are available on request)</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/10/">Mar 06 - Mar 12, 2017</a></h2><h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="https://nips.cc/Conferences/2016/Schedule" rel="noopener noreferrer">NIPS 2016 lecture and workshop videos</a> - NIPS 2016</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/9/">Feb 27 - Mar 05, 2017</a></h2><h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="http://nlp.stanford.edu/~socherr/pa4_ner.pdf" rel="noopener noreferrer">Neural Networks for Named Entity Recognition</a> <a href="http://nlp.stanford.edu/~socherr/pa4-ner.zip" rel="noopener noreferrer">zip</a></li>
</ul>
<h3><p>Researchers / Tutorials</p>
</h3>
<ul>
<li><a href="http://mil.engr.utk.edu/nmil/member/5.html" rel="noopener noreferrer"> Derek Rose </a></li>
</ul>

<ul>
<li><a href="http://mil.engr.utk.edu/nmil/member/2.html" rel="noopener noreferrer"> Itamar Arel </a></li>
</ul>

<ul>
<li><a href="http://mdenil.com/" rel="noopener noreferrer"> Misha Denil </a></li>
</ul>

<ul>
<li><a href="http://mil.engr.utk.edu/nmil/member/19.html" rel="noopener noreferrer"> Robert Coop </a></li>
</ul>

<ul>
<li><a href="http://mil.engr.utk.edu/nmil/member/36.html" rel="noopener noreferrer"> Tom Karnowski </a></li>
</ul>
<h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="https://medium.com/@ageitgey/" rel="noopener noreferrer">Machine Learning is Fun! Adam Geitgey's Blog</a></li>
</ul>
<h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="https://yahooresearch.tumblr.com/post/89783581601/one-hundred-million-creative-commons-flickr-images" rel="noopener noreferrer">Flickr Data</a> 100 Million Yahoo dataset</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/8/">Feb 20 - Feb 26, 2017</a></h2><h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="http://www2.ece.ohio-state.edu/~aleix/ARdatabase.html" rel="noopener noreferrer">The AR Face Database</a> - Contains over 4,000 color images corresponding to 126 people's faces (70 men and 56 women). Frontal views with variations in facial expressions, illumination, and occlusions. (Formats: RAW (RGB 24-bit))</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2017/6/">Feb 06 - Feb 12, 2017</a></h2><h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/" rel="noopener noreferrer">Using convolutional neural nets to detect facial keypoints tutorial</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/Lasagne/Lasagne" rel="noopener noreferrer">Lasagne - a lightweight library to build and train neural networks in Theano (‚≠ê3.9k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/50/">Dec 12 - Dec 18, 2016</a></h2><h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition" rel="noopener noreferrer">Classification on raw time series in TensorFlow with a LSTM RNN (‚≠ê3.4k)</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="http://neupy.com" rel="noopener noreferrer">NeuPy - Theano based Python library for ANN and Deep Learning</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/49/">Dec 05 - Dec 11, 2016</a></h2><h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://github.com/clementfarabet/ipam-tutorials/tree/master/th_tutorials" rel="noopener noreferrer">Torch7 Tutorials (‚≠ê129)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/43/">Oct 24 - Oct 30, 2016</a></h2><h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="https://research.google.com/youtube8m/" rel="noopener noreferrer">YouTube-8M Dataset</a> - YouTube-8M is a large-scale labeled video dataset that consists of 8 million YouTube video IDs and associated labels from a diverse vocabulary of 4800 visual entities.</li>
</ul>

<ul>
<li><a href="https://github.com/openimages/dataset" rel="noopener noreferrer">Open Images dataset (‚≠ê4.3k)</a> - Open Images is a dataset of ~9 million URLs to images that have been annotated with labels spanning over 6000 categories.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/35/">Aug 29 - Sep 04, 2016</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/baidu/paddle" rel="noopener noreferrer">Paddle - PArallel Distributed Deep LEarning by Baidu (‚≠ê23k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/34/">Aug 22 - Aug 28, 2016</a></h2><h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=FoO8qDB8gUU" rel="noopener noreferrer">Introduction to Artificial Neural Networks and Deep Learning</a> by Leo Isikdogan at Motorola Mobility HQ</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/33/">Aug 15 - Aug 21, 2016</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://www.college-de-france.fr/site/en-yann-lecun/course-2015-2016.htm" rel="noopener noreferrer">Deep Learning Course</a> by Yann LeCun (2016)</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/32/">Aug 08 - Aug 14, 2016</a></h2><h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="http://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener noreferrer">CIFAR-10 and CIFAR-100</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/denizyuret/Knet.jl" rel="noopener noreferrer">Knet.jl (‚≠ê1.4k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/27/">Jul 04 - Jul 10, 2016</a></h2><h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="http://vis-www.cs.umass.edu/~vislib/" rel="noopener noreferrer">UMass Vision Image Archive</a> - Large image database with aerial, space, stereo, medical images and more. (Formats: homebrew)</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/25/">Jun 20 - Jun 26, 2016</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=azaLcvuql_g&amp;list=PLjbUi5mgii6BWEUZf7He6nowWvGne_Y8r" rel="noopener noreferrer">Statistical Machine Learning - CMU</a> by Prof. Larry Wasserman</li>
</ul>
<h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="https://arxiv.org/abs/1502.03167" rel="noopener noreferrer">Batch Normalization</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/torchnet/torchnet" rel="noopener noreferrer">Torchnet - Torch based Deep Learning Library (‚≠ê994)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/21/">May 23 - May 29, 2016</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="http://mlpack.org/" rel="noopener noreferrer">mlpack - A scalable Machine Learning library</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/20/">May 16 - May 22, 2016</a></h2><h3><p>Researchers / Tutorials</p>
</h3>
<ul>
<li><a href="http://cs.stanford.edu/~acoates/" rel="noopener noreferrer">Adam Coates</a></li>
</ul>

<ul>
<li><a href="http://serre-lab.clps.brown.edu/person/david-reichert/" rel="noopener noreferrer"> David Reichert </a></li>
</ul>

<ul>
<li><a href="https://sites.google.com/site/blancousna/" rel="noopener noreferrer"> Justin A. Blanco </a></li>
</ul>

<ul>
<li><a href="http://ludovicarnold.altervista.org/home/" rel="noopener noreferrer"> Ludovic Arnold </a></li>
</ul>

<ul>
<li><a href="https://sites.google.com/site/drpngx/" rel="noopener noreferrer"> Patrick Nguyen </a></li>
</ul>

<ul>
<li><a href="http://cs.nyu.edu/~fergus/pmwiki/pmwiki.php" rel="noopener noreferrer"> Rob Fergus </a></li>
</ul>

<ul>
<li><a href="https://research.facebook.com/tomas-mikolov" rel="noopener noreferrer"> Tom√°≈° Mikolov </a></li>
</ul>

<ul>
<li><a href="http://yota.ro/" rel="noopener noreferrer"> Yotaro Kubo </a></li>
</ul>
<h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="https://github.com/deepmind/rc-data" rel="noopener noreferrer">DeepMind QA Corpus (‚≠ê1.3k)</a> - Textual QA corpus from CNN and DailyMail. More than 300K documents in total. <a href="http://arxiv.org/abs/1506.03340" rel="noopener noreferrer">Paper</a> for reference.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/19/">May 09 - May 15, 2016</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://www.udacity.com/course/deep-learning--ud730" rel="noopener noreferrer">Deep Learning - Udacity/Google</a> by Vincent Vanhoucke and Arpan Chakraborty (2016)</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/playlist?list=PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE" rel="noopener noreferrer">Deep Learning - UWaterloo</a> by Prof. Ali Ghodsi at University of Waterloo (2015)</li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/amznlabs/amazon-dsstne" rel="noopener noreferrer">DSSTNE - Amazon's library for building Deep Learning models (‚≠ê4.4k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/tensorflow/models/tree/master/syntaxnet" rel="noopener noreferrer">SyntaxNet - Google's syntactic parser - A TensorFlow dependency library (‚≠ê78k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/17/">Apr 25 - May 01, 2016</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="http://singa.incubator.apache.org/" rel="noopener noreferrer">Apache SINGA - A General Distributed Deep Learning Platform</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/5/">Feb 01 - Feb 07, 2016</a></h2><h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="http://cs224d.stanford.edu/" rel="noopener noreferrer">Deep Learning for Natural Language Processing - Stanford</a></li>
</ul>

<ul>
<li><a href="http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html" rel="noopener noreferrer">Neural Networks - usherbrooke</a></li>
</ul>

<ul>
<li><a href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/" rel="noopener noreferrer">Machine Learning - Oxford</a> (2014-2015)</li>
</ul>

<ul>
<li><a href="https://developer.nvidia.com/deep-learning-courses" rel="noopener noreferrer">Deep Learning - Nvidia</a> (2015)</li>
</ul>
<h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://github.com/pkmital/tensorflow_tutorials" rel="noopener noreferrer">More TensorFlow tutorials (‚≠ê5.7k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/4/">Jan 25 - Jan 31, 2016</a></h2><h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="http://www.nature.com/nature/journal/v529/n7587/pdf/nature16961.pdf" rel="noopener noreferrer">Mastering the Game of Go with Deep Neural Networks and Tree Search</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2016/2/">Jan 11 - Jan 17, 2016</a></h2><h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://github.com/Vict0rSch/deep_learning" rel="noopener noreferrer">Keras and Lasagne Deep Learning Tutorials (‚≠ê424)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/48/">Nov 30 - Dec 06, 2015</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/Samsung/veles" rel="noopener noreferrer">Veles - Samsung Distributed machine learning platform (‚≠ê911)</a></li>
</ul>

<ul>
<li><a href="https://github.com/PrincetonVision/marvin" rel="noopener noreferrer">Marvin - A Minimalist GPU-only N-Dimensional ConvNets Framework (‚≠ê425)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/47/">Nov 23 - Nov 29, 2015</a></h2><h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://github.com/aymericdamien/TensorFlow-Examples" rel="noopener noreferrer">TensorFlow Python Notebooks (‚≠ê44k)</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/google/skflow" rel="noopener noreferrer">Scikit Flow - Simplified interface for TensorFlow (mimicking Scikit Learn) (‚≠ê3.2k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/46/">Nov 16 - Nov 22, 2015</a></h2><h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="https://github.com/nlintz/TensorFlow-Tutorials" rel="noopener noreferrer">TensorFlow tutorials (‚≠ê6k)</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/tensorflow/tensorflow" rel="noopener noreferrer">Tensorflow - Open source software library for numerical computation using data flow graphs (‚≠ê191k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/Microsoft/DMTK" rel="noopener noreferrer">DMTK - Microsoft Distributed Machine Learning Tookit (‚≠ê2.7k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/44/">Nov 02 - Nov 08, 2015</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/IDSIA/brainstorm" rel="noopener noreferrer">Brainstorm - Fast, flexible and fun neural networks. (‚≠ê1.3k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/42/">Oct 19 - Oct 25, 2015</a></h2><h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="http://googleresearch.blogspot.com/2015/09/a-beginners-guide-to-deep-neural.html" rel="noopener noreferrer">A beginners Guide to Deep Neural Networks</a> By Natalie Hammel and Lorraine Yurshansky</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/37/">Sep 14 - Sep 20, 2015</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/dmlc/minerva" rel="noopener noreferrer">Minerva - a fast and flexible tool for deep learning on multi-GPU (‚≠ê702)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/34/">Aug 24 - Aug 30, 2015</a></h2><h3><p>Researchers / Tutorials</p>
</h3>
<ul>
<li><a href="http://aaroncourville.wordpress.com" rel="noopener noreferrer">Aaron Courville</a></li>
</ul>

<ul>
<li><a href="http://www.cs.toronto.edu/~asamir/" rel="noopener noreferrer">Abdel-rahman Mohamed</a></li>
</ul>

<ul>
<li><a href="http://research.microsoft.com/en-us/people/alexac/" rel="noopener noreferrer">Alex Acero</a></li>
</ul>

<ul>
<li><a href="http://www.cs.utoronto.ca/~kriz/index.html" rel="noopener noreferrer"> Alex Krizhevsky </a></li>
</ul>

<ul>
<li><a href="http://users.ics.aalto.fi/alexilin/" rel="noopener noreferrer"> Alexander Ilin </a></li>
</ul>

<ul>
<li><a href="http://homepages.inf.ed.ac.uk/amos/" rel="noopener noreferrer"> Amos Storkey </a></li>
</ul>

<ul>
<li><a href="http://www.stanford.edu/~asaxe/" rel="noopener noreferrer"> Andrew M. Saxe </a></li>
</ul>

<ul>
<li><a href="http://www.cs.stanford.edu/people/ang/" rel="noopener noreferrer"> Andrew Ng </a></li>
</ul>

<ul>
<li><a href="http://research.google.com/pubs/author37792.html" rel="noopener noreferrer"> Andrew W. Senior </a></li>
</ul>

<ul>
<li><a href="http://www.gatsby.ucl.ac.uk/~amnih/" rel="noopener noreferrer"> Andriy Mnih </a></li>
</ul>

<ul>
<li><a href="http://www.cs.nyu.edu/~naz/" rel="noopener noreferrer"> Ayse Naz Erkan </a></li>
</ul>

<ul>
<li><a href="http://reslab.elis.ugent.be/benjamin" rel="noopener noreferrer"> Benjamin Schrauwen </a></li>
</ul>

<ul>
<li><a href="https://www.cisuc.uc.pt/people/show/2020" rel="noopener noreferrer"> Bernardete Ribeiro </a></li>
</ul>

<ul>
<li><a href="http://vision.caltech.edu/~bchen3/Site/Bo_David_Chen.html" rel="noopener noreferrer"> Bo David Chen </a></li>
</ul>

<ul>
<li><a href="http://cs.nyu.edu/~ylan/" rel="noopener noreferrer"> Boureau Y-Lan </a></li>
</ul>

<ul>
<li><a href="http://researcher.watson.ibm.com/researcher/view.php?person=us-bedk" rel="noopener noreferrer"> Brian Kingsbury </a></li>
</ul>

<ul>
<li><a href="http://nlp.stanford.edu/~manning/" rel="noopener noreferrer"> Christopher Manning </a></li>
</ul>

<ul>
<li><a href="http://www.clement.farabet.net/" rel="noopener noreferrer"> Clement Farabet </a></li>
</ul>

<ul>
<li><a href="http://www.idsia.ch/~ciresan/" rel="noopener noreferrer"> Dan Claudiu Cire»ôan </a></li>
</ul>

<ul>
<li><a href="http://research.microsoft.com/en-us/people/dongyu/default.aspx" rel="noopener noreferrer"> Dong Yu </a></li>
</ul>

<ul>
<li><a href="http://www.seas.upenn.edu/~wulsin/" rel="noopener noreferrer"> Drausin Wulsin </a></li>
</ul>

<ul>
<li><a href="http://music.ece.drexel.edu/people/eschmidt" rel="noopener noreferrer"> Erik M. Schmidt </a></li>
</ul>

<ul>
<li><a href="https://engineering.purdue.edu/BME/People/viewPersonById?resource_id=71333" rel="noopener noreferrer"> Eugenio Culurciello </a></li>
</ul>

<ul>
<li><a href="http://research.microsoft.com/en-us/people/fseide/" rel="noopener noreferrer"> Frank Seide </a></li>
</ul>

<ul>
<li><a href="http://homes.cs.washington.edu/~galen/" rel="noopener noreferrer"> Galen Andrew </a></li>
</ul>

<ul>
<li><a href="http://www.cs.toronto.edu/~hinton/" rel="noopener noreferrer"> Geoffrey Hinton </a></li>
</ul>

<ul>
<li><a href="http://www.cs.toronto.edu/~gdahl/" rel="noopener noreferrer"> George Dahl </a></li>
</ul>

<ul>
<li><a href="http://www.uoguelph.ca/~gwtaylor/" rel="noopener noreferrer"> Graham Taylor </a></li>
</ul>

<ul>
<li><a href="http://gregoire.montavon.name/" rel="noopener noreferrer"> Gr√©goire Montavon </a></li>
</ul>

<ul>
<li><a href="http://personal-homepages.mis.mpg.de/montufar/" rel="noopener noreferrer"> Guido Francisco Mont√∫far </a></li>
</ul>

<ul>
<li><a href="http://brainlogging.wordpress.com/" rel="noopener noreferrer"> Guillaume Desjardins </a></li>
</ul>

<ul>
<li><a href="http://www.ais.uni-bonn.de/~schulz/" rel="noopener noreferrer"> Hannes Schulz </a></li>
</ul>

<ul>
<li><a href="http://www.lri.fr/~hpaugam/" rel="noopener noreferrer"> H√©l√®ne Paugam-Moisy </a></li>
</ul>

<ul>
<li><a href="http://web.eecs.umich.edu/~honglak/" rel="noopener noreferrer"> Honglak Lee </a></li>
</ul>

<ul>
<li><a href="http://www.dmi.usherb.ca/~larocheh/index_en.html" rel="noopener noreferrer"> Hugo Larochelle </a></li>
</ul>

<ul>
<li><a href="http://www.cs.toronto.edu/~ilya/" rel="noopener noreferrer"> Ilya Sutskever </a></li>
</ul>

<ul>
<li><a href="http://www.cs.toronto.edu/~jmartens/" rel="noopener noreferrer"> James Martens </a></li>
</ul>

<ul>
<li><a href="http://www.jasonmorton.com/" rel="noopener noreferrer"> Jason Morton </a></li>
</ul>

<ul>
<li><a href="http://www.thespermwhale.com/jaseweston/" rel="noopener noreferrer"> Jason Weston </a></li>
</ul>

<ul>
<li><a href="http://research.google.com/pubs/jeff.html" rel="noopener noreferrer"> Jeff Dean </a></li>
</ul>

<ul>
<li><a href="http://cs.stanford.edu/~jngiam/" rel="noopener noreferrer"> Jiquan Mgiam </a></li>
</ul>

<ul>
<li><a href="http://www-etud.iro.umontreal.ca/~turian/" rel="noopener noreferrer"> Joseph Turian </a></li>
</ul>

<ul>
<li><a href="http://aclab.ca/users/josh/index.html" rel="noopener noreferrer"> Joshua Matthew Susskind </a></li>
</ul>

<ul>
<li><a href="http://www.idsia.ch/~juergen/" rel="noopener noreferrer"> J√ºrgen Schmidhuber </a></li>
</ul>

<ul>
<li><a href="http://koray.kavukcuoglu.org/" rel="noopener noreferrer"> Koray Kavukcuoglu </a></li>
</ul>

<ul>
<li><a href="http://users.ics.aalto.fi/kcho/" rel="noopener noreferrer"> KyungHyun Cho </a></li>
</ul>

<ul>
<li><a href="http://research.microsoft.com/en-us/people/deng/" rel="noopener noreferrer"> Li Deng </a></li>
</ul>

<ul>
<li><a href="http://www.kyb.tuebingen.mpg.de/nc/employee/details/lucas.html" rel="noopener noreferrer"> Lucas Theis </a></li>
</ul>

<ul>
<li><a href="http://www.cs.nyu.edu/~ranzato/" rel="noopener noreferrer"> Marc'Aurelio Ranzato </a></li>
</ul>

<ul>
<li><a href="http://aass.oru.se/~mlt/" rel="noopener noreferrer"> Martin L√§ngkvist </a></li>
</ul>

<ul>
<li><a href="http://www.cs.toronto.edu/~norouzi/" rel="noopener noreferrer"> Mohammad Norouzi </a></li>
</ul>

<ul>
<li><a href="http://www.cs.ubc.ca/~nando/" rel="noopener noreferrer"> Nando de Freitas </a></li>
</ul>

<ul>
<li><a href="http://www.cs.utoronto.ca/~ndjaitly/" rel="noopener noreferrer"> Navdeep Jaitly </a></li>
</ul>

<ul>
<li><a href="http://nicolas.le-roux.name/" rel="noopener noreferrer"> Nicolas Le Roux </a></li>
</ul>

<ul>
<li><a href="http://www.cs.toronto.edu/~nitish/" rel="noopener noreferrer"> Nitish Srivastava </a></li>
</ul>

<ul>
<li><a href="https://www.cisuc.uc.pt/people/show/2028" rel="noopener noreferrer"> Noel Lopes </a></li>
</ul>

<ul>
<li><a href="http://www.cs.berkeley.edu/~vinyals/" rel="noopener noreferrer"> Oriol Vinyals </a></li>
</ul>

<ul>
<li><a href="http://www.iro.umontreal.ca/~vincentp" rel="noopener noreferrer"> Pascal Vincent </a></li>
</ul>

<ul>
<li><a href="http://homes.cs.washington.edu/~pedrod/" rel="noopener noreferrer"> Pedro Domingos </a></li>
</ul>

<ul>
<li><a href="http://homepages.inf.ed.ac.uk/pseries/" rel="noopener noreferrer"> Peggy Series </a></li>
</ul>

<ul>
<li><a href="http://cs.nyu.edu/~sermanet" rel="noopener noreferrer"> Pierre Sermanet </a></li>
</ul>

<ul>
<li><a href="http://www.cs.nyu.edu/~mirowski/" rel="noopener noreferrer"> Piotr Mirowski </a></li>
</ul>

<ul>
<li><a href="http://ai.stanford.edu/~quocle/" rel="noopener noreferrer"> Quoc V. Le </a></li>
</ul>

<ul>
<li><a href="http://bci.tugraz.at/scherer/" rel="noopener noreferrer"> Reinhold Scherer </a></li>
</ul>

<ul>
<li><a href="http://www.socher.org/" rel="noopener noreferrer"> Richard Socher </a></li>
</ul>

<ul>
<li><a href="http://homes.cs.washington.edu/~rcg/" rel="noopener noreferrer"> Robert Gens </a></li>
</ul>

<ul>
<li><a href="http://people.csail.mit.edu/rgrosse/" rel="noopener noreferrer"> Roger Grosse </a></li>
</ul>

<ul>
<li><a href="http://ronan.collobert.com/" rel="noopener noreferrer"> Ronan Collobert </a></li>
</ul>

<ul>
<li><a href="http://www.utstat.toronto.edu/~rsalakhu/" rel="noopener noreferrer"> Ruslan Salakhutdinov </a></li>
</ul>

<ul>
<li><a href="http://www.kyb.tuebingen.mpg.de/nc/employee/details/sgerwinn.html" rel="noopener noreferrer"> Sebastian Gerwinn </a></li>
</ul>

<ul>
<li><a href="http://www.cmap.polytechnique.fr/~mallat/" rel="noopener noreferrer"> St√©phane Mallat </a></li>
</ul>

<ul>
<li><a href="http://www.ais.uni-bonn.de/behnke/" rel="noopener noreferrer"> Sven Behnke </a></li>
</ul>

<ul>
<li><a href="http://users.ics.aalto.fi/praiko/" rel="noopener noreferrer"> Tapani Raiko </a></li>
</ul>

<ul>
<li><a href="https://sites.google.com/site/tsainath/" rel="noopener noreferrer"> Tara Sainath </a></li>
</ul>

<ul>
<li><a href="http://www.cs.toronto.edu/~tijmen/" rel="noopener noreferrer"> Tijmen Tieleman </a></li>
</ul>

<ul>
<li><a href="http://www.idsia.ch/~meier/" rel="noopener noreferrer"> Ueli Meier </a></li>
</ul>

<ul>
<li><a href="http://vincent.vanhoucke.com" rel="noopener noreferrer"> Vincent Vanhoucke </a></li>
</ul>

<ul>
<li><a href="http://www.cs.toronto.edu/~vmnih/" rel="noopener noreferrer"> Volodymyr Mnih </a></li>
</ul>

<ul>
<li><a href="http://yann.lecun.com/" rel="noopener noreferrer"> Yann LeCun </a></li>
</ul>

<ul>
<li><a href="http://www.cs.toronto.edu/~tang/" rel="noopener noreferrer"> Yichuan Tang </a></li>
</ul>

<ul>
<li><a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html" rel="noopener noreferrer"> Yoshua Bengio </a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/32/">Aug 10 - Aug 16, 2015</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="http://deeplearning.net/tutorial/deeplearning.pdf" rel="noopener noreferrer">Deep Learning Tutorial</a> by LISA lab, University of Montreal (Jan 6 2015)</li>
</ul>

<ul>
<li><a href="https://github.com/karpathy/neuraltalk" rel="noopener noreferrer">neuraltalk (‚≠ê5.4k)</a> by Andrej Karpathy : numpy-based RNN/LSTM implementation</li>
</ul>

<ul>
<li><a href="http://aima.cs.berkeley.edu/" rel="noopener noreferrer">Artificial Intelligence: A Modern Approach</a></li>
</ul>

<ul>
<li><a href="http://arxiv.org/pdf/1404.7828v4.pdf" rel="noopener noreferrer">Deep Learning in Neural Networks: An Overview</a></li>
</ul>
<h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="http://web.stanford.edu/class/cs224n/handouts/" rel="noopener noreferrer">Natural Language Processing</a> By Chris Manning in Stanford</li>
</ul>
<h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="http://arxiv.org/pdf/1506.07285v1.pdf" rel="noopener noreferrer">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a></li>
</ul>
<h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="http://www.robots.ox.ac.uk/~vgg/practicals/cnn/index.html" rel="noopener noreferrer">VGG Convolutional Neural Networks Practical</a></li>
</ul>
<h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="http://www.uk.research.att.com/facedatabase.html" rel="noopener noreferrer">AT&amp;T Laboratories Cambridge face database</a></li>
</ul>

<ul>
<li><a href="http://xtreme.gsfc.nasa.gov" rel="noopener noreferrer">AVHRR Pathfinder</a></li>
</ul>

<ul>
<li><a href="http://www.anc.ed.ac.uk/~amos/afreightdata.html" rel="noopener noreferrer">Air Freight</a> - The Air Freight data set is a ray-traced image sequence along with ground truth segmentation based on textural characteristics. (455 images + GT, each 160x120 pixels). (Formats: PNG)</li>
</ul>

<ul>
<li><a href="http://www.science.uva.nl/~aloi/" rel="noopener noreferrer">Amsterdam Library of Object Images</a> - ALOI is a color image collection of one-thousand small objects, recorded for scientific purposes. In order to capture the sensory variation in object recordings, we systematically varied viewing angle, illumination angle, and illumination color for each object, and additionally captured wide-baseline stereo images. We recorded over a hundred images of each object, yielding a total of 110,250 images for the collection. (Formats: png)</li>
</ul>

<ul>
<li><a href="http://www.imm.dtu.dk/~aam/" rel="noopener noreferrer">Annotated face, hand, cardiac &amp; meat images</a> - Most images &amp; annotations are supplemented by various ASM/AAM analyses using the AAM-API. (Formats: bmp,asf)</li>
</ul>

<ul>
<li><a href="http://www.imm.dtu.dk/image/" rel="noopener noreferrer">Image Analysis and Computer Graphics</a></li>
</ul>

<ul>
<li><a href="http://www.cog.brown.edu/~tarr/stimuli.html" rel="noopener noreferrer">Brown University Stimuli</a> - A variety of datasets including geons, objects, and "greebles". Good for testing recognition algorithms. (Formats: pict)</li>
</ul>

<ul>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/" rel="noopener noreferrer">CAVIAR video sequences of mall and public space behavior</a> - 90K video frames in 90 sequences of various human activities, with XML ground truth of detection and behavior classification (Formats: MPEG2 &amp; JPEG)</li>
</ul>

<ul>
<li><a href="http://www.ipab.inf.ed.ac.uk/mvu/" rel="noopener noreferrer">Machine Vision Unit</a></li>
</ul>

<ul>
<li><a href="http://www.cs.waikato.ac.nz/~singlis/ccitt.html" rel="noopener noreferrer">CCITT Fax standard images</a> - 8 images (Formats: gif)</li>
</ul>

<ul>
<li><a href="https://github.com/ChristosChristofidis/awesome-deep-learning/blob/master/README.md/cil-ster.html" rel="noopener noreferrer">CMU CIL's Stereo Data with Ground Truth</a> - 3 sets of 11 images, including color tiff images with spectroradiometry (Formats: gif, tiff)</li>
</ul>

<ul>
<li><a href="http://www.ri.cmu.edu/projects/project_418.html" rel="noopener noreferrer">CMU PIE Database</a> - A database of 41,368 face images of 68 people captured under 13 poses, 43 illuminations conditions, and with 4 different expressions.</li>
</ul>

<ul>
<li><a href="http://www.ius.cs.cmu.edu/idb/" rel="noopener noreferrer">CMU VASC Image Database</a> - Images, sequences, stereo pairs (thousands of images) (Formats: Sun Rasterimage)</li>
</ul>

<ul>
<li><a href="http://www.vision.caltech.edu/html-files/archive.html" rel="noopener noreferrer">Caltech Image Database</a> - about 20 images - mostly top-down views of small objects and toys. (Formats: GIF)</li>
</ul>

<ul>
<li><a href="http://www.cs.columbia.edu/CAVE/curet/" rel="noopener noreferrer">Columbia-Utrecht Reflectance and Texture Database</a> - Texture and reflectance measurements for over 60 samples of 3D texture, observed with over 200 different combinations of viewing and illumination directions. (Formats: bmp)</li>
</ul>

<ul>
<li><a href="http://www.cs.sfu.ca/~colour/data/index.html" rel="noopener noreferrer">Computational Colour Constancy Data</a> - A dataset oriented towards computational color constancy, but useful for computer vision in general. It includes synthetic data, camera sensor data, and over 700 images. (Formats: tiff)</li>
</ul>

<ul>
<li><a href="http://www.cs.sfu.ca/~colour/" rel="noopener noreferrer">Computational Vision Lab</a></li>
</ul>

<ul>
<li><a href="http://www.cs.washington.edu/research/imagedatabase/" rel="noopener noreferrer">Efficient Content-based Retrieval Group</a></li>
</ul>

<ul>
<li><a href="http://ls7-www.cs.uni-dortmund.de/~peters/pages/research/modeladaptsys/modeladaptsys_vba_rov.html" rel="noopener noreferrer">Densely Sampled View Spheres</a> - Densely sampled view spheres - upper half of the view sphere of two toy objects with 2500 images each. (Formats: tiff)</li>
</ul>

<ul>
<li><a href="http://ls7-www.cs.uni-dortmund.de/" rel="noopener noreferrer">Computer Science VII (Graphical Systems)</a></li>
</ul>

<ul>
<li><a href="http://www.gastrointestinalatlas.com" rel="noopener noreferrer">El Salvador Atlas of Gastrointestinal VideoEndoscopy</a> - Images and Videos of his-res of studies taken from Gastrointestinal Video endoscopy. (Formats: jpg, mpg, gif)</li>
</ul>

<ul>
<li><a href="http://sting.cycollege.ac.cy/~alanitis/fgnetaging/index.htm" rel="noopener noreferrer">FG-NET Facial Aging Database</a> - Database contains 1002 face images showing subjects at different ages. (Formats: jpg)</li>
</ul>

<ul>
<li><a href="http://bias.csr.unibo.it/fvc2000/" rel="noopener noreferrer">FVC2000 Fingerprint Databases</a> - FVC2000 is the First International Competition for Fingerprint Verification Algorithms. Four fingerprint databases constitute the FVC2000 benchmark (3520 fingerprints in all).</li>
</ul>

<ul>
<li><a href="http://www.fg-net.org" rel="noopener noreferrer">Face and Gesture images and image sequences</a> - Several image datasets of faces and gestures that are ground truth annotated for benchmarking</li>
</ul>

<ul>
<li><a href="http://www-i6.informatik.rwth-aachen.de/~dreuw/database.html" rel="noopener noreferrer">German Fingerspelling Database</a> - The database contains 35 gestures and consists of 1400 image sequences that contain gestures of 20 different persons recorded under non-uniform daylight lighting conditions. (Formats: mpg,jpg)</li>
</ul>

<ul>
<li><a href="http://www-i6.informatik.rwth-aachen.de/" rel="noopener noreferrer">Language Processing and Pattern Recognition</a></li>
</ul>

<ul>
<li><a href="http://hlab.phys.rug.nl/archive.html" rel="noopener noreferrer">Groningen Natural Image Database</a> - 4000+ 1536x1024 (16 bit) calibrated outdoor images (Formats: homebrew)</li>
</ul>

<ul>
<li><a href="http://www.icg.tu-graz.ac.at" rel="noopener noreferrer">Institute of Computer Graphics and Vision</a></li>
</ul>

<ul>
<li><a href="http://www.ien.it/is/vislib/" rel="noopener noreferrer">IEN Image Library</a> - 1000+ images, mostly outdoor sequences (Formats: raw, ppm)</li>
</ul>

<ul>
<li><a href="http://www-rocq.inria.fr/~tarel/syntim/images.html" rel="noopener noreferrer">INRIA's Syntim images database</a> - 15 color image of simple objects (Formats: gif)</li>
</ul>

<ul>
<li><a href="http://www.inria.fr/" rel="noopener noreferrer">INRIA</a></li>
</ul>

<ul>
<li><a href="http://www.ece.ncsu.edu/imaging/Archives/ImageDataBase/index.html" rel="noopener noreferrer">Image Analysis Laboratory</a> - Images obtained from a variety of imaging modalities -- raw CFA images, range images and a host of "medical images". (Formats: homebrew)</li>
</ul>

<ul>
<li><a href="http://www.prip.tuwien.ac.at/prip/image.html" rel="noopener noreferrer">Image Database</a> - An image database including some textures</li>
</ul>

<ul>
<li><a href="http://www.mic.atr.co.jp/" rel="noopener noreferrer">ATR Research, Kyoto, Japan</a></li>
</ul>

<ul>
<li><a href="http://vision.cse.psu.edu/book/testbed/images/" rel="noopener noreferrer">Machine Vision</a> - Images from the textbook by Jain, Kasturi, Schunck (20+ images) (Formats: GIF TIFF)</li>
</ul>

<ul>
<li><a href="http://marathon.csee.usf.edu/Mammography/Database.html" rel="noopener noreferrer">Mammography Image Databases</a> - 100 or more images of mammograms with ground truth. Additional images available by request, and links to several other mammography databases are provided. (Formats: homebrew)</li>
</ul>

<ul>
<li><a href="ftp://ftp.cps.msu.edu/pub/prip" rel="noopener noreferrer">ftp://ftp.cps.msu.edu/pub/prip</a> - many images (Formats: unknown)</li>
</ul>

<ul>
<li><a href="http://www.middlebury.edu/stereo/data.html" rel="noopener noreferrer">Middlebury Stereo Data Sets with Ground Truth</a> - Six multi-frame stereo data sets of scenes containing planar regions. Each data set contains 9 color images and subpixel-accuracy ground-truth data. (Formats: ppm)</li>
</ul>

<ul>
<li><a href="http://www.middlebury.edu/stereo" rel="noopener noreferrer">Middlebury Stereo Vision Research Page</a> - Middlebury College</li>
</ul>

<ul>
<li><a href="http://ltpwww.gsfc.nasa.gov/MODIS/MAS/" rel="noopener noreferrer">Modis Airborne simulator, Gallery and data set</a> - High Altitude Imagery from around the world for environmental modeling in support of NASA EOS program (Formats: JPG and HDF)</li>
</ul>

<ul>
<li><a href="ftp://sequoyah.ncsl.nist.gov/pub/databases/data" rel="noopener noreferrer">NIST Fingerprint and handwriting</a> - datasets - thousands of images (Formats: unknown)</li>
</ul>

<ul>
<li><a href="http://www.nlm.nih.gov/research/visible/visible_human.html" rel="noopener noreferrer">NLM HyperDoc Visible Human Project</a> - Color, CAT and MRI image samples - over 30 images (Formats: jpeg)</li>
</ul>

<ul>
<li><a href="http://eewww.eng.ohio-state.edu/~flynn/3DDB/Models/" rel="noopener noreferrer">OSU (MSU) 3D Object Model Database</a> - several sets of 3D object models collected over several years to use in object recognition research (Formats: homebrew, vrml)</li>
</ul>

<ul>
<li><a href="http://eewww.eng.ohio-state.edu/~flynn/3DDB/RID/" rel="noopener noreferrer">OSU (MSU/WSU) Range Image Database</a> - Hundreds of real and synthetic images (Formats: gif, homebrew)</li>
</ul>

<ul>
<li><a href="http://sampl.eng.ohio-state.edu" rel="noopener noreferrer">Signal Analysis and Machine Perception Laboratory</a></li>
</ul>

<ul>
<li><a href="http://www.cs.otago.ac.nz/research/vision/Research/OpticalFlow/opticalflow.html" rel="noopener noreferrer">Otago Optical Flow Evaluation Sequences</a> - Synthetic and real sequences with machine-readable ground truth optical flow fields, plus tools to generate ground truth for new sequences. (Formats: ppm,tif,homebrew)</li>
</ul>

<ul>
<li><a href="http://www.limsi.fr/" rel="noopener noreferrer">LIMSI-CNRS</a></li>
</ul>

<ul>
<li><a href="http://www.cee.hw.ac.uk/~mtc/sofa" rel="noopener noreferrer">SEQUENCES FOR OPTICAL FLOW ANALYSIS (SOFA)</a> - 9 synthetic sequences designed for testing motion analysis applications, including full ground truth of motion and camera parameters. (Formats: gif)</li>
</ul>

<ul>
<li><a href="http://www.cee.hw.ac.uk/~mtc/research.html" rel="noopener noreferrer">Computer Vision Group</a></li>
</ul>

<ul>
<li><a href="http://www.nada.kth.se/~zucch/CAMERA/PUB/seq.html" rel="noopener noreferrer">Sequences for Flow Based Reconstruction</a> - synthetic sequence for testing structure from motion algorithms (Formats: pgm)</li>
</ul>

<ul>
<li><a href="http://www-dbv.cs.uni-bonn.de/stereo_data/" rel="noopener noreferrer">Stereo Images with Ground Truth Disparity and Occlusion</a> - a small set of synthetic images of a hallway with varying amounts of noise added. Use these images to benchmark your stereo algorithm. (Formats: raw, viff (khoros), or tiff)</li>
</ul>

<ul>
<li><a href="http://range.informatik.uni-stuttgart.de" rel="noopener noreferrer">Stuttgart Range Image Database</a> - A collection of synthetic range images taken from high-resolution polygonal models available on the web (Formats: homebrew)</li>
</ul>

<ul>
<li><a href="http://rvl.www.ecn.purdue.edu/RVL/" rel="noopener noreferrer">Purdue Robot Vision Lab</a></li>
</ul>

<ul>
<li><a href="http://web.mit.edu/torralba/www/database.html" rel="noopener noreferrer">The MIT-CSAIL Database of Objects and Scenes</a> - Database for testing multiclass object detection and scene recognition algorithms. Over 72,000 images with 2873 annotated frames. More than 50 annotated object classes. (Formats: jpg)</li>
</ul>

<ul>
<li><a href="http://rvl1.ecn.purdue.edu/RVL/specularity_database/" rel="noopener noreferrer">The RVL SPEC-DB (SPECularity DataBase)</a> - A collection of over 300 real images of 100 objects taken under three different illuminaiton conditions (Diffuse/Ambient/Directed). -- Use these images to test algorithms for detecting and compensating specular highlights in color images. (Formats: TIFF )</li>
</ul>

<ul>
<li><a href="http://rvl1.ecn.purdue.edu/RVL/" rel="noopener noreferrer">Robot Vision Laboratory</a></li>
</ul>

<ul>
<li><a href="http://xm2vtsdb.ee.surrey.ac.uk" rel="noopener noreferrer">The Xm2vts database</a> - The XM2VTSDB contains four digital recordings of 295 people taken over a period of four months. This database contains both image and video data of faces.</li>
</ul>

<ul>
<li><a href="http://i21www.ira.uka.de/image_sequences" rel="noopener noreferrer">Traffic Image Sequences and 'Marbled Block' Sequence</a> - thousands of frames of digitized traffic image sequences as well as the 'Marbled Block' sequence (grayscale images) (Formats: GIF)</li>
</ul>

<ul>
<li><a href="ftp://ftp.iam.unibe.ch/pub/Images/FaceImages" rel="noopener noreferrer">U Bern Face images</a> - hundreds of images (Formats: Sun rasterfile)</li>
</ul>

<ul>
<li><a href="ftp://freebie.engin.umich.edu/pub/misc/textures" rel="noopener noreferrer">U Michigan textures</a> (Formats: compressed raw)</li>
</ul>

<ul>
<li><a href="ftp://sunsite.unc.edu/pub/academic/computer-science/virtual-reality/3d" rel="noopener noreferrer">UNC's 3D image database</a> - many images (Formats: GIF)</li>
</ul>

<ul>
<li><a href="http://marathon.csee.usf.edu/range/seg-comp/SegComp.html" rel="noopener noreferrer">USF Range Image Data with Segmentation Ground Truth</a> - 80 image sets (Formats: Sun rasterimage)</li>
</ul>

<ul>
<li><a href="http://www.ee.oulu.fi/research/imag/color/pbfd.html" rel="noopener noreferrer">University of Oulu Physics-based Face Database</a> - contains color images of faces under different illuminants and camera calibration conditions as well as skin spectral reflectance measurements of each person.</li>
</ul>

<ul>
<li><a href="http://www.ee.oulu.fi/mvmp/" rel="noopener noreferrer">Machine Vision and Media Processing Unit</a></li>
</ul>

<ul>
<li><a href="http://www.outex.oulu.fi" rel="noopener noreferrer">University of Oulu Texture Database</a> - Database of 320 surface textures, each captured under three illuminants, six spatial resolutions and nine rotation angles. A set of test suites is also provided so that texture segmentation, classification, and retrieval algorithms can be tested in a standard manner. (Formats: bmp, ras, xv)</li>
</ul>

<ul>
<li><a href="http://www.ee.oulu.fi/mvg" rel="noopener noreferrer">Machine Vision Group</a></li>
</ul>

<ul>
<li><a href="ftp://ftp.uu.net/published/usenix/faces" rel="noopener noreferrer">Usenix face database</a> - Thousands of face images from many different sites (circa 994)</li>
</ul>

<ul>
<li><a href="http://www-prima.inrialpes.fr/Prima/hall/view_sphere.html" rel="noopener noreferrer">View Sphere Database</a> - Images of 8 objects seen from many different view points. The view sphere is sampled using a geodesic with 172 images/sphere. Two sets for training and testing are available. (Formats: ppm)</li>
</ul>

<ul>
<li><a href="http://www-prima.inrialpes.fr/Prima/" rel="noopener noreferrer">PRIMA, GRAVIR</a></li>
</ul>

<ul>
<li><a href="ftp://ftp.vislist.com/IMAGERY/" rel="noopener noreferrer">Vision-list Imagery Archive</a> - Many images, many formats</li>
</ul>

<ul>
<li><a href="http://www.cs.cmu.edu/~owenc/word.htm" rel="noopener noreferrer">Wiry Object Recognition Database</a> - Thousands of images of a cart, ladder, stool, bicycle, chairs, and cluttered scenes with ground truth labelings of edges and regions. (Formats: jpg)</li>
</ul>

<ul>
<li><a href="http://www.cs.cmu.edu/0.000000E+003dvision/" rel="noopener noreferrer">3D Vision Group</a></li>
</ul>

<ul>
<li><a href="http://cvc.yale.edu/projects/yalefaces/yalefaces.html" rel="noopener noreferrer">Yale Face Database</a> -  165 images (15 individuals) with different lighting, expression, and occlusion configurations.</li>
</ul>

<ul>
<li><a href="http://cvc.yale.edu/" rel="noopener noreferrer">Center for Computational Vision and Control</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/vlfeat/matconvnet" rel="noopener noreferrer">MatConvNet: CNNs for MATLAB (‚≠ê1.4k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/31/">Aug 03 - Aug 09, 2015</a></h2><h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="http://news.startup.ml/" rel="noopener noreferrer">Deep Learning News</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/karpathy/char-rnn" rel="noopener noreferrer">char-rnn (‚≠ê12k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/29/">Jul 20 - Jul 26, 2015</a></h2><h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="http://www.cs.toronto.edu/~graves/preprint.pdf" rel="noopener noreferrer">Supervised Sequence Labelling with Recurrent Neural Networks</a></li>
</ul>

<ul>
<li><a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf" rel="noopener noreferrer">Statistical Language Models based on Neural Networks</a></li>
</ul>

<ul>
<li><a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf" rel="noopener noreferrer">Training Recurrent Neural Networks</a></li>
</ul>

<ul>
<li><a href="http://nlp.stanford.edu/~socherr/thesis.pdf" rel="noopener noreferrer">Recursive Deep Learning for Natural Language Processing and Computer Vision</a></li>
</ul>

<ul>
<li><a href="http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf" rel="noopener noreferrer">Bi-directional RNN</a></li>
</ul>

<ul>
<li><a href="http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf" rel="noopener noreferrer">LSTM</a></li>
</ul>

<ul>
<li><a href="http://arxiv.org/pdf/1406.1078v3.pdf" rel="noopener noreferrer">GRU - Gated Recurrent Unit</a></li>
</ul>

<ul>
<li><a href="http://arxiv.org/pdf/1502.02367v3.pdf" rel="noopener noreferrer">GFRNN</a> <a href="http://jmlr.org/proceedings/papers/v37/chung15.pdf" rel="noopener noreferrer">.</a> <a href="http://jmlr.org/proceedings/papers/v37/chung15-supp.pdf" rel="noopener noreferrer">.</a></li>
</ul>

<ul>
<li><a href="http://arxiv.org/pdf/1503.04069v1.pdf" rel="noopener noreferrer">LSTM: A Search Space Odyssey</a></li>
</ul>

<ul>
<li><a href="http://arxiv.org/pdf/1506.00019v1.pdf" rel="noopener noreferrer">A Critical Review of Recurrent Neural Networks for Sequence Learning</a></li>
</ul>

<ul>
<li><a href="http://arxiv.org/pdf/1506.02078v1.pdf" rel="noopener noreferrer">Visualizing and Understanding Recurrent Networks</a></li>
</ul>

<ul>
<li><a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" rel="noopener noreferrer">Wojciech Zaremba, Ilya Sutskever, An Empirical Exploration of Recurrent Network Architectures</a></li>
</ul>

<ul>
<li><a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="noopener noreferrer">Recurrent Neural Network based Language Model</a></li>
</ul>

<ul>
<li><a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf" rel="noopener noreferrer">Extensions of Recurrent Neural Network Language Model</a></li>
</ul>

<ul>
<li><a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/ApplicationOfRNNinMeetingRecognition_IS2011.pdf" rel="noopener noreferrer">Recurrent Neural Network based Language Modeling in Meeting Recognition</a></li>
</ul>

<ul>
<li><a href="http://cs224d.stanford.edu/papers/maas_paper.pdf" rel="noopener noreferrer">Deep Neural Networks for Acoustic Modeling in Speech Recognition</a></li>
</ul>

<ul>
<li><a href="http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf" rel="noopener noreferrer">Speech Recognition with Deep Recurrent Neural Networks</a></li>
</ul>

<ul>
<li><a href="http://arxiv.org/pdf/1505.00521v1" rel="noopener noreferrer">Reinforcement Learning Neural Turing Machines</a></li>
</ul>

<ul>
<li><a href="http://arxiv.org/pdf/1406.1078v3.pdf" rel="noopener noreferrer">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
</ul>

<ul>
<li><a href="http://arxiv.org/pdf/1410.3916v10" rel="noopener noreferrer">Memory Networks</a></li>
</ul>

<ul>
<li><a href="http://arxiv.org/pdf/1507.01273v1" rel="noopener noreferrer">Policy Learning with Continuous Memory States for Partially Observed Robotic Control</a></li>
</ul>

<ul>
<li><a href="http://arxiv.org/pdf/1505.01861v1.pdf" rel="noopener noreferrer">Microsoft - Jointly Modeling Embedding and Translation to Bridge Video and Language</a></li>
</ul>

<ul>
<li><a href="http://arxiv.org/pdf/1410.5401v2.pdf" rel="noopener noreferrer">Neural Turing Machines</a></li>
</ul>
<h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="http://deeplearning.cs.toronto.edu/i2t" rel="noopener noreferrer">deeplearning.cs.toronto.edu</a></li>
</ul>

<ul>
<li><a href="http://jeffdonahue.com/lrcn/" rel="noopener noreferrer">jeffdonahue.com/lrcn/</a></li>
</ul>

<ul>
<li><a href="http://www.visualqa.org/" rel="noopener noreferrer">visualqa.org</a></li>
</ul>

<ul>
<li><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/" rel="noopener noreferrer">www.mpi-inf.mpg.de/departments/computer-vision...</a></li>
</ul>
<h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.html" rel="noopener noreferrer">Flickr 8k</a></li>
</ul>

<ul>
<li><a href="http://shannon.cs.illinois.edu/DenotationGraph/" rel="noopener noreferrer">Flickr 30k</a></li>
</ul>

<ul>
<li><a href="http://mscoco.org/home/" rel="noopener noreferrer">Microsoft COCO</a></li>
</ul>

<ul>
<li><a href="http://www.visualqa.org/" rel="noopener noreferrer">VQA</a></li>
</ul>

<ul>
<li><a href="http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/" rel="noopener noreferrer">Image QA</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="http://rnnlm.org/" rel="noopener noreferrer">RNNLM Toolkit</a></li>
</ul>

<ul>
<li><a href="http://sourceforge.net/p/rnnl/wiki/Home/" rel="noopener noreferrer">RNNLIB - A recurrent neural network library</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/28/">Jul 13 - Jul 19, 2015</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="http://chainer.org/" rel="noopener noreferrer">Chainer - A flexible framework of neural networks for deep learning</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/22/">Jun 01 - Jun 07, 2015</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/NVIDIA/DIGITS" rel="noopener noreferrer">Nvidia DIGITS - a web app based on Caffe (‚≠ê4.2k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/21/">May 25 - May 31, 2015</a></h2><h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="http://cs.brown.edu/research/ai/" rel="noopener noreferrer">cs.brown.edu/research/ai</a></li>
</ul>

<ul>
<li><a href="http://www.eecs.umich.edu/ai/" rel="noopener noreferrer">eecs.umich.edu/ai</a></li>
</ul>

<ul>
<li><a href="http://www.cs.utexas.edu/users/ai-lab/" rel="noopener noreferrer">cs.utexas.edu/users/ai-lab</a></li>
</ul>

<ul>
<li><a href="http://www.cs.washington.edu/research/ai/" rel="noopener noreferrer">cs.washington.edu/research/ai</a></li>
</ul>

<ul>
<li><a href="http://www.aiai.ed.ac.uk/" rel="noopener noreferrer">aiai.ed.ac.uk</a></li>
</ul>

<ul>
<li><a href="http://www-aig.jpl.nasa.gov/" rel="noopener noreferrer">www-aig.jpl.nasa.gov</a></li>
</ul>

<ul>
<li><a href="http://www.csail.mit.edu/" rel="noopener noreferrer">csail.mit.edu</a></li>
</ul>

<ul>
<li><a href="http://cgi.cse.unsw.edu.au/~aishare/" rel="noopener noreferrer">cgi.cse.unsw.edu.au/~aishare</a></li>
</ul>

<ul>
<li><a href="http://www.cs.rochester.edu/research/ai/" rel="noopener noreferrer">cs.rochester.edu/research/ai</a></li>
</ul>

<ul>
<li><a href="http://www.ai.sri.com/" rel="noopener noreferrer">ai.sri.com</a></li>
</ul>

<ul>
<li><a href="http://www.isi.edu/AI/isd.htm" rel="noopener noreferrer">isi.edu/AI/isd.htm</a></li>
</ul>

<ul>
<li><a href="http://www.nrl.navy.mil/itd/aic/" rel="noopener noreferrer">nrl.navy.mil/itd/aic</a></li>
</ul>

<ul>
<li><a href="http://hips.seas.harvard.edu/" rel="noopener noreferrer">hips.seas.harvard.edu</a></li>
</ul>

<ul>
<li><a href="http://aiweekly.co" rel="noopener noreferrer">AI Weekly</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="http://keras.io" rel="noopener noreferrer">Keras - Theano based Deep Learning Library</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/20/">May 18 - May 24, 2015</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/NervanaSystems/neon" rel="noopener noreferrer">Neon - Python based Deep Learning Framework (‚≠ê3.9k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/10/">Mar 09 - Mar 15, 2015</a></h2><h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="http://melisgl.github.io/mgl-pax-world/mgl-manual.html" rel="noopener noreferrer">MGL</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/3/">Jan 19 - Jan 25, 2015</a></h2><h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="http://web.stanford.edu/class/cs294a/handouts.html" rel="noopener noreferrer">Unsupervised Deep Learning - Stanford</a> by Andrew Ng in Stanford (2011)</li>
</ul>
<h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="http://www.iro.umontreal.ca/~bengioy/papers/YB-tricks.pdf" rel="noopener noreferrer">Training tricks by YB</a></li>
</ul>
<h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="http://deeplearning.net/tutorial/deeplearning.pdf" rel="noopener noreferrer">Theano Tutorial</a></li>
</ul>

<ul>
<li><a href="http://uk.mathworks.com/help/pdf_doc/nnet/nnet_ug.pdf" rel="noopener noreferrer">Neural Networks for Matlab</a></li>
</ul>
<h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="http://nlp.stanford.edu/" rel="noopener noreferrer">nlp.stanford.edu</a></li>
</ul>

<ul>
<li><a href="http://www.ai-junkie.com/ann/evolved/nnt1.html" rel="noopener noreferrer">ai-junkie.com</a></li>
</ul>
<h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="http://archive.ics.uci.edu/ml/" rel="noopener noreferrer">UC Irvine Machine Learning Repository</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/karpathy/convnetjs" rel="noopener noreferrer">convetjs (‚≠ê11k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/rasmusbergpalm/DeepLearnToolbox" rel="noopener noreferrer">DeepLearnToolbox (‚≠ê3.8k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/nitishsrivastava/deepnet" rel="noopener noreferrer">Deepnet (‚≠ê896)</a></li>
</ul>

<ul>
<li><a href="https://github.com/andersbll/deeppy" rel="noopener noreferrer">Deeppy (‚≠ê1.4k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/ivan-vasilev/neuralnetworks" rel="noopener noreferrer">JavaNN (‚≠ê1.2k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/hannes-brt/hebel" rel="noopener noreferrer">hebel (‚≠ê1.2k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/pluskid/Mocha.jl" rel="noopener noreferrer">Mocha.jl (‚≠ê1.3k)</a></li>
</ul>

<ul>
<li><a href="https://github.com/guoding83128/OpenDL" rel="noopener noreferrer">OpenDL (‚≠ê221)</a></li>
</ul>

<ul>
<li><a href="https://developer.nvidia.com/cuDNN" rel="noopener noreferrer">cuDNN</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/2/">Jan 12 - Jan 18, 2015</a></h2><h3><p>Table of Contents / Books</p>
</h3>
<ul>
<li><a href="http://neuralnetworksanddeeplearning.com/" rel="noopener noreferrer">Neural Networks and Deep Learning</a> by  Michael Nielsen (Dec 2014)</li>
</ul>
<h3><p>Table of Contents / Courses</p>
</h3>
<ul>
<li><a href="https://class.coursera.org/ml-005" rel="noopener noreferrer">Machine Learning - Stanford</a> by Andrew Ng in Coursera (2010-2014)</li>
</ul>

<ul>
<li><a href="http://work.caltech.edu/lectures.html" rel="noopener noreferrer">Machine Learning - Caltech</a> by Yaser Abu-Mostafa (2012-2014)</li>
</ul>

<ul>
<li><a href="http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml" rel="noopener noreferrer">Machine Learning - Carnegie Mellon</a> by Tom Mitchell (Spring 2011)</li>
</ul>

<ul>
<li><a href="https://class.coursera.org/neuralnets-2012-001" rel="noopener noreferrer">Neural Networks for Machine Learning</a> by Geoffrey Hinton in Coursera (2012)</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH" rel="noopener noreferrer">Neural networks class</a> by Hugo Larochelle from Universit√© de Sherbrooke (2013)</li>
</ul>

<ul>
<li><a href="http://cilvr.cs.nyu.edu/doku.php?id=deeplearning:slides:start" rel="noopener noreferrer">Deep Learning Course</a> by CILVR lab @ NYU (2014)</li>
</ul>

<ul>
<li><a href="https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/courseware/" rel="noopener noreferrer">A.I - Berkeley</a> by Dan Klein and Pieter Abbeel (2013)</li>
</ul>

<ul>
<li><a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/" rel="noopener noreferrer">A.I - MIT</a> by Patrick Henry Winston (2010)</li>
</ul>

<ul>
<li><a href="http://web.mit.edu/course/other/i2course/www/vision_and_learning_fall_2013.html" rel="noopener noreferrer">Vision and learning - computers and brains</a> by Shimon Ullman, Tomaso Poggio, Ethan Meyers @ MIT (2013)</li>
</ul>
<h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=AyzOUbkUf3M" rel="noopener noreferrer">The Next Generation of Neural Networks</a> By Geoffrey Hinton at GoogleTechTalks</li>
</ul>

<ul>
<li><a href="http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn" rel="noopener noreferrer">The wonderful and terrifying implications of computers that can learn</a> By Jeremy Howard at TEDxBrussels</li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="https://github.com/harthur/brain" rel="noopener noreferrer">Brain (‚≠ê8k)</a></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2015/1/">Jan 05 - Jan 11, 2015</a></h2><h3><p>Table of Contents / Videos and Lectures</p>
</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=RIkxVci-R4k" rel="noopener noreferrer">How To Create A Mind</a> By Ray Kurzweil</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/watch?v=n1ViNeWhC24" rel="noopener noreferrer">Deep Learning, Self-Taught Learning and Unsupervised Feature Learning</a> By Andrew Ng</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/watch?v=vShMxxqtDDs&amp;index=3&amp;list=PL78U8qQHXgrhP9aZraxTT5-X1RccTcUYT" rel="noopener noreferrer">Recent Developments in Deep Learning</a> By Geoff Hinton</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/watch?v=sc-KbuZqGkI" rel="noopener noreferrer">The Unreasonable Effectiveness of Deep Learning</a> by Yann LeCun</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/watch?v=4xsVFLnHC_0" rel="noopener noreferrer">Deep Learning of Representations</a> by Yoshua bengio</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/watch?v=6ufPpZDmPKA" rel="noopener noreferrer">Principles of Hierarchical Temporal Memory</a> by Jeff Hawkins</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/watch?v=2QJi0ArLq7s&amp;list=PL78U8qQHXgrhP9aZraxTT5-X1RccTcUYT" rel="noopener noreferrer">Machine Learning Discussion Group - Deep Learning w/ Stanford AI Lab</a> by Adam Coates</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/watch?v=3boKlkPBckA" rel="noopener noreferrer">Visual Perception with Deep Learning</a> By Yann LeCun</li>
</ul>
<h3><p>Table of Contents / Papers</p>
</h3>
<ul>
<li><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener noreferrer">ImageNet Classification with Deep Convolutional Neural Networks</a></li>
</ul>

<ul>
<li><a href="http://www.cs.toronto.edu/~hinton/absps/esann-deep-final.pdf" rel="noopener noreferrer">Using Very Deep Autoencoders for Content Based Image Retrieval</a></li>
</ul>

<ul>
<li><a href="http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf" rel="noopener noreferrer">Learning Deep Architectures for AI</a></li>
</ul>

<ul>
<li><a href="http://deeplearning.cs.cmu.edu/" rel="noopener noreferrer">CMU‚Äôs list of papers</a></li>
</ul>
<h3><p>Table of Contents / Tutorials</p>
</h3>
<ul>
<li><a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial" rel="noopener noreferrer">UFLDL Tutorial 1</a></li>
</ul>

<ul>
<li><a href="http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/" rel="noopener noreferrer">UFLDL Tutorial 2</a></li>
</ul>

<ul>
<li><a href="http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial" rel="noopener noreferrer">Deep Learning for NLP (without Magic)</a></li>
</ul>

<ul>
<li><a href="http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks" rel="noopener noreferrer">A Deep Learning Tutorial: From Perceptrons to Deep Networks</a></li>
</ul>

<ul>
<li><a href="http://www.metacademy.org/roadmaps/rgrosse/deep_learning" rel="noopener noreferrer">Deep Learning from the Bottom up</a></li>
</ul>
<h3><p>Researchers / Websites</p>
</h3>
<ul>
<li><a href="http://deeplearning.net/" rel="noopener noreferrer">deeplearning.net</a></li>
</ul>

<ul>
<li><a href="http://deeplearning.stanford.edu/" rel="noopener noreferrer">deeplearning.stanford.edu</a></li>
</ul>
<h3><p>Researchers / Datasets</p>
</h3>
<ul>
<li><a href="http://yann.lecun.com/exdb/mnist/" rel="noopener noreferrer">MNIST</a> Handwritten digits</li>
</ul>

<ul>
<li><a href="http://ufldl.stanford.edu/housenumbers/" rel="noopener noreferrer">Google House Numbers</a> from street view</li>
</ul>

<ul>
<li><a href="http://www.image-net.org/" rel="noopener noreferrer">IMAGENET</a></li>
</ul>

<ul>
<li><a href="http://groups.csail.mit.edu/vision/TinyImages/" rel="noopener noreferrer">Tiny Images</a> 80 Million tiny images6.</li>
</ul>

<ul>
<li><a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/" rel="noopener noreferrer">Berkeley Segmentation Dataset 500</a></li>
</ul>
<h3><p>Researchers / Frameworks</p>
</h3>
<ul>
<li><a href="http://caffe.berkeleyvision.org/" rel="noopener noreferrer">Caffe</a></li>
</ul>

<ul>
<li><a href="http://torch.ch/" rel="noopener noreferrer">Torch7</a></li>
</ul>

<ul>
<li><a href="http://deeplearning.net/software/theano/" rel="noopener noreferrer">Theano</a></li>
</ul>

<ul>
<li><a href="https://code.google.com/p/cuda-convnet2/" rel="noopener noreferrer">cuda-convnet</a></li>
</ul>

<ul>
<li><a href="http://libccv.org/doc/doc-convnet/" rel="noopener noreferrer">Ccv</a></li>
</ul>

<ul>
<li><a href="http://numenta.org/nupic.html" rel="noopener noreferrer">NuPIC</a></li>
</ul>

<ul>
<li><a href="http://deeplearning4j.org/" rel="noopener noreferrer">DeepLearning4J</a></li>
</ul>

    </main>
  </body>
</html>
