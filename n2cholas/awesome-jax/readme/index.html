<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <title>Awesome Jax (n2cholas/awesome-jax) Overview - Track Awesome List</title>
    <meta property="og:url" content="https://www.trackawesomelist.com/n2cholas/awesome-jax/readme/" />
    <meta property="og:type" content="summary" />
    <meta property="og:title" content="Awesome Jax Overview" />
    <meta property="og:description" content="JAX - A curated list of resources https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;jax" />
    <meta property="og:site_name" content="Track Awesome List" />
    <style>
      main {
        max-width: 1024px;
        margin: 0 auto;
        padding: 0 0.5em;
      }
      :root,[data-color-mode=light][data-light-theme=light],[data-color-mode=dark][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}[data-color-mode=light][data-light-theme=dark],[data-color-mode=dark][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}.markdown-body{word-wrap:break-word;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;font-size:16px;line-height:1.5}.markdown-body:before{content:"";display:table}.markdown-body:after{clear:both;content:"";display:table}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:var(--color-danger-fg)}.markdown-body .anchor{float:left;margin-left:-20px;padding-right:4px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body p,.markdown-body blockquote,.markdown-body ul,.markdown-body ol,.markdown-body dl,.markdown-body table,.markdown-body pre,.markdown-body details{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;background-color:var(--color-border-default);border:0;margin:24px 0;padding:0}.markdown-body blockquote{color:var(--color-fg-muted);border-left:.25em solid var(--color-border-default);padding:0 1em}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:var(--color-fg-default);vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 tt,.markdown-body h1 code,.markdown-body h2 tt,.markdown-body h2 code,.markdown-body h3 tt,.markdown-body h3 code,.markdown-body h4 tt,.markdown-body h4 code,.markdown-body h5 tt,.markdown-body h5 code,.markdown-body h6 tt,.markdown-body h6 code{font-size:inherit;padding:0 .2em}.markdown-body h1{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:2em}.markdown-body h2{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{color:var(--color-fg-muted);font-size:.85em}.markdown-body summary h1,.markdown-body summary h2,.markdown-body summary h3,.markdown-body summary h4,.markdown-body summary h5,.markdown-body summary h6{display:inline-block}.markdown-body summary h1 .anchor,.markdown-body summary h2 .anchor,.markdown-body summary h3 .anchor,.markdown-body summary h4 .anchor,.markdown-body summary h5 .anchor,.markdown-body summary h6 .anchor{margin-left:-40px}.markdown-body summary h1,.markdown-body summary h2{border-bottom:0;padding-bottom:0}.markdown-body ul,.markdown-body ol{padding-left:2em}.markdown-body ul.no-list,.markdown-body ol.no-list{padding:0;list-style-type:none}.markdown-body ol[type="1"]{list-style-type:decimal}.markdown-body ol[type=a]{list-style-type:lower-alpha}.markdown-body ol[type=i]{list-style-type:lower-roman}.markdown-body div>ol:not([type]){list-style-type:decimal}.markdown-body ul ul,.markdown-body ul ol,.markdown-body ol ol,.markdown-body ol ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{margin-top:16px;padding:0;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{margin-bottom:16px;padding:0 16px}.markdown-body table{width:100%;width:-webkit-max-content;width:-webkit-max-content;width:max-content;max-width:100%;display:block;overflow:auto}.markdown-body table th{font-weight:600}.markdown-body table th,.markdown-body table td{border:1px solid var(--color-border-default);padding:6px 13px}.markdown-body table tr{background-color:var(--color-canvas-default);border-top:1px solid var(--color-border-muted)}.markdown-body table tr:nth-child(2n){background-color:var(--color-canvas-subtle)}.markdown-body table img{background-color:rgba(0,0,0,0)}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:var(--color-canvas-default)}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:rgba(0,0,0,0)}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{float:left;width:auto;border:1px solid var(--color-border-default);margin:13px 0 0;padding:7px;display:block;overflow:hidden}.markdown-body span.frame span img{float:left;display:block}.markdown-body span.frame span span{clear:both;color:var(--color-fg-default);padding:5px 0 0;display:block}.markdown-body span.align-center{clear:both;display:block;overflow:hidden}.markdown-body span.align-center>span{text-align:center;margin:13px auto 0;display:block;overflow:hidden}.markdown-body span.align-center span img{text-align:center;margin:0 auto}.markdown-body span.align-right{clear:both;display:block;overflow:hidden}.markdown-body span.align-right>span{text-align:right;margin:13px 0 0;display:block;overflow:hidden}.markdown-body span.align-right span img{text-align:right;margin:0}.markdown-body span.float-left{float:left;margin-right:13px;display:block;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{float:right;margin-left:13px;display:block;overflow:hidden}.markdown-body span.float-right>span{text-align:right;margin:13px auto 0;display:block;overflow:hidden}.markdown-body code,.markdown-body tt{background-color:var(--color-neutral-muted);border-radius:6px;margin:0;padding:.2em .4em;font-size:85%}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{-webkit-text-decoration:inherit;-webkit-text-decoration:inherit;text-decoration:inherit}.markdown-body samp{font-size:85%}.markdown-body pre{word-wrap:normal}.markdown-body pre code{font-size:100%}.markdown-body pre>code{word-break:normal;white-space:pre;background:0 0;border:0;margin:0;padding:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{word-break:normal;margin-bottom:0}.markdown-body .highlight pre,.markdown-body pre{background-color:var(--color-canvas-subtle);border-radius:6px;padding:16px;font-size:85%;line-height:1.45;overflow:auto}.markdown-body pre code,.markdown-body pre tt{max-width:auto;line-height:inherit;word-wrap:normal;background-color:rgba(0,0,0,0);border:0;margin:0;padding:0;display:inline;overflow:visible}.markdown-body .csv-data td,.markdown-body .csv-data th{text-align:left;white-space:nowrap;padding:5px;font-size:12px;line-height:1;overflow:hidden}.markdown-body .csv-data .blob-num{text-align:right;background:var(--color-canvas-default);border:0;padding:10px 8px 9px}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{background:var(--color-canvas-subtle);border-top:0;font-weight:600}.markdown-body [data-footnote-ref]:before{content:"["}.markdown-body [data-footnote-ref]:after{content:"]"}.markdown-body .footnotes{color:var(--color-fg-muted);border-top:1px solid var(--color-border-default);font-size:12px}.markdown-body .footnotes ol{padding-left:16px}.markdown-body .footnotes li{position:relative}.markdown-body .footnotes li:target:before{pointer-events:none;content:"";border:2px solid var(--color-accent-emphasis);border-radius:6px;position:absolute;top:-8px;bottom:-8px;left:-24px;right:-8px}.markdown-body .footnotes li:target{color:var(--color-fg-default)}.markdown-body .footnotes .data-footnote-backref g-emoji{font-family:monospace}.markdown-body{background-color:var(--color-canvas-default);color:var(--color-fg-default)}.markdown-body a{color:var(--color-accent-fg);text-decoration:none}.markdown-body a:hover{text-decoration:underline}.markdown-body iframe{background-color:#fff;border:0;margin-bottom:16px}.markdown-body svg.octicon{fill:currentColor}.markdown-body .anchor>.octicon{display:inline}.markdown-body .highlight .token.keyword,.gfm-highlight .token.keyword{color:var(--color-prettylights-syntax-keyword)}.markdown-body .highlight .token.tag .token.class-name,.markdown-body .highlight .token.tag .token.script .token.punctuation,.gfm-highlight .token.tag .token.class-name,.gfm-highlight .token.tag .token.script .token.punctuation{color:var(--color-prettylights-syntax-storage-modifier-import)}.markdown-body .highlight .token.operator,.markdown-body .highlight .token.number,.markdown-body .highlight .token.boolean,.markdown-body .highlight .token.tag .token.punctuation,.markdown-body .highlight .token.tag .token.script .token.script-punctuation,.markdown-body .highlight .token.tag .token.attr-name,.gfm-highlight .token.operator,.gfm-highlight .token.number,.gfm-highlight .token.boolean,.gfm-highlight .token.tag .token.punctuation,.gfm-highlight .token.tag .token.script .token.script-punctuation,.gfm-highlight .token.tag .token.attr-name{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.function,.gfm-highlight .token.function{color:var(--color-prettylights-syntax-entity)}.markdown-body .highlight .token.string,.gfm-highlight .token.string{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.comment,.gfm-highlight .token.comment{color:var(--color-prettylights-syntax-comment)}.markdown-body .highlight .token.class-name,.gfm-highlight .token.class-name{color:var(--color-prettylights-syntax-variable)}.markdown-body .highlight .token.regex,.gfm-highlight .token.regex{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.regex .regex-delimiter,.gfm-highlight .token.regex .regex-delimiter{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.tag .token.tag,.markdown-body .highlight .token.property,.gfm-highlight .token.tag .token.tag,.gfm-highlight .token.property{color:var(--color-prettylights-syntax-entity-tag)}
    </style>
  </head>
  <body>
    <main data-color-mode="light" data-light-theme="light" data-dark-theme="dark" class="markdown-body">
      <h1>Awesome Jax Overview</h1>
<p>JAX - A curated list of resources https://github.com/google/jax</p>
<p><a href="/">üè† Home</a><span> ¬∑ </span><a href="https://www.trackawesomelist.com/n2cholas/awesome-jax/rss.xml">üî• Feed</a><span> ¬∑ </span><a href="https://trackawesomelist.us17.list-manage.com/subscribe?u=d2f0117aa829c83a63ec63c2f&id=36a103854c">üìÆ Subscribe</a><span> ¬∑ </span><a href="https://github.com/sponsors/theowenyoung">‚ù§Ô∏è  Sponsor</a><span> ¬∑ </span><a href="https://github.com/n2cholas/awesome-jax">üò∫ n2cholas/awesome-jax</a><span> ¬∑ </span><span>‚≠ê 1.9K</span><span> ¬∑ </span><span>üè∑Ô∏è Computer Science</span></p>
<p><span>[ </span><a href="/n2cholas/awesome-jax/">Daily</a><span> / </span><a href="/n2cholas/awesome-jax/week/">Weekly</a><span> / </span><span>Overview</span><span> ]</span></p>


<h1 id="awesome-jax-awesome"><a class="anchor" aria-hidden="true" tabindex="-1" href="#awesome-jax-awesome"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Awesome JAX <a href="https://awesome.re" rel="noopener noreferrer"><img src="https://awesome.re/badge.svg" alt="Awesome" /></a><a href="https://github.com/google/jax" rel="noopener noreferrer"><img src="https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png" alt="JAX Logo" align="right" height="100" /></a></h1>

<p><a href="https://github.com/google/jax" rel="noopener noreferrer">JAX</a> brings automatic differentiation and the <a href="https://www.tensorflow.org/xla" rel="noopener noreferrer">XLA compiler</a> together through a <a href="https://numpy.org/" rel="noopener noreferrer">NumPy</a>-like API for high performance machine learning research on accelerators like GPUs and TPUs.</p>


<p>This is a curated list of awesome JAX libraries, projects, and other resources. Contributions are welcome!</p>
<h2 id="contents"><a class="anchor" aria-hidden="true" tabindex="-1" href="#contents"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contents</h2><ul>
<li><a href="#libraries">Libraries</a></li>
<li><a href="#models-and-projects">Models and Projects</a></li>
<li><a href="#videos">Videos</a></li>
<li><a href="#papers">Papers</a></li>
<li><a href="#tutorials-and-blog-posts">Tutorials and Blog Posts</a></li>
<li><a href="#books">Books</a></li>
<li><a href="#community">Community</a></li>
</ul>
<a>

<h2 id="libraries"><a class="anchor" aria-hidden="true" tabindex="-1" href="#libraries"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Libraries</h2><ul>
<li>Neural Network Libraries<ul>
<li><a href="https://github.com/google/flax" rel="noopener noreferrer">Flax</a> - Centered on flexibility and clarity. <img src="https://img.shields.io/github/stars/google/flax?style=social" align="center" /></li>
<li><a href="https://github.com/google/flax/tree/main/flax/nnx" rel="noopener noreferrer">Flax NNX</a> - An evolution on Flax by the same team <img src="https://img.shields.io/github/stars/google/flax?style=social" align="center" /></li>
<li><a href="https://github.com/deepmind/dm-haiku" rel="noopener noreferrer">Haiku</a> - Focused on simplicity, created by the authors of Sonnet at DeepMind. <img src="https://img.shields.io/github/stars/deepmind/dm-haiku?style=social" align="center" /></li>
<li><a href="https://github.com/google/objax" rel="noopener noreferrer">Objax</a> - Has an object oriented design similar to PyTorch. <img src="https://img.shields.io/github/stars/google/objax?style=social" align="center" /></li>
<li><a href="https://poets-ai.github.io/elegy/" rel="noopener noreferrer">Elegy</a> - A High Level API for Deep Learning in JAX. Supports Flax, Haiku, and Optax. <img src="https://img.shields.io/github/stars/poets-ai/elegy?style=social" align="center" /></li>
<li><a href="https://github.com/google/trax" rel="noopener noreferrer">Trax</a> - "Batteries included" deep learning library focused on providing solutions for common workloads. <img src="https://img.shields.io/github/stars/google/trax?style=social" align="center" /></li>
<li><a href="https://github.com/deepmind/jraph" rel="noopener noreferrer">Jraph</a> - Lightweight graph neural network library. <img src="https://img.shields.io/github/stars/deepmind/jraph?style=social" align="center" /></li>
<li><a href="https://github.com/google/neural-tangents" rel="noopener noreferrer">Neural Tangents</a> - High-level API for specifying neural networks of both finite and <em>infinite</em> width. <img src="https://img.shields.io/github/stars/google/neural-tangents?style=social" align="center" /></li>
<li><a href="https://github.com/huggingface/transformers" rel="noopener noreferrer">HuggingFace Transformers</a> - Ecosystem of pretrained Transformers for a wide range of natural language tasks (Flax). <img src="https://img.shields.io/github/stars/huggingface/transformers?style=social" align="center" /></li>
<li><a href="https://github.com/patrick-kidger/equinox" rel="noopener noreferrer">Equinox</a> - Callable PyTrees and filtered JIT/grad transformations =&gt; neural networks in JAX. <img src="https://img.shields.io/github/stars/patrick-kidger/equinox?style=social" align="center" /></li>
<li><a href="https://github.com/google-research/scenic" rel="noopener noreferrer">Scenic</a> - A Jax Library for Computer Vision Research and Beyond.  <img src="https://img.shields.io/github/stars/google-research/scenic?style=social" align="center" /></li>
<li><a href="https://github.com/google-deepmind/penzai" rel="noopener noreferrer">Penzai</a> - Prioritizes legibility, visualization, and easy editing of neural network models with composable tools and a simple mental model.  <img src="https://img.shields.io/github/stars/google-deepmind/penzai?style=social" align="center" /></li>
</ul>
</li>
<li><a href="https://github.com/stanford-crfm/levanter" rel="noopener noreferrer">Levanter</a> - Legible, Scalable, Reproducible Foundation Models with Named Tensors and JAX.  <img src="https://img.shields.io/github/stars/stanford-crfm/levanter?style=social" align="center" /></li>
<li><a href="https://github.com/young-geng/EasyLM" rel="noopener noreferrer">EasyLM</a> - LLMs made easy: Pre-training, finetuning, evaluating and serving LLMs in JAX/Flax.  <img src="https://img.shields.io/github/stars/young-geng/EasyLM?style=social" align="center" /></li>
<li><a href="https://github.com/pyro-ppl/numpyro" rel="noopener noreferrer">NumPyro</a> - Probabilistic programming based on the Pyro library. <img src="https://img.shields.io/github/stars/pyro-ppl/numpyro?style=social" align="center" /></li>
<li><a href="https://github.com/deepmind/chex" rel="noopener noreferrer">Chex</a> - Utilities to write and test reliable JAX code. <img src="https://img.shields.io/github/stars/deepmind/chex?style=social" align="center" /></li>
<li><a href="https://github.com/deepmind/optax" rel="noopener noreferrer">Optax</a> - Gradient processing and optimization library. <img src="https://img.shields.io/github/stars/deepmind/optax?style=social" align="center" /></li>
<li><a href="https://github.com/deepmind/rlax" rel="noopener noreferrer">RLax</a> - Library for implementing reinforcement learning agents. <img src="https://img.shields.io/github/stars/deepmind/rlax?style=social" align="center" /></li>
<li><a href="https://github.com/google/jax-md" rel="noopener noreferrer">JAX, M.D.</a> - Accelerated, differential molecular dynamics. <img src="https://img.shields.io/github/stars/google/jax-md?style=social" align="center" /></li>
<li><a href="https://github.com/coax-dev/coax" rel="noopener noreferrer">Coax</a> - Turn RL papers into code, the easy way. <img src="https://img.shields.io/github/stars/coax-dev/coax?style=social" align="center" /></li>
<li><a href="https://github.com/deepmind/distrax" rel="noopener noreferrer">Distrax</a> - Reimplementation of TensorFlow Probability, containing probability distributions and bijectors. <img src="https://img.shields.io/github/stars/deepmind/distrax?style=social" align="center" /></li>
<li><a href="https://github.com/cvxgrp/cvxpylayers" rel="noopener noreferrer">cvxpylayers</a> - Construct differentiable convex optimization layers. <img src="https://img.shields.io/github/stars/cvxgrp/cvxpylayers?style=social" align="center" /></li>
<li><a href="https://github.com/tensorly/tensorly" rel="noopener noreferrer">TensorLy</a> - Tensor learning made simple. <img src="https://img.shields.io/github/stars/tensorly/tensorly?style=social" align="center" /></li>
<li><a href="https://github.com/netket/netket" rel="noopener noreferrer">NetKet</a> - Machine Learning toolbox for Quantum Physics. <img src="https://img.shields.io/github/stars/netket/netket?style=social" align="center" /></li>
<li><a href="https://github.com/awslabs/fortuna" rel="noopener noreferrer">Fortuna</a> - AWS library for Uncertainty Quantification in Deep Learning. <img src="https://img.shields.io/github/stars/awslabs/fortuna?style=social" align="center" /></li>
<li><a href="https://github.com/blackjax-devs/blackjax" rel="noopener noreferrer">BlackJAX</a> - Library of samplers for JAX. <img src="https://img.shields.io/github/stars/blackjax-devs/blackjax?style=social" align="center" /></li>
</ul>
<a>

<h3 id="new-libraries"><a class="anchor" aria-hidden="true" tabindex="-1" href="#new-libraries"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>New Libraries</h3><p>This section contains libraries that are well-made and useful, but have not necessarily been battle-tested by a large userbase yet.</p>
<ul>
<li>Neural Network Libraries<ul>
<li><a href="https://github.com/google/fedjax" rel="noopener noreferrer">FedJAX</a> - Federated learning in JAX, built on Optax and Haiku. <img src="https://img.shields.io/github/stars/google/fedjax?style=social" align="center" /></li>
<li><a href="https://github.com/mfinzi/equivariant-MLP" rel="noopener noreferrer">Equivariant MLP</a> - Construct equivariant neural network layers. <img src="https://img.shields.io/github/stars/mfinzi/equivariant-MLP?style=social" align="center" /></li>
<li><a href="https://github.com/n2cholas/jax-resnet/" rel="noopener noreferrer">jax-resnet</a> - Implementations and checkpoints for ResNet variants in Flax. <img src="https://img.shields.io/github/stars/n2cholas/jax-resnet?style=social" align="center" /></li>
<li><a href="https://github.com/srush/parallax" rel="noopener noreferrer">Parallax</a> - Immutable Torch Modules for JAX. <img src="https://img.shields.io/github/stars/srush/parallax?style=social" align="center" /></li>
</ul>
</li>
<li>Nonlinear Optimization<ul>
<li><a href="https://github.com/patrick-kidger/optimistix" rel="noopener noreferrer">Optimistix</a> - Root finding, minimisation, fixed points, and least squares. <img src="https://img.shields.io/github/stars/deepmind/optax?style=social" align="center" /></li>
<li><a href="https://github.com/google/jaxopt" rel="noopener noreferrer">JAXopt</a> - Hardware accelerated (GPU/TPU), batchable and differentiable optimizers in JAX. <img src="https://img.shields.io/github/stars/google/jaxopt?style=social" align="center" /></li>
</ul>
</li>
<li><a href="https://github.com/ElArkk/jax-unirep" rel="noopener noreferrer">jax-unirep</a> - Library implementing the <a href="https://www.nature.com/articles/s41592-019-0598-1" rel="noopener noreferrer">UniRep model</a> for protein machine learning applications. <img src="https://img.shields.io/github/stars/ElArkk/jax-unirep?style=social" align="center" /></li>
<li><a href="https://github.com/danielward27/flowjax" rel="noopener noreferrer">flowjax</a> - Distributions and normalizing flows built as equinox modules. <img src="https://img.shields.io/github/stars/danielward27/flowjax?style=social" align="center" /></li>
<li><a href="https://github.com/ChrisWaites/jax-flows" rel="noopener noreferrer">jax-flows</a> - Normalizing flows in JAX. <img src="https://img.shields.io/github/stars/ChrisWaites/jax-flows?style=social" align="center" /></li>
<li><a href="https://github.com/ExpectationMax/sklearn-jax-kernels" rel="noopener noreferrer">sklearn-jax-kernels</a> - <code>scikit-learn</code> kernel matrices using JAX. <img src="https://img.shields.io/github/stars/ExpectationMax/sklearn-jax-kernels?style=social" align="center" /></li>
<li><a href="https://github.com/DifferentiableUniverseInitiative/jax_cosmo" rel="noopener noreferrer">jax-cosmo</a> - Differentiable cosmology library. <img src="https://img.shields.io/github/stars/DifferentiableUniverseInitiative/jax_cosmo?style=social" align="center" /></li>
<li><a href="https://github.com/NeilGirdhar/efax" rel="noopener noreferrer">efax</a> - Exponential Families in JAX. <img src="https://img.shields.io/github/stars/NeilGirdhar/efax?style=social" align="center" /></li>
<li><a href="https://github.com/PhilipVinc/mpi4jax" rel="noopener noreferrer">mpi4jax</a> - Combine MPI operations with your Jax code on CPUs and GPUs. <img src="https://img.shields.io/github/stars/PhilipVinc/mpi4jax?style=social" align="center" /></li>
<li><a href="https://github.com/4rtemi5/imax" rel="noopener noreferrer">imax</a> - Image augmentations and transformations. <img src="https://img.shields.io/github/stars/4rtemi5/imax?style=social" align="center" /></li>
<li><a href="https://github.com/rolandgvc/flaxvision" rel="noopener noreferrer">FlaxVision</a> - Flax version of TorchVision. <img src="https://img.shields.io/github/stars/rolandgvc/flaxvision?style=social" align="center" /></li>
<li><a href="https://github.com/tensorflow/probability/tree/master/spinoffs/oryx" rel="noopener noreferrer">Oryx</a> - Probabilistic programming language based on program transformations.</li>
<li><a href="https://github.com/google-research/ott" rel="noopener noreferrer">Optimal Transport Tools</a> - Toolbox that bundles utilities to solve optimal transport problems.</li>
<li><a href="https://github.com/romanodev/deltapv" rel="noopener noreferrer">delta PV</a> - A photovoltaic simulator with automatic differentation. <img src="https://img.shields.io/github/stars/romanodev/deltapv?style=social" align="center" /></li>
<li><a href="https://github.com/brentyi/jaxlie" rel="noopener noreferrer">jaxlie</a> - Lie theory library for rigid body transformations and optimization. <img src="https://img.shields.io/github/stars/brentyi/jaxlie?style=social" align="center" /></li>
<li><a href="https://github.com/google/brax" rel="noopener noreferrer">BRAX</a> - Differentiable physics engine to simulate environments along with learning algorithms to train agents for these environments. <img src="https://img.shields.io/github/stars/google/brax?style=social" align="center" /></li>
<li><a href="https://github.com/matthias-wright/flaxmodels" rel="noopener noreferrer">flaxmodels</a> - Pretrained models for Jax/Flax. <img src="https://img.shields.io/github/stars/matthias-wright/flaxmodels?style=social" align="center" /></li>
<li><a href="https://github.com/carnotresearch/cr-sparse" rel="noopener noreferrer">CR.Sparse</a> - XLA accelerated algorithms for sparse representations and compressive sensing. <img src="https://img.shields.io/github/stars/carnotresearch/cr-sparse?style=social" align="center" /></li>
<li><a href="https://github.com/HajimeKawahara/exojax" rel="noopener noreferrer">exojax</a> - Automatic differentiable spectrum modeling of exoplanets/brown dwarfs compatible to JAX. <img src="https://img.shields.io/github/stars/HajimeKawahara/exojax?style=social" align="center" /></li>
<li><a href="https://github.com/deepmind/dm_pix" rel="noopener noreferrer">PIX</a> - PIX is an image processing library in JAX, for JAX. <img src="https://img.shields.io/github/stars/deepmind/dm_pix?style=social" align="center" /></li>
<li><a href="https://github.com/alonfnt/bayex" rel="noopener noreferrer">bayex</a> - Bayesian Optimization powered by JAX. <img src="https://img.shields.io/github/stars/alonfnt/bayex?style=social" align="center" /></li>
<li><a href="https://github.com/ucl-bug/jaxdf" rel="noopener noreferrer">JaxDF</a> - Framework for differentiable simulators with arbitrary discretizations. <img src="https://img.shields.io/github/stars/ucl-bug/jaxdf?style=social" align="center" /></li>
<li><a href="https://github.com/google/tree-math" rel="noopener noreferrer">tree-math</a> - Convert functions that operate on arrays into functions that operate on PyTrees. <img src="https://img.shields.io/github/stars/google/tree-math?style=social" align="center" /></li>
<li><a href="https://github.com/DarshanDeshpande/jax-models" rel="noopener noreferrer">jax-models</a> - Implementations of research papers originally without code or code written with frameworks other than JAX. <img src="https://img.shields.io/github/stars/DarshanDeshpande/jax-modelsa?style=social" align="center" /></li>
<li><a href="https://github.com/vicariousinc/PGMax" rel="noopener noreferrer">PGMax</a> - A framework for building discrete Probabilistic Graphical Models (PGM's) and running inference inference on them via JAX. <img src="https://img.shields.io/github/stars/vicariousinc/pgmax?style=social" align="center" /></li>
<li><a href="https://github.com/google/evojax" rel="noopener noreferrer">EvoJAX</a> - Hardware-Accelerated Neuroevolution <img src="https://img.shields.io/github/stars/google/evojax?style=social" align="center" /></li>
<li><a href="https://github.com/RobertTLange/evosax" rel="noopener noreferrer">evosax</a> - JAX-Based Evolution Strategies <img src="https://img.shields.io/github/stars/RobertTLange/evosax?style=social" align="center" /></li>
<li><a href="https://github.com/SymJAX/SymJAX" rel="noopener noreferrer">SymJAX</a> - Symbolic CPU/GPU/TPU programming. <img src="https://img.shields.io/github/stars/SymJAX/SymJAX?style=social" align="center" /></li>
<li><a href="https://github.com/rlouf/mcx" rel="noopener noreferrer">mcx</a> - Express &amp; compile probabilistic programs for performant inference. <img src="https://img.shields.io/github/stars/rlouf/mcx?style=social" align="center" /></li>
<li><a href="https://github.com/deepmind/einshape" rel="noopener noreferrer">Einshape</a> - DSL-based reshaping library for JAX and other frameworks. <img src="https://img.shields.io/github/stars/deepmind/einshape?style=social" align="center" /></li>
<li><a href="https://github.com/google-research/google-research/tree/master/alx" rel="noopener noreferrer">ALX</a> - Open-source library for distributed matrix factorization using Alternating Least Squares, more info in <a href="https://arxiv.org/abs/2112.02194" rel="noopener noreferrer"><em>ALX: Large Scale Matrix Factorization on TPUs</em></a>.</li>
<li><a href="https://github.com/patrick-kidger/diffrax" rel="noopener noreferrer">Diffrax</a> - Numerical differential equation solvers in JAX. <img src="https://img.shields.io/github/stars/patrick-kidger/diffrax?style=social" align="center" /></li>
<li><a href="https://github.com/dfm/tinygp" rel="noopener noreferrer">tinygp</a> - The <em>tiniest</em> of Gaussian process libraries in JAX. <img src="https://img.shields.io/github/stars/dfm/tinygp?style=social" align="center" /></li>
<li><a href="https://github.com/RobertTLange/gymnax" rel="noopener noreferrer">gymnax</a> - Reinforcement Learning Environments with the well-known gym API. <img src="https://img.shields.io/github/stars/RobertTLange/gymnax?style=social" align="center" /></li>
<li><a href="https://github.com/deepmind/mctx" rel="noopener noreferrer">Mctx</a> - Monte Carlo tree search algorithms in native JAX. <img src="https://img.shields.io/github/stars/deepmind/mctx?style=social" align="center" /></li>
<li><a href="https://github.com/deepmind/kfac-jax" rel="noopener noreferrer">KFAC-JAX</a> - Second Order Optimization with Approximate Curvature for NNs. <img src="https://img.shields.io/github/stars/deepmind/kfac-jax?style=social" align="center" /></li>
<li><a href="https://github.com/deepmind/tf2jax" rel="noopener noreferrer">TF2JAX</a> - Convert functions/graphs to JAX functions. <img src="https://img.shields.io/github/stars/deepmind/tf2jax?style=social" align="center" /></li>
<li><a href="https://github.com/ucl-bug/jwave" rel="noopener noreferrer">jwave</a> - A library for differentiable acoustic simulations <img src="https://img.shields.io/github/stars/ucl-bug/jwave?style=social" align="center" /></li>
<li><a href="https://github.com/thomaspinder/GPJax" rel="noopener noreferrer">GPJax</a> - Gaussian processes in JAX.</li>
<li><a href="https://github.com/instadeepai/jumanji" rel="noopener noreferrer">Jumanji</a> - A Suite of Industry-Driven Hardware-Accelerated RL Environments written in JAX. <img src="https://img.shields.io/github/stars/instadeepai/jumanji?style=social" align="center" /></li>
<li><a href="https://github.com/paganpasta/eqxvision" rel="noopener noreferrer">Eqxvision</a> - Equinox version of Torchvision. <img src="https://img.shields.io/github/stars/paganpasta/eqxvision?style=social" align="center" /></li>
<li><a href="https://github.com/dipolar-quantum-gases/jaxfit" rel="noopener noreferrer">JAXFit</a> - Accelerated curve fitting library for nonlinear least-squares problems (see <a href="https://arxiv.org/abs/2208.12187" rel="noopener noreferrer">arXiv paper</a>). <img src="https://img.shields.io/github/stars/dipolar-quantum-gases/jaxfit?style=social" align="center" /></li>
<li><a href="https://github.com/gboehl/econpizza" rel="noopener noreferrer">econpizza</a> - Solve macroeconomic models with hetereogeneous agents using JAX. <img src="https://img.shields.io/github/stars/gboehl/econpizza?style=social" align="center" /></li>
<li><a href="https://github.com/secretflow/spu" rel="noopener noreferrer">SPU</a> - A domain-specific compiler and runtime suite to run JAX code with MPC(Secure Multi-Party Computation). <img src="https://img.shields.io/github/stars/secretflow/spu?style=social" align="center" /></li>
<li><a href="https://github.com/jeremiecoullon/jax-tqdm" rel="noopener noreferrer">jax-tqdm</a> - Add a tqdm progress bar to JAX scans and loops. <img src="https://img.shields.io/github/stars/jeremiecoullon/jax-tqdm?style=social" align="center" /></li>
<li><a href="https://github.com/alvarobartt/safejax" rel="noopener noreferrer">safejax</a> - Serialize JAX, Flax, Haiku, or Objax model params with ü§ó<code>safetensors</code>. <img src="https://img.shields.io/github/stars/alvarobartt/safejax?style=social" align="center" /></li>
<li><a href="https://github.com/ASEM000/kernex" rel="noopener noreferrer">Kernex</a> - Differentiable stencil decorators in JAX. <img src="https://img.shields.io/github/stars/ASEM000/kernex?style=social" align="center" /></li>
<li><a href="https://github.com/google/maxtext" rel="noopener noreferrer">MaxText</a> - A simple, performant and scalable Jax LLM written in pure Python/Jax and targeting Google Cloud TPUs. <img src="https://img.shields.io/github/stars/google/maxtext?style=social" align="center" /></li>
<li><a href="https://github.com/google/paxml" rel="noopener noreferrer">Pax</a> - A Jax-based machine learning framework for training large scale models. <img src="https://img.shields.io/github/stars/google/paxml?style=social" align="center" /></li>
<li><a href="https://github.com/google/praxis" rel="noopener noreferrer">Praxis</a> - The layer library for Pax with a goal to be usable by other JAX-based ML projects. <img src="https://img.shields.io/github/stars/google/praxis?style=social" align="center" /></li>
<li><a href="https://github.com/luchris429/purejaxrl" rel="noopener noreferrer">purejaxrl</a> - Vectorisable, end-to-end RL algorithms in JAX. <img src="https://img.shields.io/github/stars/luchris429/purejaxrl?style=social" align="center" /></li>
<li><a href="https://github.com/davisyoshida/lorax" rel="noopener noreferrer">Lorax</a> - Automatically apply LoRA to JAX models (Flax, Haiku, etc.)</li>
<li><a href="https://github.com/lanl/scico" rel="noopener noreferrer">SCICO</a> - Scientific computational imaging in JAX. <img src="https://img.shields.io/github/stars/lanl/scico?style=social" align="center" /></li>
<li><a href="https://github.com/kmheckel/spyx" rel="noopener noreferrer">Spyx</a> - Spiking Neural Networks in JAX for machine learning on neuromorphic hardware. <img src="https://img.shields.io/github/stars/kmheckel/spyx?style=social" align="center" /></li>
<li>Brain Dynamics Programming Ecosystem<ul>
<li><a href="https://github.com/brainpy/BrainPy" rel="noopener noreferrer">BrainPy</a> - Brain Dynamics Programming in Python. <img src="https://img.shields.io/github/stars/brainpy/BrainPy?style=social" align="center" /></li>
<li><a href="https://github.com/chaobrain/brainunit" rel="noopener noreferrer">brainunit</a> - Physical units and unit-aware mathematical system in JAX. <img src="https://img.shields.io/github/stars/chaobrain/brainunit?style=social" align="center" /></li>
<li><a href="https://github.com/chaobrain/dendritex" rel="noopener noreferrer">dendritex</a> - Dendritic Modeling in JAX. <img src="https://img.shields.io/github/stars/chaobrain/dendritex?style=social" align="center" /></li>
<li><a href="https://github.com/chaobrain/brainstate" rel="noopener noreferrer">brainstate</a> - State-based Transformation System for Program Compilation and Augmentation. <img src="https://img.shields.io/github/stars/chaobrain/brainstate?style=social" align="center" /></li>
<li><a href="https://github.com/chaobrain/braintaichi" rel="noopener noreferrer">braintaichi</a> - Leveraging Taichi Lang to customize brain dynamics operators. <img src="https://img.shields.io/github/stars/chaobrain/braintaichi?style=social" align="center" /></li>
</ul>
</li>
<li><a href="https://github.com/ott-jax/ott" rel="noopener noreferrer">OTT-JAX</a> - Optimal transport tools in JAX. <img src="https://img.shields.io/github/stars/ott-jax/ott?style=social" align="center" /></li>
<li><a href="https://github.com/adaptive-intelligent-robotics/QDax" rel="noopener noreferrer">QDax</a> - Quality Diversity optimization in Jax. <img src="https://img.shields.io/github/stars/adaptive-intelligent-robotics/QDax?style=social" align="center" /></li>
<li><a href="https://github.com/NVIDIA/JAX-Toolbox" rel="noopener noreferrer">JAX Toolbox</a> - Nightly CI and optimized examples for JAX on NVIDIA GPUs using libraries such as T5x, Paxml, and Transformer Engine. <img src="https://img.shields.io/github/stars/NVIDIA/JAX-Toolbox?style=social" align="center" /></li>
<li><a href="http://github.com/sotetsuk/pgx" rel="noopener noreferrer">Pgx</a> - Vectorized board game environments for RL with an AlphaZero example. <img src="https://img.shields.io/github/stars/sotetsuk/pgx?style=social" align="center" /></li>
<li><a href="https://github.com/erfanzar/EasyDeL" rel="noopener noreferrer">EasyDeL</a> - EasyDeL üîÆ is an OpenSource Library to make your training faster and more Optimized With cool Options for training and serving (Llama, MPT, Mixtral, Falcon, etc) in JAX <img src="https://img.shields.io/github/stars/erfanzar/EasyDeL?style=social" align="center" /></li>
<li><a href="https://github.com/Autodesk/XLB" rel="noopener noreferrer">XLB</a> - A Differentiable Massively Parallel Lattice Boltzmann Library in Python for Physics-Based Machine Learning. <img src="https://img.shields.io/github/stars/Autodesk/XLB?style=social" align="center" /></li>
<li><a href="https://github.com/dynamiqs/dynamiqs" rel="noopener noreferrer">dynamiqs</a> - High-performance and differentiable simulations of quantum systems with JAX. <img src="https://img.shields.io/github/stars/dynamiqs/dynamiqs?style=social" align="center" /></li>
<li><a href="https://github.com/i-m-iron-man/Foragax" rel="noopener noreferrer">foragax</a> - Agent-Based modelling framework in JAX.  <img src="https://img.shields.io/github/stars/i-m-iron-man/Foragax?style=social" align="center" /></li>
<li><a href="https://github.com/bahremsd/tmmax" rel="noopener noreferrer">tmmax</a> - Vectorized calculation of optical properties in thin-film structures using JAX. Swiss Army knife tool for thin-film optics research <img src="https://img.shields.io/github/stars/bahremsd/tmmax" align="center" /></li>
<li><a href="https://github.com/gchq/coreax" rel="noopener noreferrer">Coreax</a> - Algorithms for finding coresets to compress large datasets while retaining their statistical properties. <img src="https://img.shields.io/github/stars/gchq/coreax?style=social" align="center" /></li>
<li><a href="https://github.com/epignatelli/navix" rel="noopener noreferrer">NAVIX</a> - A reimplementation of MiniGrid, a Reinforcement Learning environment, in JAX <img src="https://img.shields.io/github/stars/epignatelli/navix?style=social" align="center" /></li>
</ul>
<a>

<h2 id="models-and-projects"><a class="anchor" aria-hidden="true" tabindex="-1" href="#models-and-projects"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Models and Projects</h2><h3 id="jax"><a class="anchor" aria-hidden="true" tabindex="-1" href="#jax"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>JAX</h3><ul>
<li><a href="https://github.com/tancik/fourier-feature-networks" rel="noopener noreferrer">Fourier Feature Networks</a> - Official implementation of <a href="https://people.eecs.berkeley.edu/~bmild/fourfeat" rel="noopener noreferrer"><em>Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</em></a>.</li>
<li><a href="https://github.com/AaltoML/kalman-jax" rel="noopener noreferrer">kalman-jax</a> - Approximate inference for Markov (i.e., temporal) Gaussian processes using iterated Kalman filtering and smoothing.</li>
<li><a href="https://github.com/Joshuaalbert/jaxns" rel="noopener noreferrer">jaxns</a> - Nested sampling in JAX.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/amortized_bo" rel="noopener noreferrer">Amortized Bayesian Optimization</a> - Code related to <a href="http://www.auai.org/uai2020/proceedings/329_main_paper.pdf" rel="noopener noreferrer"><em>Amortized Bayesian Optimization over Discrete Spaces</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/aqt" rel="noopener noreferrer">Accurate Quantized Training</a> - Tools and libraries for running and analyzing neural network quantization experiments in JAX and Flax.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/bnn_hmc" rel="noopener noreferrer">BNN-HMC</a> - Implementation for the paper <a href="https://arxiv.org/abs/2104.14421" rel="noopener noreferrer"><em>What Are Bayesian Neural Network Posteriors Really Like?</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/jax_dft" rel="noopener noreferrer">JAX-DFT</a> - One-dimensional density functional theory (DFT) in JAX, with implementation of <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.126.036401" rel="noopener noreferrer"><em>Kohn-Sham equations as regularizer: building prior knowledge into machine-learned physics</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/robust_loss_jax" rel="noopener noreferrer">Robust Loss</a> - Reference code for the paper <a href="https://arxiv.org/abs/1701.03077" rel="noopener noreferrer"><em>A General and Adaptive Robust Loss Function</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/symbolic_functionals" rel="noopener noreferrer">Symbolic Functionals</a> - Demonstration from <a href="https://arxiv.org/abs/2203.02540" rel="noopener noreferrer"><em>Evolving symbolic density functionals</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/trimap" rel="noopener noreferrer">TriMap</a> - Official JAX implementation of <a href="https://arxiv.org/abs/1910.00204" rel="noopener noreferrer"><em>TriMap: Large-scale Dimensionality Reduction Using Triplets</em></a>.</li>
</ul>
<h3 id="flax"><a class="anchor" aria-hidden="true" tabindex="-1" href="#flax"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Flax</h3><ul>
<li><a href="https://github.com/J-Rosser-UK/Torch2Jax-DeepSeek-R1-Distill-Qwen-1.5B" rel="noopener noreferrer">DeepSeek-R1-Flax-1.5B-Distill</a> - Flax implementation of DeepSeek-R1 1.5B distilled reasoning LLM.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/performer/fast_attention/jax" rel="noopener noreferrer">Performer</a> - Flax implementation of the Performer (linear transformer via FAVOR+) architecture.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/jaxnerf" rel="noopener noreferrer">JaxNeRF</a> - Implementation of <a href="http://www.matthewtancik.com/nerf" rel="noopener noreferrer"><em>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</em></a> with multi-device GPU/TPU support.</li>
<li><a href="https://github.com/google/mipnerf" rel="noopener noreferrer">mip-NeRF</a> - Official implementation of <a href="https://jonbarron.info/mipnerf" rel="noopener noreferrer"><em>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/regnerf" rel="noopener noreferrer">RegNeRF</a> - Official implementation of <a href="https://m-niemeyer.github.io/regnerf/" rel="noopener noreferrer"><em>RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs</em></a>.</li>
<li><a href="https://github.com/huangjuite/jaxneus" rel="noopener noreferrer">JaxNeuS</a> - Implementation of <a href="https://lingjie0206.github.io/papers/NeuS/" rel="noopener noreferrer"><em>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction</em></a></li>
<li><a href="https://github.com/google-research/big_transfer" rel="noopener noreferrer">Big Transfer (BiT)</a> - Implementation of <a href="https://arxiv.org/abs/1912.11370" rel="noopener noreferrer"><em>Big Transfer (BiT): General Visual Representation Learning</em></a>.</li>
<li><a href="https://github.com/ikostrikov/jax-rl" rel="noopener noreferrer">JAX RL</a> - Implementations of reinforcement learning algorithms.</li>
<li><a href="https://github.com/SauravMaheshkar/gMLP" rel="noopener noreferrer">gMLP</a> - Implementation of <a href="https://arxiv.org/abs/2105.08050" rel="noopener noreferrer"><em>Pay Attention to MLPs</em></a>.</li>
<li><a href="https://github.com/SauravMaheshkar/MLP-Mixer" rel="noopener noreferrer">MLP Mixer</a> - Minimal implementation of <a href="https://arxiv.org/abs/2105.01601" rel="noopener noreferrer"><em>MLP-Mixer: An all-MLP Architecture for Vision</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/scalable_shampoo" rel="noopener noreferrer">Distributed Shampoo</a> - Implementation of <a href="https://arxiv.org/abs/2002.09018" rel="noopener noreferrer"><em>Second Order Optimization Made Practical</em></a>.</li>
<li><a href="https://github.com/google-research/nested-transformer" rel="noopener noreferrer">NesT</a> - Official implementation of <a href="https://arxiv.org/abs/2105.12723" rel="noopener noreferrer"><em>Aggregating Nested Transformers</em></a>.</li>
<li><a href="https://github.com/google-research/xmcgan_image_generation" rel="noopener noreferrer">XMC-GAN</a> - Official implementation of <a href="https://arxiv.org/abs/2101.04702" rel="noopener noreferrer"><em>Cross-Modal Contrastive Learning for Text-to-Image Generation</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/f_net" rel="noopener noreferrer">FNet</a> - Official implementation of <a href="https://arxiv.org/abs/2105.03824" rel="noopener noreferrer"><em>FNet: Mixing Tokens with Fourier Transforms</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/gfsa" rel="noopener noreferrer">GFSA</a> - Official implementation of <a href="https://arxiv.org/abs/2007.04929" rel="noopener noreferrer"><em>Learning Graph Structure With A Finite-State Automaton Layer</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/ipagnn" rel="noopener noreferrer">IPA-GNN</a> - Official implementation of <a href="https://arxiv.org/abs/2010.12621" rel="noopener noreferrer"><em>Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/flax_models" rel="noopener noreferrer">Flax Models</a> - Collection of models and methods implemented in Flax.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/protein_lm" rel="noopener noreferrer">Protein LM</a> - Implements BERT and autoregressive models for proteins, as described in <a href="https://www.biorxiv.org/content/10.1101/622803v1.full" rel="noopener noreferrer"><em>Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences</em></a> and <a href="https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2" rel="noopener noreferrer"><em>ProGen: Language Modeling for Protein Generation</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/ptopk_patch_selection" rel="noopener noreferrer">Slot Attention</a> - Reference implementation for <a href="https://arxiv.org/abs/2104.03059" rel="noopener noreferrer"><em>Differentiable Patch Selection for Image Recognition</em></a>.</li>
<li><a href="https://github.com/google-research/vision_transformer" rel="noopener noreferrer">Vision Transformer</a> - Official implementation of <a href="https://arxiv.org/abs/2010.11929" rel="noopener noreferrer"><em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em></a>.</li>
<li><a href="https://github.com/matthias-wright/jax-fid" rel="noopener noreferrer">FID computation</a> - Port of <a href="https://github.com/mseitzer/pytorch-fid" rel="noopener noreferrer">mseitzer/pytorch-fid</a> to Flax.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/autoregressive_diffusion" rel="noopener noreferrer">ARDM</a> - Official implementation of <a href="https://arxiv.org/abs/2110.02037" rel="noopener noreferrer"><em>Autoregressive Diffusion Models</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/d3pm" rel="noopener noreferrer">D3PM</a> - Official implementation of <a href="https://arxiv.org/abs/2107.03006" rel="noopener noreferrer"><em>Structured Denoising Diffusion Models in Discrete State-Spaces</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/gumbel_max_causal_gadgets" rel="noopener noreferrer">Gumbel-max Causal Mechanisms</a> - Code for <a href="https://arxiv.org/abs/2111.06888" rel="noopener noreferrer"><em>Learning Generalized Gumbel-max Causal Mechanisms</em></a>, with extra code in <a href="https://github.com/GuyLor/gumbel_max_causal_gadgets_part2" rel="noopener noreferrer">GuyLor/gumbel_max_causal_gadgets_part2</a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/latent_programmer" rel="noopener noreferrer">Latent Programmer</a> - Code for the ICML 2021 paper <a href="https://arxiv.org/abs/2012.00377" rel="noopener noreferrer"><em>Latent Programmer: Discrete Latent Codes for Program Synthesis</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/snerg" rel="noopener noreferrer">SNeRG</a> - Official implementation of <a href="https://phog.github.io/snerg" rel="noopener noreferrer"><em>Baking Neural Radiance Fields for Real-Time View Synthesis</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/spin_spherical_cnns" rel="noopener noreferrer">Spin-weighted Spherical CNNs</a> - Adaptation of <a href="https://arxiv.org/abs/2006.10731" rel="noopener noreferrer"><em>Spin-Weighted Spherical CNNs</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/vdvae_flax" rel="noopener noreferrer">VDVAE</a> - Adaptation of <a href="https://arxiv.org/abs/2011.10650" rel="noopener noreferrer"><em>Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images</em></a>, original code at <a href="https://github.com/openai/vdvae" rel="noopener noreferrer">openai/vdvae</a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/musiq" rel="noopener noreferrer">MUSIQ</a> - Checkpoints and model inference code for the ICCV 2021 paper <a href="https://arxiv.org/abs/2108.05997" rel="noopener noreferrer"><em>MUSIQ: Multi-scale Image Quality Transformer</em></a></li>
<li><a href="https://github.com/google-research/google-research/tree/master/aquadem" rel="noopener noreferrer">AQuaDem</a> - Official implementation of <a href="https://arxiv.org/abs/2110.10149" rel="noopener noreferrer"><em>Continuous Control with Action Quantization from Demonstrations</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/combiner" rel="noopener noreferrer">Combiner</a> - Official implementation of <a href="https://arxiv.org/abs/2107.05768" rel="noopener noreferrer"><em>Combiner: Full Attention Transformer with Sparse Computation Cost</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/dreamfields" rel="noopener noreferrer">Dreamfields</a> - Official implementation of the ICLR 2022 paper <a href="https://ajayj.com/dreamfields" rel="noopener noreferrer"><em>Progressive Distillation for Fast Sampling of Diffusion Models</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/gift" rel="noopener noreferrer">GIFT</a> - Official implementation of <a href="https://arxiv.org/abs/2106.06080" rel="noopener noreferrer"><em>Gradual Domain Adaptation in the Wild:When Intermediate Distributions are Absent</em></a>.</li>
<li><a href="https://github.com/google-research/google-research/tree/master/light_field_neural_rendering" rel="noopener noreferrer">Light Field Neural Rendering</a> - Official implementation of <a href="https://arxiv.org/abs/2112.09687" rel="noopener noreferrer"><em>Light Field Neural Rendering</em></a>.</li>
<li><a href="https://colab.research.google.com/drive/1KUKFEMneQMS3OzPYnWZGkEnry3PdzCfn?usp=sharing" rel="noopener noreferrer">Sharpened Cosine Similarity in JAX by Raphael Pisoni</a> -  A JAX/Flax implementation of the Sharpened Cosine Similarity layer.</li>
<li><a href="https://github.com/IvanIsCoding/GNN-for-Combinatorial-Optimization" rel="noopener noreferrer">GNNs for Solving Combinatorial Optimization Problems</a> -  A JAX + Flax implementation of <a href="https://arxiv.org/abs/2107.01188" rel="noopener noreferrer">Combinatorial Optimization with Physics-Inspired Graph Neural Networks</a>.</li>
<li><a href="https://github.com/MasterSkepticista/detr" rel="noopener noreferrer">DETR</a> - Flax implementation of <a href="https://github.com/facebookresearch/detr" rel="noopener noreferrer"><em>DETR: End-to-end Object Detection with Transformers</em></a> using Sinkhorn solver and parallel bipartite matching.</li>
</ul>
<h3 id="haiku"><a class="anchor" aria-hidden="true" tabindex="-1" href="#haiku"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Haiku</h3><ul>
<li><a href="https://github.com/deepmind/alphafold" rel="noopener noreferrer">AlphaFold</a> - Implementation of the inference pipeline of AlphaFold v2.0, presented in <a href="https://www.nature.com/articles/s41586-021-03819-2" rel="noopener noreferrer"><em>Highly accurate protein structure prediction with AlphaFold</em></a>.</li>
<li><a href="https://github.com/deepmind/deepmind-research/tree/master/adversarial_robustness" rel="noopener noreferrer">Adversarial Robustness</a> - Reference code for <a href="https://arxiv.org/abs/2010.03593" rel="noopener noreferrer"><em>Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples</em></a> and <a href="https://arxiv.org/abs/2103.01946" rel="noopener noreferrer"><em>Fixing Data Augmentation to Improve Adversarial Robustness</em></a>.</li>
<li><a href="https://github.com/deepmind/deepmind-research/tree/master/byol" rel="noopener noreferrer">Bootstrap Your Own Latent</a> - Implementation for the paper <a href="https://arxiv.org/abs/2006.07733" rel="noopener noreferrer"><em>Bootstrap your own latent: A new approach to self-supervised Learning</em></a>.</li>
<li><a href="https://github.com/deepmind/deepmind-research/tree/master/gated_linear_networks" rel="noopener noreferrer">Gated Linear Networks</a> - GLNs are a family of backpropagation-free neural networks.</li>
<li><a href="https://github.com/deepmind/deepmind-research/tree/master/glassy_dynamics" rel="noopener noreferrer">Glassy Dynamics</a> - Open source implementation of the paper <a href="https://www.nature.com/articles/s41567-020-0842-8" rel="noopener noreferrer"><em>Unveiling the predictive power of static structure in glassy systems</em></a>.</li>
<li><a href="https://github.com/deepmind/deepmind-research/tree/master/mmv" rel="noopener noreferrer">MMV</a> - Code for the models in <a href="https://arxiv.org/abs/2006.16228" rel="noopener noreferrer"><em>Self-Supervised MultiModal Versatile Networks</em></a>.</li>
<li><a href="https://github.com/deepmind/deepmind-research/tree/master/nfnets" rel="noopener noreferrer">Normalizer-Free Networks</a> - Official Haiku implementation of <a href="https://arxiv.org/abs/2102.06171" rel="noopener noreferrer"><em>NFNets</em></a>.</li>
<li><a href="https://github.com/Information-Fusion-Lab-Umass/NuX" rel="noopener noreferrer">NuX</a> - Normalizing flows with JAX.</li>
<li><a href="https://github.com/deepmind/deepmind-research/tree/master/ogb_lsc" rel="noopener noreferrer">OGB-LSC</a> - This repository contains DeepMind's entry to the <a href="https://ogb.stanford.edu/kddcup2021/pcqm4m/" rel="noopener noreferrer">PCQM4M-LSC</a> (quantum chemistry) and <a href="https://ogb.stanford.edu/kddcup2021/mag240m/" rel="noopener noreferrer">MAG240M-LSC</a> (academic graph)
tracks of the <a href="https://ogb.stanford.edu/kddcup2021/" rel="noopener noreferrer">OGB Large-Scale Challenge</a> (OGB-LSC).</li>
<li><a href="https://github.com/google-research/google-research/tree/master/persistent_es" rel="noopener noreferrer">Persistent Evolution Strategies</a> - Code used for the paper <a href="http://proceedings.mlr.press/v139/vicol21a.html" rel="noopener noreferrer"><em>Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies</em></a>.</li>
<li><a href="https://github.com/degregat/two-player-auctions" rel="noopener noreferrer">Two Player Auction Learning</a> - JAX implementation of the paper <a href="https://arxiv.org/abs/2006.05684" rel="noopener noreferrer"><em>Auction learning as a two-player game</em></a>.</li>
<li><a href="https://github.com/deepmind/deepmind-research/tree/master/wikigraphs" rel="noopener noreferrer">WikiGraphs</a> - Baseline code to reproduce results in <a href="https://aclanthology.org/2021.textgraphs-1.7" rel="noopener noreferrer"><em>WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Datase</em></a>.</li>
</ul>
<h3 id="trax"><a class="anchor" aria-hidden="true" tabindex="-1" href="#trax"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Trax</h3><ul>
<li><a href="https://github.com/google/trax/tree/master/trax/models/reformer" rel="noopener noreferrer">Reformer</a> - Implementation of the Reformer (efficient transformer) architecture.</li>
</ul>
<h3 id="numpyro"><a class="anchor" aria-hidden="true" tabindex="-1" href="#numpyro"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>NumPyro</h3><ul>
<li><a href="https://github.com/RothkopfLab/lqg" rel="noopener noreferrer">lqg</a> - Official implementation of Bayesian inverse optimal control for linear-quadratic Gaussian problems from the paper <a href="https://elifesciences.org/articles/76635" rel="noopener noreferrer"><em>Putting perception into action with inverse optimal control for continuous psychophysics</em></a></li>
</ul>
<a>

<h2 id="videos"><a class="anchor" aria-hidden="true" tabindex="-1" href="#videos"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Videos</h2><ul>
<li><a href="https://www.youtube.com/watch?v=iDxJxIyzSiM" rel="noopener noreferrer">NeurIPS 2020: JAX Ecosystem Meetup</a> - JAX, its use at DeepMind, and discussion between engineers, scientists, and JAX core team.</li>
<li><a href="https://youtu.be/0mVmRHMaOJ4" rel="noopener noreferrer">Introduction to JAX</a> - Simple neural network from scratch in JAX.</li>
<li><a href="https://youtu.be/z-WSrQDXkuM" rel="noopener noreferrer">JAX: Accelerated Machine Learning Research | SciPy 2020 | VanderPlas</a> - JAX's core design, how it's powering new research, and how you can start using it.</li>
<li><a href="https://youtu.be/CecuWGpoztw" rel="noopener noreferrer">Bayesian Programming with JAX + NumPyro ‚Äî Andy Kitchen</a> - Introduction to Bayesian modelling using NumPyro.</li>
<li><a href="https://slideslive.com/38923687/jax-accelerated-machinelearning-research-via-composable-function-transformations-in-python" rel="noopener noreferrer">JAX: Accelerated machine-learning research via composable function transformations in Python | NeurIPS 2019 | Skye Wanderman-Milne</a> - JAX intro presentation in <a href="https://program-transformations.github.io" rel="noopener noreferrer"><em>Program Transformations for Machine Learning</em></a> workshop.</li>
<li><a href="https://drive.google.com/file/d/1jKxefZT1xJDUxMman6qrQVed7vWI0MIn/edit" rel="noopener noreferrer">JAX on Cloud TPUs | NeurIPS 2020 | Skye Wanderman-Milne and James Bradbury</a> - Presentation of TPU host access with demo.</li>
<li><a href="https://slideslive.com/38935810/deep-implicit-layers-neural-odes-equilibrium-models-and-beyond" rel="noopener noreferrer">Deep Implicit Layers - Neural ODEs, Deep Equilibirum Models, and Beyond | NeurIPS 2020</a> - Tutorial created by Zico Kolter, David Duvenaud, and Matt Johnson with Colab notebooks avaliable in <a href="http://implicit-layers-tutorial.org" rel="noopener noreferrer"><em>Deep Implicit Layers</em></a>.</li>
<li><a href="http://matpalm.com/blog/ymxb_pod_slice/" rel="noopener noreferrer">Solving y=mx+b with Jax on a TPU Pod slice - Mat Kelcey</a> - A four part YouTube tutorial series with Colab notebooks that starts with Jax fundamentals and moves up to training with a data parallel approach on a v3-32 TPU Pod slice.</li>
<li><a href="https://github.com/huggingface/transformers/blob/9160d81c98854df44b1d543ce5d65a6aa28444a2/examples/research_projects/jax-projects/README.md#talks" rel="noopener noreferrer">JAX, Flax &amp; Transformers ü§ó</a> - 3 days of talks around JAX / Flax, Transformers, large-scale language modeling and other great topics.</li>
</ul>
<a>

<h2 id="papers"><a class="anchor" aria-hidden="true" tabindex="-1" href="#papers"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Papers</h2><p>This section contains papers focused on JAX (e.g. JAX-based library whitepapers, research on JAX, etc). Papers implemented in JAX are listed in the <a href="#projects">Models/Projects</a> section.</p>


<ul>
<li><a href="https://mlsys.org/Conferences/doc/2018/146.pdf" rel="noopener noreferrer"><strong>Compiling machine learning programs via high-level tracing</strong>. Roy Frostig, Matthew James Johnson, Chris Leary. <em>MLSys 2018</em>.</a> - White paper describing an early version of JAX, detailing how computation is traced and compiled.</li>
<li><a href="https://arxiv.org/abs/1912.04232" rel="noopener noreferrer"><strong>JAX, M.D.: A Framework for Differentiable Physics</strong>. Samuel S. Schoenholz, Ekin D. Cubuk. <em>NeurIPS 2020</em>.</a> - Introduces JAX, M.D., a differentiable physics library which includes simulation environments, interaction potentials, neural networks, and more.</li>
<li><a href="https://arxiv.org/abs/2010.09063" rel="noopener noreferrer"><strong>Enabling Fast Differentially Private SGD via Just-in-Time Compilation and Vectorization</strong>. Pranav Subramani, Nicholas Vadivelu, Gautam Kamath. <em>arXiv 2020</em>.</a> - Uses JAX's JIT and VMAP to achieve faster differentially private than existing libraries.</li>
<li><a href="https://arxiv.org/abs/2311.16080" rel="noopener noreferrer"><strong>XLB: A Differentiable Massively Parallel Lattice Boltzmann Library in Python</strong>. Mohammadmehdi Ataei, Hesam Salehipour. <em>arXiv 2023</em>.</a> - White paper describing the XLB library: benchmarks, validations, and more details about the library.</li>
</ul>


<a>

<h2 id="tutorials-and-blog-posts"><a class="anchor" aria-hidden="true" tabindex="-1" href="#tutorials-and-blog-posts"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tutorials and Blog Posts</h2><ul>
<li><a href="https://deepmind.com/blog/article/using-jax-to-accelerate-our-research" rel="noopener noreferrer">Using JAX to accelerate our research by David Budden and Matteo Hessel</a> - Describes the state of JAX and the JAX ecosystem at DeepMind.</li>
<li><a href="https://roberttlange.github.io/posts/2020/03/blog-post-10/" rel="noopener noreferrer">Getting started with JAX (MLPs, CNNs &amp; RNNs) by Robert Lange</a> - Neural network building blocks from scratch with the basic JAX operators.</li>
<li><a href="https://www.kaggle.com/code/truthr/jax-0" rel="noopener noreferrer">Learn JAX: From Linear Regression to Neural Networks by Rito Ghosh</a> - A gentle introduction to JAX and using it to implement Linear and Logistic Regression, and Neural Network models and using them to solve real world problems.</li>
<li><a href="https://github.com/8bitmp3/JAX-Flax-Tutorial-Image-Classification-with-Linen" rel="noopener noreferrer">Tutorial: image classification with JAX and Flax Linen by 8bitmp3</a> - Learn how to create a simple convolutional network with the Linen API by Flax and train it to recognize handwritten digits.</li>
<li><a href="https://medium.com/swlh/plugging-into-jax-16c120ec3302" rel="noopener noreferrer">Plugging Into JAX by Nick Doiron</a> - Compares Flax, Haiku, and Objax on the Kaggle flower classification challenge.</li>
<li><a href="https://blog.evjang.com/2019/02/maml-jax.html" rel="noopener noreferrer">Meta-Learning in 50 Lines of JAX by Eric Jang</a> - Introduction to both JAX and Meta-Learning.</li>
<li><a href="https://blog.evjang.com/2019/07/nf-jax.html" rel="noopener noreferrer">Normalizing Flows in 100 Lines of JAX by Eric Jang</a> - Concise implementation of <a href="https://arxiv.org/abs/1605.08803" rel="noopener noreferrer">RealNVP</a>.</li>
<li><a href="https://blog.evjang.com/2019/11/jaxpt.html" rel="noopener noreferrer">Differentiable Path Tracing on the GPU/TPU by Eric Jang</a> - Tutorial on implementing path tracing.</li>
<li><a href="http://matpalm.com/blog/ensemble_nets" rel="noopener noreferrer">Ensemble networks by Mat Kelcey</a> - Ensemble nets are a method of representing an ensemble of models as one single logical model.</li>
<li><a href="http://matpalm.com/blog/ood_using_focal_loss" rel="noopener noreferrer">Out of distribution (OOD) detection by Mat Kelcey</a> - Implements different methods for OOD detection.</li>
<li><a href="https://www.radx.in/jax.html" rel="noopener noreferrer">Understanding Autodiff with JAX by Srihari Radhakrishna</a> - Understand how autodiff works using JAX.</li>
<li><a href="https://sjmielke.com/jax-purify.htm" rel="noopener noreferrer">From PyTorch to JAX: towards neural net frameworks that purify stateful code by Sabrina J. Mielke</a> - Showcases how to go from a PyTorch-like style of coding to a more Functional-style of coding.</li>
<li><a href="https://github.com/dfm/extending-jax" rel="noopener noreferrer">Extending JAX with custom C++ and CUDA code by Dan Foreman-Mackey</a> - Tutorial demonstrating the infrastructure required to provide custom ops in JAX.</li>
<li><a href="https://roberttlange.github.io/posts/2021/02/cma-es-jax/" rel="noopener noreferrer">Evolving Neural Networks in JAX by Robert Tjarko Lange</a> - Explores how JAX can power the next generation of scalable neuroevolution algorithms.</li>
<li><a href="http://lukemetz.com/exploring-hyperparameter-meta-loss-landscapes-with-jax/" rel="noopener noreferrer">Exploring hyperparameter meta-loss landscapes with JAX by Luke Metz</a> - Demonstrates how to use JAX to perform inner-loss optimization with SGD and Momentum, outer-loss optimization with gradients, and outer-loss optimization using evolutionary strategies.</li>
<li><a href="https://martiningram.github.io/deterministic-advi/" rel="noopener noreferrer">Deterministic ADVI in JAX by Martin Ingram</a> - Walk through of implementing automatic differentiation variational inference (ADVI) easily and cleanly with JAX.</li>
<li><a href="http://matpalm.com/blog/evolved_channel_selection/" rel="noopener noreferrer">Evolved channel selection by Mat Kelcey</a> - Trains a classification model robust to different combinations of input channels at different resolutions, then uses a genetic algorithm to decide the best combination for a particular loss.</li>
<li><a href="https://colab.research.google.com/github/probml/probml-notebooks/blob/main/notebooks/jax_intro.ipynb" rel="noopener noreferrer">Introduction to JAX by Kevin Murphy</a> - Colab that introduces various aspects of the language and applies them to simple ML problems.</li>
<li><a href="https://www.jeremiecoullon.com/2020/11/10/mcmcjax3ways/" rel="noopener noreferrer">Writing an MCMC sampler in JAX by Jeremie Coullon</a> - Tutorial on the different ways to write an MCMC sampler in JAX along with speed benchmarks.</li>
<li><a href="https://www.jeremiecoullon.com/2021/01/29/jax_progress_bar/" rel="noopener noreferrer">How to add a progress bar to JAX scans and loops by Jeremie Coullon</a> - Tutorial on how to add a progress bar to compiled loops in JAX using the <code>host_callback</code> module.</li>
<li><a href="https://github.com/gordicaleksa/get-started-with-JAX" rel="noopener noreferrer">Get started with JAX by Aleksa Gordiƒá</a> - A series of notebooks and videos going from zero JAX knowledge to building neural networks in Haiku.</li>
<li><a href="https://wandb.ai/jax-series/simple-training-loop/reports/Writing-a-Training-Loop-in-JAX-FLAX--VmlldzoyMzA4ODEy" rel="noopener noreferrer">Writing a Training Loop in JAX + FLAX by Saurav Maheshkar and Soumik Rakshit</a> - A tutorial on writing a simple end-to-end training and evaluation pipeline in JAX, Flax and Optax.</li>
<li><a href="https://wandb.ai/wandb/nerf-jax/reports/Implementing-NeRF-in-JAX--VmlldzoxODA2NDk2?galleryTag=jax" rel="noopener noreferrer">Implementing NeRF in JAX by Soumik Rakshit and Saurav Maheshkar</a> - A tutorial on 3D volumetric rendering of scenes represented by Neural Radiance Fields in JAX.</li>
<li><a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html" rel="noopener noreferrer">Deep Learning tutorials with JAX+Flax by Phillip Lippe</a> - A series of notebooks explaining various deep learning concepts, from basics (e.g. intro to JAX/Flax, activiation functions) to recent advances (e.g., Vision Transformers, SimCLR), with translations to PyTorch.</li>
<li><a href="https://chrislu.page/blog/meta-disco/" rel="noopener noreferrer">Achieving 4000x Speedups with PureJaxRL</a> - A blog post on how JAX can massively speedup RL training through vectorisation.</li>
<li><a href="https://levelup.gitconnected.com/create-your-own-automatically-differentiable-simulation-with-python-jax-46951e120fbb?sk=e8b9213dd2c6a5895926b2695d28e4aa" rel="noopener noreferrer">Simple PDE solver + Constrained Optimization with JAX by Philip Mocz</a> - A simple example of solving the advection-diffusion equations with JAX and using it in a constrained optimization problem to find initial conditions that yield desired result.</li>
</ul>
<a>

<h2 id="books"><a class="anchor" aria-hidden="true" tabindex="-1" href="#books"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Books</h2><ul>
<li><a href="https://www.manning.com/books/jax-in-action" rel="noopener noreferrer">Jax in Action</a> - A hands-on guide to using JAX for deep learning and other mathematically-intensive applications.</li>
</ul>
<a>

<h2 id="community"><a class="anchor" aria-hidden="true" tabindex="-1" href="#community"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Community</h2><ul>
<li><a href="https://discord.com/channels/1107832795377713302/1107832795688083561" rel="noopener noreferrer">JaxLLM (Unofficial) Discord</a></li>
<li><a href="https://github.com/google/jax/discussions" rel="noopener noreferrer">JAX GitHub Discussions</a></li>
<li><a href="https://www.reddit.com/r/JAX/" rel="noopener noreferrer">Reddit</a></li>
</ul>
<h2 id="contributing"><a class="anchor" aria-hidden="true" tabindex="-1" href="#contributing"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributing</h2><p>Contributions welcome! Read the <a href="https://github.com/n2cholas/awesome-jax/blob/main/readme.md/contributing.md" rel="noopener noreferrer">contribution guidelines</a> first.</p>
</a></a></a></a></a></a></a></a>

    </main>
  </body>
</html>
