<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <title>Awesome List Updates on Nov 18, 2021 - Track Awesome List</title>
    <meta property="og:url" content="https://www.trackawesomelist.com" />
    <meta property="og:type" content="summary" />
    <meta property="og:title" content="Awesome List Updates on Nov 18, 2021" />
    <meta property="og:description" content="11 awesome lists updated today." />
    <meta property="og:site_name" content="Track Awesome List" />
    <style>
      main {
        max-width: 1024px;
        margin: 0 auto;
        padding: 0 0.5em;
      }
      :root,[data-color-mode=light][data-light-theme=light],[data-color-mode=dark][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}[data-color-mode=light][data-light-theme=dark],[data-color-mode=dark][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}.markdown-body{word-wrap:break-word;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;font-size:16px;line-height:1.5}.markdown-body:before{content:"";display:table}.markdown-body:after{clear:both;content:"";display:table}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:var(--color-danger-fg)}.markdown-body .anchor{float:left;margin-left:-20px;padding-right:4px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body p,.markdown-body blockquote,.markdown-body ul,.markdown-body ol,.markdown-body dl,.markdown-body table,.markdown-body pre,.markdown-body details{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;background-color:var(--color-border-default);border:0;margin:24px 0;padding:0}.markdown-body blockquote{color:var(--color-fg-muted);border-left:.25em solid var(--color-border-default);padding:0 1em}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:var(--color-fg-default);vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 tt,.markdown-body h1 code,.markdown-body h2 tt,.markdown-body h2 code,.markdown-body h3 tt,.markdown-body h3 code,.markdown-body h4 tt,.markdown-body h4 code,.markdown-body h5 tt,.markdown-body h5 code,.markdown-body h6 tt,.markdown-body h6 code{font-size:inherit;padding:0 .2em}.markdown-body h1{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:2em}.markdown-body h2{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{color:var(--color-fg-muted);font-size:.85em}.markdown-body summary h1,.markdown-body summary h2,.markdown-body summary h3,.markdown-body summary h4,.markdown-body summary h5,.markdown-body summary h6{display:inline-block}.markdown-body summary h1 .anchor,.markdown-body summary h2 .anchor,.markdown-body summary h3 .anchor,.markdown-body summary h4 .anchor,.markdown-body summary h5 .anchor,.markdown-body summary h6 .anchor{margin-left:-40px}.markdown-body summary h1,.markdown-body summary h2{border-bottom:0;padding-bottom:0}.markdown-body ul,.markdown-body ol{padding-left:2em}.markdown-body ul.no-list,.markdown-body ol.no-list{padding:0;list-style-type:none}.markdown-body ol[type="1"]{list-style-type:decimal}.markdown-body ol[type=a]{list-style-type:lower-alpha}.markdown-body ol[type=i]{list-style-type:lower-roman}.markdown-body div>ol:not([type]){list-style-type:decimal}.markdown-body ul ul,.markdown-body ul ol,.markdown-body ol ol,.markdown-body ol ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{margin-top:16px;padding:0;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{margin-bottom:16px;padding:0 16px}.markdown-body table{width:100%;width:-webkit-max-content;width:-webkit-max-content;width:max-content;max-width:100%;display:block;overflow:auto}.markdown-body table th{font-weight:600}.markdown-body table th,.markdown-body table td{border:1px solid var(--color-border-default);padding:6px 13px}.markdown-body table tr{background-color:var(--color-canvas-default);border-top:1px solid var(--color-border-muted)}.markdown-body table tr:nth-child(2n){background-color:var(--color-canvas-subtle)}.markdown-body table img{background-color:rgba(0,0,0,0)}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:var(--color-canvas-default)}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:rgba(0,0,0,0)}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{float:left;width:auto;border:1px solid var(--color-border-default);margin:13px 0 0;padding:7px;display:block;overflow:hidden}.markdown-body span.frame span img{float:left;display:block}.markdown-body span.frame span span{clear:both;color:var(--color-fg-default);padding:5px 0 0;display:block}.markdown-body span.align-center{clear:both;display:block;overflow:hidden}.markdown-body span.align-center>span{text-align:center;margin:13px auto 0;display:block;overflow:hidden}.markdown-body span.align-center span img{text-align:center;margin:0 auto}.markdown-body span.align-right{clear:both;display:block;overflow:hidden}.markdown-body span.align-right>span{text-align:right;margin:13px 0 0;display:block;overflow:hidden}.markdown-body span.align-right span img{text-align:right;margin:0}.markdown-body span.float-left{float:left;margin-right:13px;display:block;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{float:right;margin-left:13px;display:block;overflow:hidden}.markdown-body span.float-right>span{text-align:right;margin:13px auto 0;display:block;overflow:hidden}.markdown-body code,.markdown-body tt{background-color:var(--color-neutral-muted);border-radius:6px;margin:0;padding:.2em .4em;font-size:85%}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{-webkit-text-decoration:inherit;-webkit-text-decoration:inherit;text-decoration:inherit}.markdown-body samp{font-size:85%}.markdown-body pre{word-wrap:normal}.markdown-body pre code{font-size:100%}.markdown-body pre>code{word-break:normal;white-space:pre;background:0 0;border:0;margin:0;padding:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{word-break:normal;margin-bottom:0}.markdown-body .highlight pre,.markdown-body pre{background-color:var(--color-canvas-subtle);border-radius:6px;padding:16px;font-size:85%;line-height:1.45;overflow:auto}.markdown-body pre code,.markdown-body pre tt{max-width:auto;line-height:inherit;word-wrap:normal;background-color:rgba(0,0,0,0);border:0;margin:0;padding:0;display:inline;overflow:visible}.markdown-body .csv-data td,.markdown-body .csv-data th{text-align:left;white-space:nowrap;padding:5px;font-size:12px;line-height:1;overflow:hidden}.markdown-body .csv-data .blob-num{text-align:right;background:var(--color-canvas-default);border:0;padding:10px 8px 9px}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{background:var(--color-canvas-subtle);border-top:0;font-weight:600}.markdown-body [data-footnote-ref]:before{content:"["}.markdown-body [data-footnote-ref]:after{content:"]"}.markdown-body .footnotes{color:var(--color-fg-muted);border-top:1px solid var(--color-border-default);font-size:12px}.markdown-body .footnotes ol{padding-left:16px}.markdown-body .footnotes li{position:relative}.markdown-body .footnotes li:target:before{pointer-events:none;content:"";border:2px solid var(--color-accent-emphasis);border-radius:6px;position:absolute;top:-8px;bottom:-8px;left:-24px;right:-8px}.markdown-body .footnotes li:target{color:var(--color-fg-default)}.markdown-body .footnotes .data-footnote-backref g-emoji{font-family:monospace}.markdown-body{background-color:var(--color-canvas-default);color:var(--color-fg-default)}.markdown-body a{color:var(--color-accent-fg);text-decoration:none}.markdown-body a:hover{text-decoration:underline}.markdown-body iframe{background-color:#fff;border:0;margin-bottom:16px}.markdown-body svg.octicon{fill:currentColor}.markdown-body .anchor>.octicon{display:inline}.markdown-body .highlight .token.keyword,.gfm-highlight .token.keyword{color:var(--color-prettylights-syntax-keyword)}.markdown-body .highlight .token.tag .token.class-name,.markdown-body .highlight .token.tag .token.script .token.punctuation,.gfm-highlight .token.tag .token.class-name,.gfm-highlight .token.tag .token.script .token.punctuation{color:var(--color-prettylights-syntax-storage-modifier-import)}.markdown-body .highlight .token.operator,.markdown-body .highlight .token.number,.markdown-body .highlight .token.boolean,.markdown-body .highlight .token.tag .token.punctuation,.markdown-body .highlight .token.tag .token.script .token.script-punctuation,.markdown-body .highlight .token.tag .token.attr-name,.gfm-highlight .token.operator,.gfm-highlight .token.number,.gfm-highlight .token.boolean,.gfm-highlight .token.tag .token.punctuation,.gfm-highlight .token.tag .token.script .token.script-punctuation,.gfm-highlight .token.tag .token.attr-name{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.function,.gfm-highlight .token.function{color:var(--color-prettylights-syntax-entity)}.markdown-body .highlight .token.string,.gfm-highlight .token.string{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.comment,.gfm-highlight .token.comment{color:var(--color-prettylights-syntax-comment)}.markdown-body .highlight .token.class-name,.gfm-highlight .token.class-name{color:var(--color-prettylights-syntax-variable)}.markdown-body .highlight .token.regex,.gfm-highlight .token.regex{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.regex .regex-delimiter,.gfm-highlight .token.regex .regex-delimiter{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.tag .token.tag,.markdown-body .highlight .token.property,.gfm-highlight .token.tag .token.tag,.gfm-highlight .token.property{color:var(--color-prettylights-syntax-entity-tag)}
    </style>
  </head>
  <body>
    <main data-color-mode="light" data-light-theme="light" data-dark-theme="dark" class="markdown-body">
      <h1>Awesome List Updates on Nov 18, 2021</h1>
<p>11 awesome lists updated today.</p>
<p><a href="/">üè† Home</a><span> ¬∑ </span><a href="https://www.trackawesomelist.com/search/">üîç Search</a><span> ¬∑ </span><a href="https://www.trackawesomelist.com/rss.xml">üî• Feed</a><span> ¬∑ </span><a href="https://trackawesomelist.us17.list-manage.com/subscribe?u=d2f0117aa829c83a63ec63c2f&id=36a103854c">üìÆ Subscribe</a><span> ¬∑ </span><a href="https://github.com/sponsors/theowenyoung">‚ù§Ô∏è  Sponsor</a></p>
<h2><a href="https://www.trackawesomelist.com/newTendermint/awesome-bigdata/">1. Awesome Bigdata</a></h2><h3><p>Business Intelligence</p>
</h3><ul>
<li><a href="https://github.com/lightdash/lightdash" rel="noopener noreferrer">Lightdash (‚≠ê4.9k)</a> - The open source Looker alternative built on dbt</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/Anant/awesome-cassandra/">2. Awesome Cassandra</a></h2><h3><p>Cassandra Deployment / Cassandra as a Service / Managed Cassandra Based on Proprietary Technology</p>
</h3><ul>
<li><a href="https://www.instaclustr.com/benchmarking-cassandra-with-local-storage-on-azure/" rel="noopener noreferrer">Benchmarking Cassandra with Local Storage on Azure</a> - Learn about comparing Cassandra on Azure VMs w/ Local vs. Remote storage.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/craftcms/awesome/">3. Awesome</a></h2><h3><p>Praises / Tutorials</p>
</h3><ul>
<li><a href="https://www.madebybridge.com/blog/craft-cms-vs-wordpress-a-comparison" rel="noopener noreferrer">Craft CMS vs WordPress: A comparison &amp; why we love Craft CMS</a> by Chris Cox/Made by Bridge - <code>17 Nov 2021</code></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/public-apis/public-apis/">4. Public Apis</a></h2><h3><p>Security</p>
</h3><ul>
<li><p>API: <a href="https://github.com/fingerprintjs/aev" rel="noopener noreferrer">Application Environment Verification (‚≠ê84)</a></p>
<p>Description: Android library and API to verify the safety of user devices, detect rooted devices and other risks</p>
<p>Auth: <code>apiKey</code></p>
<p>HTTPS: Yes</p>
<p>CORS: Yes</p>
</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/unixorn/awesome-zsh-plugins/">5. Awesome Zsh Plugins</a></h2><h3><p>Generic ZSH</p>
</h3><ul>
<li><a href="https://github.com/mattmc3/zsh_unplugged" rel="noopener noreferrer">ZSH Unplugged (‚≠ê443)</a> - Good resource if you want to eliminate using a framework but still easily use plugins.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/sirredbeard/Awesome-WSL/">6. Awesome WSL</a></h2><h3><p>WSL Tools / For Managing WSL Installations</p>
</h3><ul>
<li><a href="https://github.com/emeric-martineau/wsl-gui-tool" rel="noopener noreferrer">WSL GUI Tool (‚≠ê99)</a> - A graphical tool to manage (run, stop, import, export...) WSL. <img src="https://raw.githubusercontent.com/sirredbeard/Awesome-WSL/master/github-icon.png" alt="github project" /></li>
</ul>
<h2><a href="https://www.trackawesomelist.com/zoidbergwill/awesome-ebpf/">7. Awesome Ebpf</a></h2><h3><p>eBPF Workflow: Tools and Utilities / oxidebpf</p>
</h3><ul>
<li><a href="https://github.com/redcanaryco/oxidebpf" rel="noopener noreferrer">oxidebpf (‚≠ê120)</a> - A pure Rust library for managing eBPF programs, designed for security use cases. The featureset is more limited than other libraries but emphasizes stability across a wide range of kernels and backwards-compatible compile-once-run-most-places.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/YuzheSHI/awesome-agi-cocosci/">8. Awesome Agi Cocosci</a></h2><h3><p>Abduction / Explanation</p>
</h3><ul>
<li><a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2506_2" rel="noopener noreferrer">Use of current explanations in multicausal abductive reasoning</a> - <em><strong>Cognitive Science</strong></em>, 2001. [<a href="https://scholar.google.com/scholar?cluster=7816050625957759346&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S1570868314000895" rel="noopener noreferrer">Abduction: A categorical characterization</a> - <em><strong>Journal of Applied Logic</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=17834260152484836885&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.journals.uchicago.edu/doi/abs/10.1086/392744" rel="noopener noreferrer">Defending Abduction</a> - <em><strong>Philosophy of Science</strong></em>, 1999. [<a href="https://scholar.google.com/scholar?cluster=13895790050138832555&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://link.springer.com/article/10.1007/s11229-019-02337-z" rel="noopener noreferrer">Abduction‚Äâ‚àí‚Äâthe context of discovery‚Äâ+‚Äâunderdetermination‚Äâ=‚Äâinference to the best explanation</a> - <em><strong>Synthese</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4261649938116694095&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://link.springer.com/chapter/10.1007%2F3-540-45004-1_14" rel="noopener noreferrer">Towards an Architecture for Cognitive Vision Using Qualitative Spatio-temporal Representations and Abduction</a> - <em><strong>Spatial Cognition</strong></em>, 2002. [<a href="https://scholar.google.com/scholar?cluster=8072265283930278310&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://link.springer.com/article/10.1007/s11229-018-1824-6" rel="noopener noreferrer">Abductive inference within a pragmatic framework</a> - <em><strong>Synthese</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=10285954503043361393&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://link.springer.com/article/10.1007/s00354-019-00059-x" rel="noopener noreferrer">Disjunctive Abduction</a> - <em><strong>New Generation Computing</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=6664745483675209831&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.tandfonline.com/doi/full/10.1080/09528130600558141?scroll=top&amp;needAccess=true" rel="noopener noreferrer">The order effect in human abductive reasoning: an empirical and computational study</a> - <em><strong>Journal of Experimental &amp; Theoretical Artificial Intelligence</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=3803536062463585043&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01142.x" rel="noopener noreferrer">The AHA! Experience: Creativity Through Emergent Binding in Neural Networks</a> - <em><strong>Cognitive Science</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=10006889101167052798&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Abduction / Scientific Discovery</p>
</h3><ul>
<li><a href="https://psycnet.apa.org/record/2010-22980-001" rel="noopener noreferrer">Hypothesis generation, sparse categories, and the positive test strategy</a> - <em><strong>Psychological Review</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?cluster=4329636480235863472&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Bayesian Modeling / Nonparametric Model</p>
</h3><ul>
<li><a href="https://www.aaai.org/Papers/AAAI/2006/AAAI06-061.pdf" rel="noopener noreferrer">Learning Systems of Concepts with an Infinite Relational Model</a> - <em><strong>AAAI'06</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=3207350432755252565&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://dl.acm.org/doi/abs/10.1145/1667053.1667056" rel="noopener noreferrer">The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies</a> - <em><strong>Journal of the ACM</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=8216933258869737505&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Susan Carey / Commonsense Knowledgebase</p>
</h3><ul>
<li><a href="https://hk1lib.org/book/3659332/11fa44" rel="noopener noreferrer">Conceptual Change in Childhood</a> - <em><strong>MIT Press</strong></em>, 1985. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=conceptual+change+in+childhood+susan+carey&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://hk1lib.org/book/844457/42178f?id=844457&amp;secret=42178f" rel="noopener noreferrer">The Origin of Concepts</a> - <em><strong>Oxford University Press</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=11493102398422813821&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Communications / Non-Verbal Communication</p>
</h3><ul>
<li><a href="https://journals.sagepub.com/doi/abs/10.1177/108835769400900301" rel="noopener noreferrer">The Picture Exchange Communication System</a> - <em><strong>Behavior Modification</strong></em>, 1994. [<a href="https://scholar.google.com/scholar?cluster=18113491434570143349&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0378216614001830" rel="noopener noreferrer">A multimodal discourse theory of visual narrative</a> - <em><strong>Journal of Pragmatics</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=912273653379961242&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Communications / Pragmatics</p>
</h3><ul>
<li><a href="https://semprag.org/index.php/sp/article/view/sp.5.6/pdf" rel="noopener noreferrer">Information Structure in Discourse: Towards an Integrated Formal Theory of Pragmatics</a> - <em><strong>Semantics and Pragmatics</strong></em>, 1998. [<a href="https://scholar.google.com/scholar?cluster=9127222314768938599&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Communications / Language Compositionality</p>
</h3><ul>
<li><a href="https://arxiv.org/abs/1804.03980" rel="noopener noreferrer">Emergent communication through negotiation</a> - <em><strong>ICLR'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8825869866742501521&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://psycnet.apa.org/record/2019-07481-001" rel="noopener noreferrer">The language of generalization</a> - <em><strong>Psychological Review</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7723877614160376324&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/abs/2004.09124" rel="noopener noreferrer">Compositionality and Generalization in Emergent Languages</a> - <em><strong>ACL'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5792073344743965767&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Problem Solving / Human-Level Problem Solving</p>
</h3><ul>
<li><a href="http://196.223.158.148/bitstream/handle/123456789/2978/596.pdf?sequence=1&amp;isAllowed=y" rel="noopener noreferrer">Learning to Solve Problems: A Handbook for Designing Problem-Solving Learning Environments</a> - <em><strong>Taylorfrancis</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=13262690779319271809&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://link.springer.com/article/10.1007/BF00058926" rel="noopener noreferrer">Learning to perceive and act by trial and error</a> - <em><strong>Machine Learning</strong></em>, 1991. [<a href="https://scholar.google.com/scholar?cluster=1987606770603964473&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1801_3" rel="noopener noreferrer">Representations in distributed cognitive tasks</a> - <em><strong>Cognitive Science</strong></em>, 1994. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=14781266698447195483" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0364021399800226" rel="noopener noreferrer">The nature of external representations in problem solving</a> - <em><strong>Cognitive Science</strong></em>, 1997. [<a href="https://scholar.google.com/scholar?cluster=10698887231200401430&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S2352154620301236" rel="noopener noreferrer">Exploration: from machines to humans</a> - <em><strong>Current Opinion in Behavioral Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=8015078432419172621&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S2352154620301467" rel="noopener noreferrer">Balancing exploration and exploitation with information and randomization</a> - <em><strong>Current Opinion in Behavioral Sciences</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=8164388137243077863&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0092867421008369" rel="noopener noreferrer">Hippocampal neurons construct a map of an abstract value space</a> - <em><strong>Cell</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=12658820581876003172&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Problem Solving / Planning</p>
</h3><ul>
<li><a href="https://onlinelibrary.wiley.com/doi/ftr/10.1111/cogs.12928" rel="noopener noreferrer">What Is the Model in Model-Based Planning?</a> - <em><strong>Cognitive Science</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=10598397017491369972&amp;hl=en&amp;scisbd=1&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Problem Solving / Reinforcement Learning</p>
</h3><ul>
<li><a href="https://openreview.net/forum?id=9SS69KwomAM" rel="noopener noreferrer">Solving Compositional Reinforcement Learning Problems via Task Reduction</a> - <em><strong>ICLR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15628616147808752058&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8460689" rel="noopener noreferrer">Neural Task Programming: Learning to Generalize Across Hierarchical Tasks</a> - <em><strong>ICRA'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=7155333517647976638&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://academic.oup.com/logcom/article-abstract/28/2/337/4695480" rel="noopener noreferrer">Learning to act: qualitative learning of deterministic action models</a> - <em><strong>Journal of Logic and Computation</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=14570482854600886953&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/abs/2109.06076" rel="noopener noreferrer">Learning to Act and Observe in Partially Observable Domains</a> - 2021. [<a href="https://scholar.google.com/scholar?cluster=2258600434630687063&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://ieeexplore.ieee.org/abstract/document/9387127" rel="noopener noreferrer">Data-Efficient Learning for Complex and Real-Time Physical Problem Solving Using Augmented Simulation</a> - <em><strong>Robotics and Automation Letters</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=3140653562829320759&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Problem Solving / Inverse Reinforcement Learning</p>
</h3><ul>
<li><a href="https://arxiv.org/abs/1902.07742" rel="noopener noreferrer">From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following</a> - <em><strong>ICLR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=9128320307925997063&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/1904.06317.pdf" rel="noopener noreferrer">Few-shot Bayesian imitation learning with logical program policies.</a> - <em><strong>AAAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5103854692762145813&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>System 1 &amp; System 2 / Neural-Symbolic AI</p>
</h3><ul>
<li><a href="https://arxiv.org/pdf/1904.10729.pdf" rel="noopener noreferrer">Neural Logic Reinforcement Learning</a> - <em><strong>ICML'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=18074632043038701502&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://link.springer.com/article/10.1007/s11432-018-9801-4" rel="noopener noreferrer">Abductive learning: towards bridging machine learning and logical reasoning</a> - <em><strong>Science China Information Sciences</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=8541635351775190855&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/2010.03514.pdf" rel="noopener noreferrer">Abductive Knowledge Induction From Raw Data</a> - <em><strong>IJCAI'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7027142960863064076&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0004370220301855" rel="noopener noreferrer">Making sense of sensory input</a> - <em><strong>Artificial Intelligence</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=11875529139573472578&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/2103.14230v1.pdf" rel="noopener noreferrer">Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution</a> - <em><strong>CVPR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=4172146500538799638&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://openreview.net/pdf?id=SJlh8CEYDB" rel="noopener noreferrer">Learn to explain efÔ¨Åciently via neural logic inductive learning</a> - <em><strong>ICLR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=4550874980727321525&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/gblackout/NLIL" rel="noopener noreferrer">Project (‚≠ê44)</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/abs/2006.06649" rel="noopener noreferrer">Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning</a> - <em><strong>ICML'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9257372000778020812&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/abs/2003.08978" rel="noopener noreferrer">Generating new concepts with hybrid neuro-symbolic models.</a> - <em><strong>CogSci'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=1912020791698331044" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/abs/2006.14448" rel="noopener noreferrer">Learning Task-General Representations with Generative Neuro-Symbolic Modeling</a> - <em><strong>ICLR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=1335404082385789329" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/2006.11524.pdf" rel="noopener noreferrer">Neuro-Symbolic Visual Reasoning: Disentangling ‚ÄúVisual‚Äù from ‚ÄúReasoning‚Äù</a> - <em><strong>ICML'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13160160974887139307&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/1905.10307.pdf" rel="noopener noreferrer">An Explicitly Relational Neural Network Architecture</a> - <em><strong>ICML'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=37732747764322837&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://openreview.net/pdf?id=H1eSS3CcKX" rel="noopener noreferrer">Stochastic Optimization of Sorting Networks via Continuous Relaxations</a> - <em><strong>ICLR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=10619362619006891050&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://openreview.net/pdf?id=BkxUvnEYDH" rel="noopener noreferrer">Program Guided Agent</a> - <em><strong>ICLR'20</strong></em>, 2020. [<a href="https://openreview.net/forum?id=BkxUvnEYDH" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/1904.11694.pdf" rel="noopener noreferrer">Neural Logic Machines</a> - <em><strong>ICLR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4525183211642569463&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/1904.12584.pdf" rel="noopener noreferrer">The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision</a> - <em><strong>ICLR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=8837128214653317831&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/abs/2103.16564" rel="noopener noreferrer">Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning</a> - <em><strong>ICLR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=16735976343684307244&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://jiajunwu.com/papers/toqnet_ijcai.pdf" rel="noopener noreferrer">Temporal and Object Quantification Networks</a> - <em><strong>IJCAI'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=17251222943638414124&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Embodied Intelligence / Explainable Deep Learning</p>
</h3><ul>
<li><a href="https://journals.sagepub.com/doi/abs/10.1177/0956797610371962" rel="noopener noreferrer">Rapid Assimilation of External Objects Into the Body Schema</a> - <em><strong>Psychological Science</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=854636910326733489&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.eva.mpg.de/documents/Cambridge/Tennie_Cultural_BehBrainSci_2012_1566208.pdf" rel="noopener noreferrer">The cognitive bases of human tool use</a> - <em><strong>Behavioral and Brain Sciences</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=4648150119820414671&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00214/full" rel="noopener noreferrer">The embodied mind extended: using words as social tools</a> - <em><strong>Frontiers in Psychology</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=14719988081062606352&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://royalsocietypublishing.org/doi/10.1098/rstb.2012.0408" rel="noopener noreferrer">Tool use as adaptation</a> - <em><strong>Philosophical Transactions of the Royal Society B: Biological Sciences</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=8060841461200774807&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0028393214000232" rel="noopener noreferrer">Intensive tool-practice and skillfulness facilitate the extension of body representations in humans</a> - <em><strong>Neuropsychologia</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=10578024091098127929&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://robotics.sciencemag.org/content/6/54/eabd7935.abstract" rel="noopener noreferrer">Robotic hand augmentation drives changes in neural body representation</a> - <em><strong>Science Robotics</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=1622125726197763917&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.jneurosci.org/content/jneuro/41/13/2980.full.pdf" rel="noopener noreferrer">Expert Tool Users Show Increased Differentiation between Visual Representations of Hands and Tools</a> - <em><strong>Journal of Neuroscience</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=13454164767827515188&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/2106.05654.pdf" rel="noopener noreferrer">Visual scoping operations for physical assembly</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7238090583833839&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.cc.gatech.edu/ai/robot-lab/online-publications/StoytchevICRA2005.pdf" rel="noopener noreferrer">Behavior-grounded representation of tool affordances</a> - <em><strong>ICRA'05</strong></em>, 2005. [<a href="https://scholar.google.com/scholar?cluster=6115815663915603675&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://link.springer.com/chapter/10.1007/978-3-642-38812-5_1" rel="noopener noreferrer">A Relational Approach to Tool-Use Learning in Robots</a> - <em><strong>ILP'12</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=18374178227592386332&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://link.springer.com/article/10.1007/s10514-017-9637-x" rel="noopener noreferrer">Relational affordances for multiple-object manipulation</a> - <em><strong>Autonomous Robots</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=6357646940615855682&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://m.roboticsproceedings.org/rss15/p01.pdf" rel="noopener noreferrer">Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight</a> - <em><strong>RSS'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4316276917607326251&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Methodologies for Experiments / Human-Machine Comparison</p>
</h3><ul>
<li><a href="https://stacks.stanford.edu/file/druid:qv796fc9687/qv796fc9687.pdf" rel="noopener noreferrer">Problem Solving and Rule Induction: A Unified View</a> - <em><strong>Knowledge and cognition</strong></em>, 1974. [<a href="https://scholar.google.com/scholar?cluster=12943734683291006234&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://link.springer.com/content/pdf/10.1007/s42113-019-00053-y.pdf" rel="noopener noreferrer">People Infer Recursive Visual Concepts from Just a Few Examples</a> - <em><strong>Computational Brain &amp; Behavior</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=3871396883970734141&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://escholarship.org/content/qt3xf2n3vc/qt3xf2n3vc.pdf" rel="noopener noreferrer">One-shot learning of generative speech concepts</a> - <em><strong>CogSci'14</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=15482292457660075957&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/abs/1901.04587" rel="noopener noreferrer">Human few-shot learning of compositional instructions</a> - <em><strong>CogSci'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=12841163907815018136&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/2103.05823.pdf" rel="noopener noreferrer">Fast and flexible: Human program induction in abstract reasoning tasks</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=5294483826040237516&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://proceedings.mlr.press/v80/dubey18a.html" rel="noopener noreferrer">Investigating Human Priors for Playing Video Games</a> - <em><strong>ICML'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=2202192690517876762&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S2352154619300622" rel="noopener noreferrer">Tasks for aligning human and machine planning</a> - <em><strong>Current Opinion in Behavioral Sciences</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=8308872468787875598&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Meta-Level Considerations / Cognitive Architecture</p>
</h3><ul>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/0004370287900506" rel="noopener noreferrer">SOAR: An architecture for general intelligence</a> - <em><strong>Artificial Intelligence</strong></em>, 1987. [<a href="https://scholar.google.com/scholar?cluster=10873259207109132615&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0004370205001530" rel="noopener noreferrer">Metacognition in computation: A selected research review</a> - <em><strong>Artificial Intelligence</strong></em>, 2005. [<a href="https://scholar.google.com/scholar?cluster=4240334051245008914&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0010027718301604" rel="noopener noreferrer">Basic functional trade-offs in cognition: An integrative framework</a> - <em><strong>Cognition</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=11475742130443069967&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.worldscientific.com/doi/abs/10.1142/S2705078521500028" rel="noopener noreferrer">A Theoretical Computer Science Perspective on Consciousness</a> - <em><strong>Journal of Artificial Intelligence and Consciousness</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16430561748075101972&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Theory of Mind / AI Assisted Research</p>
</h3><ul>
<li><a href="https://plato.stanford.edu/entries/intentionality/" rel="noopener noreferrer">Intentionality</a> - <em><strong>Plato Stanford</strong></em>.</li>
</ul>
<ul>
<li><a href="https://psyarxiv.com/f692k/" rel="noopener noreferrer">The Signature of All Things: Children Infer Knowledge States from Static Images</a> - <em><strong>CogSci'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=12380982112592086477&amp;hl=en&amp;as_sdt=0,5&amp;as_ylo=2017" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://psycnet.apa.org/fulltext/2019-58384-001.pdf?auth_token=0859666184839448b848053cd7bdceb2bdf2745a" rel="noopener noreferrer">Leveraging Facial Expressions and Contextual Information to Investigate Opaque Representations of Emotion</a> - <em><strong>Emotion</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=9634378462684744548&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://linkinghub.elsevier.com/retrieve/pii/S0010027712002235" rel="noopener noreferrer">Waiting and weighting: Information sampling is a balance between efficiency and error-reduction</a> - <em><strong>Cognition</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=12787722822882067638&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0896627313005503?via%3Dihub" rel="noopener noreferrer">Natural scene statistics account for the representation of scene categories in human visual cortex</a> - <em><strong>Neuron</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=14030885492052338412&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://psycnet.apa.org/record/2019-27729-001" rel="noopener noreferrer">Unit of visual working memory: A Boolean map provides a better account than an object does</a> - <em><strong>Journal of Experimental Psychology</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=14909735035752892020&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Analogy / AI Assisted Research</p>
</h3><ul>
<li><a href="https://www.iiia.csic.es/~enric/papers/generalize_and_blend.pdf" rel="noopener noreferrer">Generalize and Blend: Concept Blending Based on Generalization, Analogy, and Amalgams</a> - <em><strong>ICCC'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=11073359237116879862&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://ieeexplore.ieee.org/document/9010418" rel="noopener noreferrer">Detecting Unseen Visual Relations Using Analogies</a> - <em><strong>CVPR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=16686853801653819556&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/abs/1902.00120" rel="noopener noreferrer">Learning to Make Analogies by Contrasting Abstract Relational Structure</a> - <em><strong>ICLR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15521573039503233138&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://aclanthology.org/2020.figlang-1.pdf#page=140" rel="noopener noreferrer">Sky + Fire = Sunset. Exploring Parallels between Visually Grounded Metaphors and Image Classifiers</a> - <em><strong>ACL'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5747285277687442001&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/2006.04156.pdf" rel="noopener noreferrer">Analogy as Nonparametric Bayesian Inference over Relational Systems</a> - <em><strong>CogSci'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1798148167130120057&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Causality / AI Assisted Research</p>
</h3><ul>
<li><a href="https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780195376746.001.0001/oxfordhb-9780195376746-e-46" rel="noopener noreferrer">Causal Reasoning</a> - <em><strong>The Oxford Handbook of Cognitive Psychology</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=11361740093816709089&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2703_6" rel="noopener noreferrer">Inferring causal networks from observations and interventions</a> - <em><strong>Cognitive Science</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=12050301037347772984&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://cogsci.mindmodeling.org/2015/papers/0418/paper0418.pdf" rel="noopener noreferrer">Constraints on Hypothesis Selection in Causal Learning</a> - <em><strong>CogSci'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=2005&amp;sciodt=0%2C5&amp;cites=16920774374067505248&amp;scipsc=&amp;q=Constraints+on+hypothesis+selection+in+causal+learning&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=d0TfP8EAAAAJ&amp;sortby=pubdate&amp;citation_for_view=d0TfP8EAAAAJ:S16KYo8Pm5AC" rel="noopener noreferrer">What happened? Reconstructing the past through vision and sound</a> - 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=12975579257004398798" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Commonsense / AI Commonsense Reasoning</p>
</h3><ul>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8953217" rel="noopener noreferrer">From Recognition to Cognition: Visual Commonsense Reasoning</a> - <em><strong>CVPR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15467433880059136365&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="http://visualcommonsense.com/" rel="noopener noreferrer">Project</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/1911.11641.pdf" rel="noopener noreferrer">PIQA: Reasoning about Physical Commonsense in Natural Language</a> - <em><strong>AAAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=10110424163152713144&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9156347" rel="noopener noreferrer">Visual Commonsense R-CNN</a> - <em><strong>CVPR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=6886229776034162585&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://link.springer.com/chapter/10.1007%2F978-3-030-58558-7_30" rel="noopener noreferrer">VisualCOMET: Reasoning About the Dynamic Context of a Still Image</a> - <em><strong>ECCV'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7681600847940772451&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Inductive Logic &amp; Program Synthesis / Commonsense Knowledgebase</p>
</h3><ul>
<li><a href="https://link.springer.com/chapter/10.1007%2F3-540-44797-0_10" rel="noopener noreferrer">Towards combining inductive logic programming with Bayesian networks</a> - <em><strong>ILP'01</strong></em>, 2001. [<a href="https://scholar.google.com/scholar?cluster=2904180673047700407&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://andrewcropper.com/pubs/ijcai15-metagolo.pdf" rel="noopener noreferrer">Learning Efficient Logical Robot Strategies Involving Composable Objects</a> - <em><strong>IJCAI'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=5109851972354087162&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://andrewcropper.com/pubs/ijcai16-metafunc.pdf" rel="noopener noreferrer">Learning Higher-Order Logic Programs through Abstraction and Invention</a> - <em><strong>IJCAI'16</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=10945054943203858325&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://link.springer.com/chapter/10.1007%2F978-3-319-99960-9_3" rel="noopener noreferrer">How Much Can Experimental Cost Be Reduced in Active Learning of Agent Strategies?</a> - <em><strong>ILP'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8152380236842970357&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://link.springer.com/article/10.1007/s10994-018-5710-8" rel="noopener noreferrer">Meta-Interpretive Learning from noisy images</a> - <em><strong>Machine Learning</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=5719375383968868329&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://andrewcropper.com/pubs/mlj18-metaopt.pdf" rel="noopener noreferrer">Learning efficient logic programs</a> - <em><strong>Machine Learning</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=17955696870252443734&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://andrewcropper.com/pubs/mlj19-metaho.pdf" rel="noopener noreferrer">Learning higher-order logic programs</a> - <em><strong>Machine Learning</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=6723896359456002413&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://andrewcropper.com/pubs/mlj19-reduce.pdf" rel="noopener noreferrer">Logical reduction of metarules</a> - <em><strong>Machine Learning</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4577603126537024540&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://andrewcropper.com/pubs/ijcai19-playgol.pdf" rel="noopener noreferrer">Playgol: Learning Programs Through Play</a> - <em><strong>IJCAI'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=556522464212000763&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://link.springer.com/article/10.1007%2Fs00354-019-00054-2" rel="noopener noreferrer">Machine Discovery of Comprehensible Strategies for Simple Games Using Meta-interpretive Learning</a> - <em><strong>New Generation Computing</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=11019349634035542991&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://andrewcropper.com/pubs/aaai20-forgetgol.pdf" rel="noopener noreferrer">Forgetting to Learn Logic Programs</a> - <em><strong>AAAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13676986733133377042&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.ijcai.org/proceedings/2020/673" rel="noopener noreferrer">Turning 30: New Ideas in Inductive Logic Programming</a> - <em><strong>IJCAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=17980870844719684257&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/2005.02259.pdf" rel="noopener noreferrer">Learning programs by learning from failures</a> - <em><strong>Machine Learning</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=6797200487935462023&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.ijcai.org/proceedings/2020/320" rel="noopener noreferrer">Complete Bottom-Up Predicate Invention in Meta-Interpretive Learning</a> - <em><strong>IJCAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=6085183078630665234&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/2106.07464.pdf" rel="noopener noreferrer">Meta-Interpretive Learning as Metarule Specialisation</a> - <em><strong>Machine Learning</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=14684315775211086859&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.ijcai.org/Proceedings/16/Papers/278.pdf" rel="noopener noreferrer">Derivative-free optimization of high-dimensional non-convex functions by sequential random embeddings</a> - <em><strong>IJCAI'16</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=15955040483290586781&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/S0024610704006106" rel="noopener noreferrer">Finitely Generated Groups and First-Order Logic</a> - <em><strong>Journal of The London Mathematical Society-second Series</strong></em>, 2005. [<a href="https://scholar.google.com/scholar?cluster=3457158221419711506&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://vigilworkshop.github.io/static/papers-2021/25.pdf" rel="noopener noreferrer">Leveraging Language for Abstraction and Program Search</a> - <em><strong>ICML'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Leveraging+Language+for+Abstraction+and+Program+Search&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://cogtoolslab.github.io/pdf/wang_cogsci_2021a.pdf" rel="noopener noreferrer">Learning Part-Based Abstractions for Visual Object Concepts</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?lookup=0&amp;q=Learning+Part-Based+Abstractions+for+Visual+Object+Concepts&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Knowledge Representation / Commonsense Knowledgebase</p>
</h3><ul>
<li><a href="https://plato.stanford.edu/entries/knowledge-analysis/" rel="noopener noreferrer">The Analysis of Knowledge</a> - <em><strong>Plato Stanford</strong></em>.</li>
</ul>
<ul>
<li><a href="https://plato.stanford.edu/entries/common-knowledge/" rel="noopener noreferrer">Common Knowledge</a> - <em><strong>Plato Stanford</strong></em>.</li>
</ul>
<ul>
<li><a href="https://plato.stanford.edu/entries/sense-data/" rel="noopener noreferrer">Sense-Data</a> - <em><strong>Plato Stanford</strong></em>.</li>
</ul>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Epistemic_modal_logic" rel="noopener noreferrer">Epistemic Modal Logic</a> - <em><strong>Wikipedia</strong></em>.</li>
</ul>
<ul>
<li><a href="https://link.springer.com/article/10.1023/B:SYNT.0000024912.56773.5e" rel="noopener noreferrer">Logics for Epistemic Programs</a> - <em><strong>Synthese</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=11403619699670839488&amp;hl=en&amp;as_sdt=0,5&amp;as_vis=1" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://tomgruber.org/writing/ontolingua-kaj-1993.pdf" rel="noopener noreferrer">A Translation Approach to Portable Ontology Specifications</a> - <em><strong>Knowledge Acquisition</strong></em>, 1993. [<a href="https://scholar.google.com/scholar?cluster=14668658395073605123&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://www.cs.ox.ac.uk/activities/ieg/e-library/sources/harnad90_sgproblem.pdf" rel="noopener noreferrer">The Symbolic Grounding Problem</a> - <em><strong>Physica D: Nonlinear Phenomena</strong></em>, 1990. [<a href="https://scholar.google.com/scholar?cluster=6279614024681929496&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-7687.2007.00585.x?__cf_chl_captcha_tk__=pmd_Q6xVT1AstoEUxA7xS3_10HyDVsk8W_DzWgOPho_Njnw-1635210931-0-gqNtZGzNA1CjcnBszQvl" rel="noopener noreferrer">Learning overhypotheses with hierarchical Bayesian models</a> - <em><strong>Developmental Science</strong></em>, 2007. [<a href="https://scholar.google.com/scholar?cluster=18041836774924845900&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://escholarship.org/content/qt19v2r2ws/qt19v2r2ws.pdf" rel="noopener noreferrer">Learning Causal Schemata</a> - <em><strong>CogSci'07</strong></em>, 2007, [<a href="https://scholar.google.com/scholar?cluster=5008191267417189643&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://onlinelibrary.wiley.com/doi/full/10.1080/03640210701802071" rel="noopener noreferrer">A Rational Analysis of Rule-Based Concept Learning</a> - <em><strong>Cognitive Science</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=7765061503727822620&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://escholarship.org/content/qt50r1c7qh/qt50r1c7qh.pdf" rel="noopener noreferrer">Modeling semantic cognition as logical dimensionality reduction</a> - <em><strong>CogSci'08</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=17061801746839695691&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://web.mit.edu/tomeru/www/papers/tlss2010.pdf" rel="noopener noreferrer">Theory Acquisition as Stochastic Search</a> - <em><strong>CogSci'10</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=16324634056226561429&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://www.charleskemp.com/papers/kemptng09.pdf" rel="noopener noreferrer">A probabilistic model of theory formation</a> - <em><strong>Cognition</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=7705799129887482041&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://core.ac.uk/display/78064072" rel="noopener noreferrer">Bootstrapping in a language of thought: A formal model of numerical concept learning</a> - <em><strong>Cognition</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=13046606910781656302&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://www.charleskemp.com/papers/kemp_exploringtheconceptualuniverse.pdf" rel="noopener noreferrer">Exploring the Conceptual Universe</a> - <em><strong>Psychological Review</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=17824067813343816306&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://www.charleskemp.com/papers/kempj_ataxonomyofinductiveproblems.pdf" rel="noopener noreferrer">A taxonomy of inductive problems</a> - <em><strong>Psychonomic Bulletin &amp; Review</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=2571009743105592927&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://colala.berkeley.edu/papers/piantadosi2016logical.pdf" rel="noopener noreferrer">The Logical Primitives of Thought: Empirical Foundations for Compositional Cognitive Models</a> - <em><strong>Psychological Review</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=5316027496661813145&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12580" rel="noopener noreferrer">The Emergence of Organizing Structure in Conceptual Representation</a> - <em><strong>Cognitive Science</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=4986316323923233074&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://cogtoolslab.github.io/pdf/wang_cogsci_2021b.pdf" rel="noopener noreferrer">Theory Acquisition as Constraint-Based Program Synthesis</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=525148607069840280&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://escholarship.org/uc/item/9j00x928" rel="noopener noreferrer">Connecting perceptual and procedural abstractions in physical construction</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Connecting+perceptual+and+procedural+abstractions+in+physical+construction&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.biorxiv.org/content/10.1101/2021.03.19.385641v1.full.pdf" rel="noopener noreferrer">Invariant representation of physical stability in the human brain</a> - 2021. [<a href="https://scholar.google.com/scholar?cluster=17431019238600295521&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.4086&amp;rep=rep1&amp;type=pdf" rel="noopener noreferrer">Introduction to The Fluent Calculus</a> - <em><strong>Linkoeping University Electronic Press</strong></em>, 1998. [<a href="https://scholar.google.com/scholar?cluster=12069059079023496731&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0004370299000338" rel="noopener noreferrer">From situation calculus to fluent calculus: State update axioms as a solution to the inferential frame problem</a> - <em><strong>Artificial Intelligence</strong></em>, 1999. [<a href="https://scholar.google.com/scholar?cluster=10854895617698839149&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/0010027795006743" rel="noopener noreferrer">A representational analysis of numeration systems</a> - <em><strong>Cognition</strong></em>, 1995. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=8852566070856662412" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Cognitive Development / Commonsense Knowledgebase</p>
</h3><ul>
<li><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Cognitive+Development%3A+an+information+processing+approach&amp;btnG=" rel="noopener noreferrer">Cognitive development: An information processing approach</a> - <em><strong>B.Blackwell</strong></em>, 1991. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Cognitive+development%3A+An+information+processing+approach&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://psycnet.apa.org/record/1981-32566-001" rel="noopener noreferrer">From exploration to play: A cross-sectional study of infant free play behavior</a> - <em><strong>Developmental Psychology</strong></em>, 1981. [<a href="https://scholar.google.com/scholar?cluster=15547331535034599545&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://srcd.onlinelibrary.wiley.com/doi/abs/10.1111/1467-8624.00224" rel="noopener noreferrer">Detecting Blickets: How Young Children Use Information about Novel Causal Powers in Categorization and Induction</a> - <em><strong>Children Development</strong></em>, 2003. [<a href="https://scholar.google.com/scholar?cluster=9049737233568227380&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://eccl.scripts.mit.edu/papers/bonawitzandschulzseriousfun.pdf" rel="noopener noreferrer">Serious fun: Preschoolers engage in more exploratory play when evidence is confounded</a> - <em><strong>Developmental Psychology</strong></em>, 2007. [<a href="https://scholar.google.com/scholar?cluster=3033619407322882147&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0010027711000916" rel="noopener noreferrer">Where science starts: Spontaneous experiments in preschoolers' exploratory play</a> - <em><strong>Cognition</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=16321989770180281706" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://alisongopnik.com/Papers_Alison/Scientific%20Thinking%20in%20young%20Children.pdf" rel="noopener noreferrer">Scientific thinking in young children: Theoretical advances, empirical research, and policy implications</a> - <em><strong>Science</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=9103846738385460508&amp;hl=en&amp;as_sdt=2005" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="http://eccl.scripts.mit.edu/papers/Finding%20New%20Facts_%20Thinking%20New%20Thoughts.pdf" rel="noopener noreferrer">Finding New Facts; Thinking New Thoughts</a> - <em><strong>Advances in Child Development and Behavior</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Finding+new+facts%3B+thinking+new+thoughts&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0885201412000445" rel="noopener noreferrer">Theory learning as stochastic search in the language of thought</a> - <em><strong>Cognitive Development</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=8036476579458645432&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.science.org/doi/abs/10.1126/science.aan2317" rel="noopener noreferrer">Infants make more attempts to achieve a goal when they see adults persist</a> - <em><strong>Science</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=2617011825272996810&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://cognitivesciencesociety.org/cogsci20/papers/0716/0716.pdf" rel="noopener noreferrer">Knowing when to quit: Children consider access to solutions when deciding whether to persist</a> - <em><strong>CogSci'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=15997297570269958414&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://psyarxiv.com/aq3rp/" rel="noopener noreferrer">Bayesian Models of Conceptual Development: Learning as Building Models of the World</a> - <em><strong>Annual Review of Developmental Psychology</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=646614032563248495&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12765" rel="noopener noreferrer">Sticking to the Evidence? A Behavioral and Computational Case Study of Micro-Theory Change in the Domain of Magnetism</a> - <em><strong>Cognitive Science</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4409900195679222965&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://junyichu.mit.edu/sites/default/files/documents/2018-05-14%20CogSci%20Final.pdf" rel="noopener noreferrer">Cognitive pragmatism: Children flexibly choose between facts and conjectures</a> - <em><strong>CogSci'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=6978944437676543728&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://psyarxiv.com/9yra2/" rel="noopener noreferrer">Exploratory play, rational action, and efficient search</a> - <em><strong>CogSci'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=17529638197045429028&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://srcd.onlinelibrary.wiley.com/doi/full/10.1111/cdev.13647?saml_referrer" rel="noopener noreferrer">Children selectively endorse speculative conjectures</a> - <em><strong>Child Development</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=5672344544260882286&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://psycnet.apa.org/buy/2017-12497-003" rel="noopener noreferrer">Learning higher-order generalizations through free play: Evidence from 2- and 3-year-old children</a> - <em><strong>Developmental Psychology</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=4386474921214936914&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0502" rel="noopener noreferrer">Childhood as a solution to explore‚Äìexploit tensions</a> - <em><strong>Philosophical Transactions of the Royal Society B: Biological Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=11960188575664977017&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://www.nature.com/articles/s41467-021-23431-2" rel="noopener noreferrer">Children's exploratory play tracks the discriminability of hypotheses</a> - <em><strong>Nature Communications</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=12389351553206792907&amp;hl=en&amp;as_sdt=0,5&amp;as_ylo=2020" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://srcd.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-8624.2010.01499.x?saml_referrer" rel="noopener noreferrer">A Developmental Perspective on Executive Function</a> - <em><strong>Child Development</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=11347590808138984649&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://journals.sagepub.com/doi/pdf/10.1177/1745691620904771" rel="noopener noreferrer">Rethinking Executive Function and Its Development</a> - <em><strong>Psychological Science</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16570230278367237499&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Learning in the Open World / Commonsense Knowledgebase</p>
</h3><ul>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S002224961730010X" rel="noopener noreferrer">Online learning of symbolic concepts</a> - <em><strong>Journal of Mathematical Psychology</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?start=20&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5&amp;cites=8036476579458645432&amp;scipsc=" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7780542" rel="noopener noreferrer">Towards Open Set Deep Networks</a> - <em><strong>CVPR'16</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=3571743951915089896&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/2002.04108.pdf" rel="noopener noreferrer">Adversarial Filters of Dataset Biases</a> - <em><strong>ICML'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=11617966867048191189&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/2009.01797.pdf" rel="noopener noreferrer">A Wholistic View of Continual Learning with Deep Neural Networks: Forgotten Lessons and the Bridge to Active and Open World Learning</a> - 2020. [<a href="https://scholar.google.com/scholar?cluster=2640432662088551010&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Learning_to_Learn_Image_Classifiers_With_Visual_Analogy_CVPR_2019_paper.pdf" rel="noopener noreferrer">Learning to Learn Image Classifiers with Visual Analogy</a> - <em><strong>CVPR'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=6285495755337309034&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/1804.04340v2.pdf" rel="noopener noreferrer">Zero-Shot Object Detection</a> - <em><strong>ECCV'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=2027060030559987993&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/2103.02603v1.pdf" rel="noopener noreferrer">Towards Open World Object Detection</a> - <em><strong>CVPR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=9715328489246217151&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/JosephKJ/OWOD" rel="noopener noreferrer">Project (‚≠ê1.1k)</a>].</li>
</ul>
<ul>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3123266.3123323" rel="noopener noreferrer">Learning to Recognise Unseen Classes by A Few Similes</a> - <em><strong>MM'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?q=related:FZZr2BK0U6YJ:scholar.google.com/&amp;scioq=Learning+to+Recognise+Unseen+Classes+by+A+Few+Similes&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Ulf Grenander / Commonsense Knowledgebase</p>
</h3><ul>
<li><a href="https://www.dam.brown.edu/ptg/REPORTS/calculustext.PDF" rel="noopener noreferrer">A Calculus of Ideas: A Mathematical Study of Thinking</a> - <em><strong>World Scientific Publishing Company</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=12182416000849265255&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://global.oup.com/academic/product/general-pattern-theory-9780198536710?cc=lt&amp;lang=de#" rel="noopener noreferrer">General Pattern Theory: A Mathematical Study of Regular Structures</a> - <em><strong>Oxford University Press</strong></em>, 1993. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=General+Pattern+Theory&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Michael Tomasello / Commonsense Knowledgebase</p>
</h3><ul>
<li><a href="https://1lib.net/book/541274/39859f" rel="noopener noreferrer">Origins of human communication</a> - <em><strong>MIT Press</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=2553369883266458474" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://hk1lib.org/book/541275/1452f8?id=541275&amp;secret=1452f8" rel="noopener noreferrer">The cultural origins of human cognition</a> - <em><strong>Havard University Press</strong></em>, 2000. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=5000469061641945144" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Judea Pearl / Commonsense Knowledgebase</p>
</h3><ul>
<li><a href="http://bayes.cs.ucla.edu/WHY/" rel="noopener noreferrer">The Book of Why: The New Science of Cause and Effect</a> - <em><strong>Basic Books</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=2505901292485349932&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<ul>
<li><a href="https://hk1lib.org/book/2780725/2ec8f1?id=2780725&amp;secret=2ec8f1" rel="noopener noreferrer">Causality: Models, Reasoning and Inference</a> - <em><strong>Cambridge University Press</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=10996260119229499611&amp;hl=en&amp;as_sdt=0,5&amp;as_vis=1" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h3><p>Daniel Kahneman / Commonsense Knowledgebase</p>
</h3><ul>
<li><a href="https://hk1lib.org/book/2181569/f5e85a?id=2181569&amp;secret=f5e85a" rel="noopener noreferrer">Thinking, fast and slow</a> - <em><strong>Farrar Straus Giroux</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=3255681708785115121" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/joho/awesome-code-review/">9. Awesome Code Review</a></h2><h3><p>Articles</p>
</h3><ul>
<li><a href="https://rethought.se/research/modern-code-reviews/" rel="noopener noreferrer">Modern Code Reviews</a> Arguably belongs in academic paper, but it's a website giving an overview of evidence on code review by facet/context. Links off to a bunch of papers.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/ligurio/awesome-ci/">10. Awesome Ci</a></h2><ul>
<li><p>Name: <a href="https://woodpecker.laszlo.cloud/" rel="noopener noreferrer">Woodpecker</a></p>
<p>Description: Continuous Integration service</p>
<p>Features: Woodpecker is a community fork of the <a href="https://github.com/drone" rel="noopener noreferrer">Drone CI</a> system.</p>
<p>Supported repositories: GitHub, GitLab, Gitea, BitBucket, Custom</p>
<p>Documentation: <a href="https://woodpecker-ci.github.io/docs/intro" rel="noopener noreferrer">Documentation</a></p>
<p>Price: Free Open Source (Apache License 2.0)</p>
<p>Stars: <a href="https://github.com/laszlocph/woodpecker/" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/laszlocph/woodpecker.svg" alt="Stars" /></a></p>
</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/riderx/awesome-capacitor/">11. Awesome Capacitor</a></h2><h3><p>Official plugins</p>
</h3><ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/action-sheet" rel="noopener noreferrer">Action Sheet (‚≠ê601)</a> - Provides access to native Action Sheets.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/app-launcher" rel="noopener noreferrer">App Launcher (‚≠ê601)</a> - Allows to check if an app can be opened and open it.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/browser" rel="noopener noreferrer">Browser (‚≠ê601)</a> - Provides the ability to open an in-app browser and subscribe to browser events.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/camera" rel="noopener noreferrer">Camera (‚≠ê601)</a> - Provides the ability to take a photo with the camera or choose an existing one from the photo album.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/clipboard" rel="noopener noreferrer">Clipboard (‚≠ê601)</a> - Enables copy and pasting to/from the system clipboard.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/dialog" rel="noopener noreferrer">Dialog (‚≠ê601)</a> - Provides methods for triggering native dialog windows for alerts, confirmations, and input prompts.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/geolocation" rel="noopener noreferrer">Geolocation (‚≠ê601)</a> - Provides simple methods for getting and tracking the current position of the device using GPS, along with altitude, heading, and speed information if available.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/haptics" rel="noopener noreferrer">Haptics (‚≠ê601)</a> - Provides physical feedback to the user through touch or vibration.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/keyboard" rel="noopener noreferrer">Keyboard (‚≠ê601)</a> - Provides keyboard display and visibility control, along with event tracking when the keyboard shows and hides.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/local-notifications" rel="noopener noreferrer">Local Notifications (‚≠ê601)</a> - Provides a way to schedule device notifications locally (i.e. without a server sending push notifications).</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/motion" rel="noopener noreferrer">Motion (‚≠ê601)</a> - Tracks accelerometer and device orientation (compass heading, etc.).</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/network" rel="noopener noreferrer">Network (‚≠ê601)</a> - Provides network and connectivity information.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/push-notifications" rel="noopener noreferrer">Push Notifications (‚≠ê601)</a> - Provides access to native push notifications.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/share" rel="noopener noreferrer">Share (‚≠ê601)</a> - Provides methods for sharing content in any sharing-enabled apps the user may have installed.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/splash-screen" rel="noopener noreferrer">Splash Screen (‚≠ê601)</a> - Provides methods for showing or hiding a Splash image.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/status-bar" rel="noopener noreferrer">Status Bar (‚≠ê601)</a> - Provides methods for configuring the style of the Status Bar, along with showing or hiding it.</li>
</ul>
<ul>
<li><a href="https://github.com/ionic-team/capacitor-plugins/tree/main/text-zoom" rel="noopener noreferrer">Text Zoom (‚≠ê601)</a> - Provides the ability to change Web View text size for visual accessibility.</li>
</ul>
<h3><p>Community plugins</p>
</h3><ul>
<li><a href="https://github.com/capacitor-community/admob" rel="noopener noreferrer">Admob (‚≠ê247)</a> - A native plugin for AdMob.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/apple-sign-in" rel="noopener noreferrer">Apple sign in (‚≠ê157)</a> - Capacitor Sign in with Apple.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/background-geolocation" rel="noopener noreferrer">Background geolocation (‚≠ê219)</a> - Receive geolocation updates even while app is backgrounded.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/barcode-scanner" rel="noopener noreferrer">Barcode scanner (‚≠ê443)</a> - A fast and efficient QR / barcode scanner for Capacitor.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/bluetooth-le" rel="noopener noreferrer">Bluetooth-le (‚≠ê325)</a> - Bluetooth Low Energy.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/camera-preview" rel="noopener noreferrer">Camera preview (‚≠ê207)</a> - Camera preview.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/capacitor-googlemaps-native" rel="noopener noreferrer">Googlemaps (‚≠ê157)</a> - Plugin using native Maps API for Android and iOS.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/contacts" rel="noopener noreferrer">Contacts (‚≠ê139)</a> - Contacts Plugin for Capacitor.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/date-picker" rel="noopener noreferrer">Date picker (‚≠ê92)</a> - Native DateTime Picker Plugin for Capacitor Apps.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/facebook-login" rel="noopener noreferrer">Facebook Login (‚≠ê110)</a> - A native plugin for Facebook Login.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/flipper" rel="noopener noreferrer">Flipper (‚≠ê12)</a> - A native plugin for flipper debugger.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/http" rel="noopener noreferrer">Http (‚≠ê211)</a> - A native HTTP plugin for CORS-free requests and file transfers.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/intercom" rel="noopener noreferrer">Intercom (‚≠ê66)</a> - Enable Intercom features for Capacitor apps.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/media" rel="noopener noreferrer">Media (‚≠ê125)</a> - Enable some media features for Capacitor such as create albums, save videos, gifs and more.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/native-audio" rel="noopener noreferrer">Native audio (‚≠ê137)</a> - A native plugin for native audio engine.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/native-market" rel="noopener noreferrer">Native market (‚≠ê30)</a> - A native market plugin for linking to google play or app store.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/react-hooks" rel="noopener noreferrer">React hooks (‚≠ê258)</a> - React Hooks for Capacitor apps.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/realm" rel="noopener noreferrer">Realm (‚≠ê26)</a> - A native plugin for MongoDB Realm.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/speech-recognition" rel="noopener noreferrer">Speech recognition (‚≠ê109)</a> - A native plugin for speech recognition.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/sqlite" rel="noopener noreferrer">Sqlite (‚≠ê579)</a> - Native &amp; electron SQLite databases.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/stripe" rel="noopener noreferrer">Stripe (‚≠ê213)</a> - Stripe SDK bindings for Capacitor Applications.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/tauri" rel="noopener noreferrer">Tauri (‚≠ê127)</a> - Support for the Tauri platform.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/text-to-speech" rel="noopener noreferrer">Text to speech (‚≠ê112)</a> - Synthesizing speech from text.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/twitter" rel="noopener noreferrer">Twitter (‚≠ê11)</a> - Enable TwitterKit features for Capacitor.</li>
</ul>
<ul>
<li><a href="https://github.com/capacitor-community/uxcam" rel="noopener noreferrer">Uxcam (‚≠ê5)</a> - UXCam and FullStory analytics. It uses UXCam for Android and iOS platforms and FullStory for Web/PWA.</li>
</ul>
<h3><p>Other plugins</p>
</h3><ul>
<li><a href="https://github.com/mahnuh/capacitor-plugin-app-tracking-transparency" rel="noopener noreferrer">App Tracking Transparency (‚≠ê35)</a> - Request user authorization to access app-related data for tracking the user or the device. iOS only.</li>
</ul>
<ul>
<li><a href="https://github.com/rdlabo-team/capacitor-brotherprint" rel="noopener noreferrer">Brother Print (‚≠ê21)</a> - A native Brother Print SDK implementation for iOS &amp; Android.</li>
</ul>
<ul>
<li><a href="https://github.com/epicshaggy/capacitor-native-biometric" rel="noopener noreferrer">Biometric (‚≠ê193)</a> - Use biometrics confirm device owner presence or authenticate users.</li>
</ul>
<ul>
<li><a href="https://github.com/micahlt/ionicCapacitorDarkMode" rel="noopener noreferrer">Dark mode (‚≠ê1)</a> - Monitor the changes made to system's dark mode.</li>
</ul>
<ul>
<li><a href="https://github.com/EinfachHans/capacitor-email-composer" rel="noopener noreferrer">Email composer (‚≠ê24)</a> - Open a native E-Mail Composer within your Capacitor App.</li>
</ul>
<ul>
<li><a href="https://github.com/SpellChucker/capacitor-plugin-facebook-analytics" rel="noopener noreferrer">Facebook Analytics (‚≠ê5)</a> - Facebook Analytics Plugin for Capacitor.</li>
</ul>
<ul>
<li><a href="https://github.com/hinddeep/capacitor-file-selector" rel="noopener noreferrer">File selector (‚≠ê14)</a> - Select files form Android/iOS devices and the web.</li>
</ul>
<ul>
<li><a href="https://github.com/calvinckho/capacitor-jitsi-meet" rel="noopener noreferrer">Jitsi (‚≠ê91)</a> - Make video calls through the free, open-sourced Jitsi video platform.</li>
</ul>
<ul>
<li><a href="https://github.com/Elvincth/capacitor-plugin-lightsensor" rel="noopener noreferrer">Lightsensor (‚≠ê3)</a> - Get the illuminance level on the device.</li>
</ul>
<ul>
<li><a href="https://github.com/RaphaelWoude/capacitor-native-settings" rel="noopener noreferrer">Native settings (‚≠ê116)</a> - Open native settings screens.</li>
</ul>
<ul>
<li>NativeScript<ul>
<li><a href="https://github.com/NativeScript/capacitor" rel="noopener noreferrer">NativeScript Capacitor (‚≠ê42)</a> - Empower Capacitor with native APIs.</li>
<li><a href="https://github.com/EddyVerbruggen/nativescript-ar" rel="noopener noreferrer">Augmented Reality (‚≠ê117)</a> - Add AR experiences.</li>
</ul>
</li>
</ul>
<ul>
<li><a href="https://github.com/hugotomazi/navigation-bar" rel="noopener noreferrer">Navigation bar (‚≠ê79)</a> - Navigation Bar manipulation, hide/show.</li>
</ul>
<ul>
<li><a href="https://github.com/Nodonisko/capacitor-rate-app" rel="noopener noreferrer">Rate app (‚≠ê191)</a> - Let users rate your app using native rate app dialog for both Android and iOS.</li>
</ul>
<ul>
<li><a href="https://github.com/Ayush-Rajniwal/cap-read-sms" rel="noopener noreferrer">Read sms (‚≠ê3)</a> - Read the user's SMS with their permission.</li>
</ul>
<ul>
<li><a href="https://github.com/ludufre/capacitor-screenshot" rel="noopener noreferrer">Screenshot (‚≠ê18)</a> - Take a screenshot of the current view.</li>
</ul>
<ul>
<li><a href="https://github.com/tavosansal/capacitor-plugin-send-intent" rel="noopener noreferrer">Send intent (‚≠ê12)</a> - Expose a listener in your JavaScript application for when another application sends data to your Capacitor application via the Android share menu or share sheet.</li>
</ul>
<ul>
<li><a href="https://github.com/getsentry/sentry-capacitor" rel="noopener noreferrer">Sentry (‚≠ê132)</a> - Add Sentry error tracking and performance monitoring for Capacitor apps.</li>
</ul>
<ul>
<li><a href="https://github.com/eventOneHQ/capacitor-stripe-terminal" rel="noopener noreferrer">Stripe terminal (‚≠ê30)</a> - Stripe Terminal Plugin for Capacitor.</li>
</ul>
<ul>
<li><a href="https://github.com/crabbydavis/sprig" rel="noopener noreferrer">Sprig (‚≠ê0)</a> - All-in-one‚Äâ product research platform.</li>
</ul>
<ul>
<li><a href="https://github.com/jbrown0824/capacitor-square-payments" rel="noopener noreferrer">Square Payments (‚≠ê2)</a> - Enable Square Payments for Capacitor.</li>
</ul>
<ul>
<li><a href="https://github.com/rbedemann/capacitor-sumup-plugin" rel="noopener noreferrer">Sumup (‚≠ê4)</a> - SumUp Mobile SDK.</li>
</ul>
<hr><ul><li> Prev: <a href="/2021/11/19/">Nov 19, 2021</a></li><li> Next: <a href="/2021/11/17/">Nov 17, 2021</a></li></ul>
    </main>
  </body>
</html>
