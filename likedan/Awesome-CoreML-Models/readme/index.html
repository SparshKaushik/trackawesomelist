<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <title>Awesome CoreML Models (likedan/Awesome-CoreML-Models) Overview - Track Awesome List</title>
    <meta property="og:url" content="https://www.trackawesomelist.com/likedan/Awesome-CoreML-Models/readme/" />
    <meta property="og:type" content="summary" />
    <meta property="og:title" content="Awesome CoreML Models Overview" />
    <meta property="og:description" content="Largest list of models for Core ML (for iOS 11+)" />
    <meta property="og:site_name" content="Track Awesome List" />
    <style>
      main {
        max-width: 1024px;
        margin: 0 auto;
        padding: 0 0.5em;
      }
      :root,[data-color-mode=light][data-light-theme=light],[data-color-mode=dark][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}[data-color-mode=light][data-light-theme=dark],[data-color-mode=dark][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}.markdown-body{word-wrap:break-word;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;font-size:16px;line-height:1.5}.markdown-body:before{content:"";display:table}.markdown-body:after{clear:both;content:"";display:table}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:var(--color-danger-fg)}.markdown-body .anchor{float:left;margin-left:-20px;padding-right:4px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body p,.markdown-body blockquote,.markdown-body ul,.markdown-body ol,.markdown-body dl,.markdown-body table,.markdown-body pre,.markdown-body details{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;background-color:var(--color-border-default);border:0;margin:24px 0;padding:0}.markdown-body blockquote{color:var(--color-fg-muted);border-left:.25em solid var(--color-border-default);padding:0 1em}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:var(--color-fg-default);vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 tt,.markdown-body h1 code,.markdown-body h2 tt,.markdown-body h2 code,.markdown-body h3 tt,.markdown-body h3 code,.markdown-body h4 tt,.markdown-body h4 code,.markdown-body h5 tt,.markdown-body h5 code,.markdown-body h6 tt,.markdown-body h6 code{font-size:inherit;padding:0 .2em}.markdown-body h1{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:2em}.markdown-body h2{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{color:var(--color-fg-muted);font-size:.85em}.markdown-body summary h1,.markdown-body summary h2,.markdown-body summary h3,.markdown-body summary h4,.markdown-body summary h5,.markdown-body summary h6{display:inline-block}.markdown-body summary h1 .anchor,.markdown-body summary h2 .anchor,.markdown-body summary h3 .anchor,.markdown-body summary h4 .anchor,.markdown-body summary h5 .anchor,.markdown-body summary h6 .anchor{margin-left:-40px}.markdown-body summary h1,.markdown-body summary h2{border-bottom:0;padding-bottom:0}.markdown-body ul,.markdown-body ol{padding-left:2em}.markdown-body ul.no-list,.markdown-body ol.no-list{padding:0;list-style-type:none}.markdown-body ol[type="1"]{list-style-type:decimal}.markdown-body ol[type=a]{list-style-type:lower-alpha}.markdown-body ol[type=i]{list-style-type:lower-roman}.markdown-body div>ol:not([type]){list-style-type:decimal}.markdown-body ul ul,.markdown-body ul ol,.markdown-body ol ol,.markdown-body ol ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{margin-top:16px;padding:0;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{margin-bottom:16px;padding:0 16px}.markdown-body table{width:100%;width:-webkit-max-content;width:-webkit-max-content;width:max-content;max-width:100%;display:block;overflow:auto}.markdown-body table th{font-weight:600}.markdown-body table th,.markdown-body table td{border:1px solid var(--color-border-default);padding:6px 13px}.markdown-body table tr{background-color:var(--color-canvas-default);border-top:1px solid var(--color-border-muted)}.markdown-body table tr:nth-child(2n){background-color:var(--color-canvas-subtle)}.markdown-body table img{background-color:rgba(0,0,0,0)}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:var(--color-canvas-default)}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:rgba(0,0,0,0)}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{float:left;width:auto;border:1px solid var(--color-border-default);margin:13px 0 0;padding:7px;display:block;overflow:hidden}.markdown-body span.frame span img{float:left;display:block}.markdown-body span.frame span span{clear:both;color:var(--color-fg-default);padding:5px 0 0;display:block}.markdown-body span.align-center{clear:both;display:block;overflow:hidden}.markdown-body span.align-center>span{text-align:center;margin:13px auto 0;display:block;overflow:hidden}.markdown-body span.align-center span img{text-align:center;margin:0 auto}.markdown-body span.align-right{clear:both;display:block;overflow:hidden}.markdown-body span.align-right>span{text-align:right;margin:13px 0 0;display:block;overflow:hidden}.markdown-body span.align-right span img{text-align:right;margin:0}.markdown-body span.float-left{float:left;margin-right:13px;display:block;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{float:right;margin-left:13px;display:block;overflow:hidden}.markdown-body span.float-right>span{text-align:right;margin:13px auto 0;display:block;overflow:hidden}.markdown-body code,.markdown-body tt{background-color:var(--color-neutral-muted);border-radius:6px;margin:0;padding:.2em .4em;font-size:85%}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{-webkit-text-decoration:inherit;-webkit-text-decoration:inherit;text-decoration:inherit}.markdown-body samp{font-size:85%}.markdown-body pre{word-wrap:normal}.markdown-body pre code{font-size:100%}.markdown-body pre>code{word-break:normal;white-space:pre;background:0 0;border:0;margin:0;padding:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{word-break:normal;margin-bottom:0}.markdown-body .highlight pre,.markdown-body pre{background-color:var(--color-canvas-subtle);border-radius:6px;padding:16px;font-size:85%;line-height:1.45;overflow:auto}.markdown-body pre code,.markdown-body pre tt{max-width:auto;line-height:inherit;word-wrap:normal;background-color:rgba(0,0,0,0);border:0;margin:0;padding:0;display:inline;overflow:visible}.markdown-body .csv-data td,.markdown-body .csv-data th{text-align:left;white-space:nowrap;padding:5px;font-size:12px;line-height:1;overflow:hidden}.markdown-body .csv-data .blob-num{text-align:right;background:var(--color-canvas-default);border:0;padding:10px 8px 9px}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{background:var(--color-canvas-subtle);border-top:0;font-weight:600}.markdown-body [data-footnote-ref]:before{content:"["}.markdown-body [data-footnote-ref]:after{content:"]"}.markdown-body .footnotes{color:var(--color-fg-muted);border-top:1px solid var(--color-border-default);font-size:12px}.markdown-body .footnotes ol{padding-left:16px}.markdown-body .footnotes li{position:relative}.markdown-body .footnotes li:target:before{pointer-events:none;content:"";border:2px solid var(--color-accent-emphasis);border-radius:6px;position:absolute;top:-8px;bottom:-8px;left:-24px;right:-8px}.markdown-body .footnotes li:target{color:var(--color-fg-default)}.markdown-body .footnotes .data-footnote-backref g-emoji{font-family:monospace}.markdown-body{background-color:var(--color-canvas-default);color:var(--color-fg-default)}.markdown-body a{color:var(--color-accent-fg);text-decoration:none}.markdown-body a:hover{text-decoration:underline}.markdown-body iframe{background-color:#fff;border:0;margin-bottom:16px}.markdown-body svg.octicon{fill:currentColor}.markdown-body .anchor>.octicon{display:inline}.markdown-body .highlight .token.keyword,.gfm-highlight .token.keyword{color:var(--color-prettylights-syntax-keyword)}.markdown-body .highlight .token.tag .token.class-name,.markdown-body .highlight .token.tag .token.script .token.punctuation,.gfm-highlight .token.tag .token.class-name,.gfm-highlight .token.tag .token.script .token.punctuation{color:var(--color-prettylights-syntax-storage-modifier-import)}.markdown-body .highlight .token.operator,.markdown-body .highlight .token.number,.markdown-body .highlight .token.boolean,.markdown-body .highlight .token.tag .token.punctuation,.markdown-body .highlight .token.tag .token.script .token.script-punctuation,.markdown-body .highlight .token.tag .token.attr-name,.gfm-highlight .token.operator,.gfm-highlight .token.number,.gfm-highlight .token.boolean,.gfm-highlight .token.tag .token.punctuation,.gfm-highlight .token.tag .token.script .token.script-punctuation,.gfm-highlight .token.tag .token.attr-name{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.function,.gfm-highlight .token.function{color:var(--color-prettylights-syntax-entity)}.markdown-body .highlight .token.string,.gfm-highlight .token.string{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.comment,.gfm-highlight .token.comment{color:var(--color-prettylights-syntax-comment)}.markdown-body .highlight .token.class-name,.gfm-highlight .token.class-name{color:var(--color-prettylights-syntax-variable)}.markdown-body .highlight .token.regex,.gfm-highlight .token.regex{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.regex .regex-delimiter,.gfm-highlight .token.regex .regex-delimiter{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.tag .token.tag,.markdown-body .highlight .token.property,.gfm-highlight .token.tag .token.tag,.gfm-highlight .token.property{color:var(--color-prettylights-syntax-entity-tag)}
    </style>
  </head>
  <body>
    <main data-color-mode="light" data-light-theme="light" data-dark-theme="dark" class="markdown-body">
      <h1>Awesome CoreML Models Overview</h1>
<p>Largest list of models for Core ML (for iOS 11+)</p>
<p><a href="/">🏠 Home</a><span> · </span><a href="https://www.trackawesomelist.com/likedan/Awesome-CoreML-Models/rss.xml">🔥 Feed</a><span> · </span><a href="https://trackawesomelist.us17.list-manage.com/subscribe?u=d2f0117aa829c83a63ec63c2f&id=36a103854c">📮 Subscribe</a><span> · </span><a href="https://github.com/sponsors/theowenyoung">❤️  Sponsor</a><span> · </span><a href="https://github.com/likedan/Awesome-CoreML-Models">😺 likedan/Awesome-CoreML-Models</a><span> · </span><span>⭐ 6.7K</span><span> · </span><span>🏷️ Computer Science</span></p>
<p><span>[ </span><a href="/likedan/Awesome-CoreML-Models/">Daily</a><span> / </span><a href="/likedan/Awesome-CoreML-Models/week/">Weekly</a><span> / </span><span>Overview</span><span> ]</span></p>


<p>
<img src="https://github.com/likedan/Awesome-CoreML-Models/raw/master/images/coreml.png" width="329" height="295" />
</p>

<p>Since iOS 11, Apple released Core ML framework to help developers integrate machine learning models into applications. <a href="https://developer.apple.com/documentation/coreml" rel="noopener noreferrer">The official documentation</a></p>
<p>We've put up the largest collection of machine learning models in Core ML format, to help  iOS, macOS, tvOS, and watchOS developers experiment with machine learning techniques.</p>
<p>If you've converted a Core ML model, feel free to submit a <a href="https://github.com/likedan/Awesome-CoreML-Models/compare" rel="noopener noreferrer">pull request</a>.</p>
<p>Recently, we've included visualization tools. And here's one <a href="https://lutzroeder.github.io/Netron" rel="noopener noreferrer">Netron</a>.</p>
<p><a href="https://github.com/sindresorhus/awesome" rel="noopener noreferrer"><img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" alt="Awesome" /></a>
<a href="http://makeapullrequest.com" rel="noopener noreferrer"><img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg" alt="PRs Welcome" /></a></p>
<h1 id="models"><a class="anchor" aria-hidden="true" tabindex="-1" href="#models"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Models</h1><h2 id="image---metadatatext"><a class="anchor" aria-hidden="true" tabindex="-1" href="#image---metadatatext"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image - Metadata/Text</h2><p><em>Models that take image data as input and output useful information about the image.</em></p>
<ul>
<li><strong>TextDetection</strong> - Detecting text using Vision built-in model in real-time. <a href="https://github.com/likedan/Awesome-CoreML-Models/blob/master/README.md/" rel="noopener noreferrer">Download</a> | <a href="https://github.com/tucan9389/TextDetection-CoreML" rel="noopener noreferrer">Demo</a> | <a href="https://developer.apple.com/documentation/vision" rel="noopener noreferrer">Reference</a></li>
<li><strong>PhotoAssessment</strong> - Photo Assessment using Core ML and Metal. <a href="https://github.com/yulingtianxia/PhotoAssessment/blob/master/PhotoAssessment-Sample/Sources/NIMANasnet.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/yulingtianxia/PhotoAssessment" rel="noopener noreferrer">Demo</a> | <a href="https://arxiv.org/abs/1709.05424" rel="noopener noreferrer">Reference</a></li>
<li><strong>PoseEstimation</strong> - Estimating human pose from a picture for mobile. <a href="https://github.com/edvardHua/PoseEstimationForMobile/tree/master/release" rel="noopener noreferrer">Download</a> | <a href="https://github.com/tucan9389/PoseEstimation-CoreML" rel="noopener noreferrer">Demo</a> | <a href="https://github.com/edvardHua/PoseEstimationForMobile" rel="noopener noreferrer">Reference</a></li>
<li><strong>MobileNet</strong> - Detects the dominant objects present in an image. <a href="https://github.com/hollance/MobileNet-CoreML/raw/master/MobileNet.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/hollance/MobileNet-CoreML" rel="noopener noreferrer">Demo</a> | <a href="https://arxiv.org/abs/1704.04861" rel="noopener noreferrer">Reference</a></li>
<li><strong>Places CNN</strong> - Detects the scene of an image from 205 categories such as bedroom, forest, coast etc. <a href="https://github.com/hollance/MobileNet-CoreML/raw/master/MobileNet.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/chenyi1989/CoreMLDemo" rel="noopener noreferrer">Demo</a> | <a href="http://places.csail.mit.edu/index.html" rel="noopener noreferrer">Reference</a></li>
<li><strong>Inception v3</strong> - Detects the dominant objects present in an image. <a href="https://github.com/yulingtianxia/Core-ML-Sample/blob/master/CoreMLSample/Inceptionv3.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/yulingtianxia/Core-ML-Sample/" rel="noopener noreferrer">Demo</a> | <a href="https://arxiv.org/abs/1512.00567" rel="noopener noreferrer">Reference</a></li>
<li><strong>ResNet50</strong> - Detects the dominant objects present in an image. <a href="https://github.com/ytakzk/CoreML-samples/blob/master/CoreML-samples/Resnet50.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/ytakzk/CoreML-samples" rel="noopener noreferrer">Demo</a> | <a href="https://arxiv.org/abs/1512.03385" rel="noopener noreferrer">Reference</a></li>
<li><strong>VGG16</strong> - Detects the dominant objects present in an image. <a href="https://docs-assets.developer.apple.com/coreml/models/VGG16.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/alaphao/CoreMLExample" rel="noopener noreferrer">Demo</a> | <a href="https://arxiv.org/abs/1409.1556" rel="noopener noreferrer">Reference</a></li>
<li><strong>Car Recognition</strong> - Predict the brand &amp; model of a car. <a href="https://github.com/likedan/Core-ML-Car-Recognition/blob/master/Convert/CarRecognition.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/ytakzk/CoreML-samples" rel="noopener noreferrer">Demo</a> | <a href="http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html" rel="noopener noreferrer">Reference</a></li>
<li><strong>YOLO</strong> - Recognize what the objects are inside a given image and where they are in the image. <a href="https://github.com/hollance/YOLO-CoreML-MPSNNGraph/blob/master/TinyYOLO-CoreML/TinyYOLO-CoreML/TinyYOLO.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/hollance/YOLO-CoreML-MPSNNGraph" rel="noopener noreferrer">Demo</a> | <a href="http://machinethink.net/blog/object-detection-with-yolo" rel="noopener noreferrer">Reference</a></li>
<li><strong>AgeNet</strong> - Predict a person's age from one's portrait. <a href="https://drive.google.com/file/d/0B1ghKa_MYL6mT1J3T1BEeWx4TWc/view?usp=sharing" rel="noopener noreferrer">Download</a> | <a href="https://github.com/cocoa-ai/FacesVisionDemo" rel="noopener noreferrer">Demo</a> | <a href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/" rel="noopener noreferrer">Reference</a></li>
<li><strong>GenderNet</strong> - Predict a person's gender from one's portrait. <a href="https://drive.google.com/file/d/0B1ghKa_MYL6mYkNsZHlyc2ZuaFk/view?usp=sharing" rel="noopener noreferrer">Download</a> | <a href="https://github.com/cocoa-ai/FacesVisionDemo" rel="noopener noreferrer">Demo</a> | <a href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/" rel="noopener noreferrer">Reference</a></li>
<li><strong>MNIST</strong> - Predict handwritten (drawn) digits from images. <a href="https://github.com/ph1ps/MNIST-CoreML/raw/master/MNISTPrediction/MNIST.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/ph1ps/MNIST-CoreML" rel="noopener noreferrer">Demo</a> | <a href="http://yann.lecun.com/exdb/mnist/" rel="noopener noreferrer">Reference</a></li>
<li><strong>EmotionNet</strong> - Predict a person's emotion from one's portrait. <a href="https://drive.google.com/file/d/0B1ghKa_MYL6mTlYtRGdXNFlpWDQ/view?usp=sharing" rel="noopener noreferrer">Download</a> | <a href="https://github.com/cocoa-ai/FacesVisionDemo" rel="noopener noreferrer">Demo</a> | <a href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/" rel="noopener noreferrer">Reference</a></li>
<li><strong>SentimentVision</strong> - Predict positive or negative sentiments from images. <a href="https://drive.google.com/open?id=0B1ghKa_MYL6mZ0dITW5uZlgyNTg" rel="noopener noreferrer">Download</a> | <a href="https://github.com/cocoa-ai/SentimentVisionDemo" rel="noopener noreferrer">Demo</a> | <a href="http://www.sciencedirect.com/science/article/pii/S0262885617300355?via%3Dihub" rel="noopener noreferrer">Reference</a></li>
<li><strong>Food101</strong> - Predict the type of foods from images. <a href="https://drive.google.com/open?id=0B5TjkH3njRqnVjBPZGRZbkNITjA" rel="noopener noreferrer">Download</a> | <a href="https://github.com/ph1ps/Food101-CoreML" rel="noopener noreferrer">Demo</a> | <a href="http://visiir.lip6.fr/explore" rel="noopener noreferrer">Reference</a></li>
<li><strong>Oxford102</strong> - Detect the type of flowers from images. <a href="https://drive.google.com/file/d/0B1ghKa_MYL6meDBHT2NaZGxkNzQ/view?usp=sharing" rel="noopener noreferrer">Download</a> | <a href="https://github.com/cocoa-ai/FlowersVisionDemo" rel="noopener noreferrer">Demo</a> | <a href="http://jimgoo.com/flower-power/" rel="noopener noreferrer">Reference</a></li>
<li><strong>FlickrStyle</strong> - Detect the artistic style of images. <a href="https://drive.google.com/file/d/0B1ghKa_MYL6meDBHT2NaZGxkNzQ/view?usp=sharing" rel="noopener noreferrer">Download</a> | <a href="https://github.com/cocoa-ai/StylesVisionDemo" rel="noopener noreferrer">Demo</a> | <a href="http://sergeykarayev.com/files/1311.3715v3.pdf" rel="noopener noreferrer">Reference</a></li>
<li><strong>RN1015k500</strong> - Predict the location where a picture was taken. <a href="https://s3.amazonaws.com/aws-bigdata-blog/artifacts/RN1015k500/RN1015k500.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/awslabs/MXNet2CoreML_iOS_sample_app" rel="noopener noreferrer">Demo</a> | <a href="https://aws.amazon.com/blogs/ai/estimating-the-location-of-images-using-mxnet-and-multimedia-commons-dataset-on-aws-ec2" rel="noopener noreferrer">Reference</a></li>
<li><strong>Nudity</strong> - Classifies an image either as NSFW (nude) or SFW (not nude)
<a href="https://drive.google.com/open?id=0B5TjkH3njRqncDJpdDB1Tkl2S2s" rel="noopener noreferrer">Download</a> | <a href="https://github.com/ph1ps/Nudity-CoreML" rel="noopener noreferrer">Demo</a> | <a href="https://github.com/yahoo/open_nsfw" rel="noopener noreferrer">Reference</a></li>
<li><strong>TextRecognition (ML Kit)</strong> - Recognizing text using ML Kit built-in model in real-time. <a href="https://github.com/likedan/Awesome-CoreML-Models/blob/master/README.md/" rel="noopener noreferrer">Download</a> | <a href="https://github.com/tucan9389/TextRecognition-MLKit" rel="noopener noreferrer">Demo</a> | <a href="https://firebase.google.com/docs/ml-kit/ios/recognize-text" rel="noopener noreferrer">Reference</a></li>
<li><strong>ImageSegmentation</strong> - Segment the pixels of a camera frame or image into a predefined set of classes. <a href="https://developer.apple.com/machine-learning/models/" rel="noopener noreferrer">Download</a> | <a href="https://github.com/tucan9389/ImageSegmentation-CoreML" rel="noopener noreferrer">Demo</a> | <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" rel="noopener noreferrer">Reference</a></li>
<li><strong>DepthPrediction</strong> - Predict the depth from a single image. <a href="https://developer.apple.com/machine-learning/models/" rel="noopener noreferrer">Download</a> | <a href="https://github.com/tucan9389/DepthPrediction-CoreML" rel="noopener noreferrer">Demo</a> | <a href="https://github.com/iro-cp/FCRN-DepthPrediction" rel="noopener noreferrer">Reference</a></li>
</ul>
<h2 id="image---image"><a class="anchor" aria-hidden="true" tabindex="-1" href="#image---image"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image - Image</h2><p><em>Models that transform images.</em></p>
<ul>
<li><strong>HED</strong> - Detect nested edges from a color image. <a href="https://github.com/s1ddok/HED-CoreML/blob/master/HED-CoreML/Models/HED_so.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/s1ddok/HED-CoreML" rel="noopener noreferrer">Demo</a> | <a href="http://dl.acm.org/citation.cfm?id=2654889" rel="noopener noreferrer">Reference</a></li>
<li><strong>AnimeScale2x</strong> - Process a bicubic-scaled anime-style artwork <a href="https://github.com/imxieyi/waifu2x-ios/blob/master/waifu2x/models/anime_noise0_model.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/imxieyi/waifu2x-ios" rel="noopener noreferrer">Demo</a> | <a href="https://arxiv.org/abs/1501.00092" rel="noopener noreferrer">Reference</a></li>
</ul>
<h2 id="text---metadatatext"><a class="anchor" aria-hidden="true" tabindex="-1" href="#text---metadatatext"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Text - Metadata/Text</h2><p><em>Models that process text data</em></p>
<ul>
<li><strong>Sentiment Polarity</strong> - Predict positive or negative sentiments from sentences. <a href="https://github.com/cocoa-ai/SentimentCoreMLDemo/raw/master/SentimentPolarity/Resources/SentimentPolarity.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/cocoa-ai/SentimentCoreMLDemo" rel="noopener noreferrer">Demo</a> | <a href="http://boston.lti.cs.cmu.edu/classes/95-865-K/HW/HW3/" rel="noopener noreferrer">Reference</a></li>
<li><strong>DocumentClassification</strong> - Classify news articles into 1 of 5 categories. <a href="https://github.com/toddkramer/DocumentClassifier/blob/master/Sources/DocumentClassification.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/toddkramer/DocumentClassifier" rel="noopener noreferrer">Demo</a> | <a href="https://github.com/toddkramer/DocumentClassifier/" rel="noopener noreferrer">Reference</a></li>
<li><strong>iMessage Spam Detection</strong> - Detect whether a message is spam. <a href="https://github.com/gkswamy98/imessage-spam-detection/blob/master/MessageClassifier.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/gkswamy98/imessage-spam-detection/tree/master" rel="noopener noreferrer">Demo</a> | <a href="http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/" rel="noopener noreferrer">Reference</a></li>
<li><strong>NamesDT</strong> - Gender Classification using DecisionTreeClassifier <a href="https://github.com/cocoa-ai/NamesCoreMLDemo/blob/master/Names/Resources/NamesDT.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/cocoa-ai/NamesCoreMLDemo" rel="noopener noreferrer">Demo</a> | <a href="http://nlpforhackers.io/" rel="noopener noreferrer">Reference</a></li>
<li><strong>Personality Detection</strong> - Predict personality based on user documents (sentences). <a href="https://github.com/novinfard/profiler-sentiment-analysis/tree/master/ios_app/ProfilerSA/ML%20Models" rel="noopener noreferrer">Download</a> | <a href="https://github.com/novinfard/profiler-sentiment-analysis/" rel="noopener noreferrer">Demo</a> | <a href="https://github.com/novinfard/profiler-sentiment-analysis/blob/master/dissertation-v6.pdf" rel="noopener noreferrer">Reference</a></li>
<li><strong>BERT for Question answering</strong> - Swift Core ML 3 implementation of BERT for Question answering <a href="https://github.com/huggingface/swift-coreml-transformers/blob/master/Resources/BERTSQUADFP16.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/huggingface/swift-coreml-transformers#-bert" rel="noopener noreferrer">Demo</a> | <a href="https://github.com/huggingface/pytorch-transformers#run_squadpy-fine-tuning-on-squad-for-question-answering" rel="noopener noreferrer">Reference</a></li>
<li><strong>GPT-2</strong> - OpenAI GPT-2 Text generation (Core ML 3) <a href="https://github.com/huggingface/swift-coreml-transformers/blob/master/Resources/gpt2-512.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/huggingface/swift-coreml-transformers#-gpt-2" rel="noopener noreferrer">Demo</a> | <a href="https://github.com/huggingface/pytorch-transformers" rel="noopener noreferrer">Reference</a></li>
</ul>
<h2 id="miscellaneous"><a class="anchor" aria-hidden="true" tabindex="-1" href="#miscellaneous"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Miscellaneous</h2><ul>
<li><strong>Exermote</strong> - Predicts the exercise, when iPhone is worn on right upper arm. <a href="https://github.com/Lausbert/Exermote/tree/master/ExermoteInference" rel="noopener noreferrer">Download</a> | <a href="https://github.com/Lausbert/Exermote/tree/master/ExermoteInference" rel="noopener noreferrer">Demo</a> | <a href="http://lausbert.com/2017/08/03/exermote/" rel="noopener noreferrer">Reference</a></li>
<li><strong>GestureAI</strong> - Recommend an artist based on given location and genre. <a href="https://goo.gl/avdMjD" rel="noopener noreferrer">Download</a> | <a href="https://github.com/akimach/GestureAI-CoreML-iOS" rel="noopener noreferrer">Demo</a> | <a href="https://github.com/akimach/GestureAI-iOS/tree/master/GestureAI" rel="noopener noreferrer">Reference</a></li>
<li><strong>Artists Recommendation</strong> - Recommend an artist based on given location and genre. <a href="https://github.com/agnosticdev/Blog-Examples/blob/master/UsingCoreMLtoCreateASongRecommendationEngine/Artist.mlmodel" rel="noopener noreferrer">Download</a> | <a href="https://github.com/likedan/Awesome-CoreML-Models/blob/master/README.md/" rel="noopener noreferrer">Demo</a> | <a href="https://www.agnosticdev.com/blog-entry/python/using-scikit-learn-and-coreml-create-music-recommendation-engine" rel="noopener noreferrer">Reference</a></li>
<li><strong>ChordSuggester</strong> - Predicts the most likely next chord based on the entered Chord Progression. <a href="https://github.com/carlosmbe/Mac-CoreML-Chord-Suggester/blob/main/MLChordSuggester.mlpackage.zip" rel="noopener noreferrer">Download</a> | <a href="https://github.com/carlosmbe/Mac-CoreML-Chord-Suggester/tree/main" rel="noopener noreferrer">Demo</a> | <a href="https://medium.com/@huanlui/chordsuggester-i-3a1261d4ea9e" rel="noopener noreferrer">Reference</a></li>
</ul>
<h2 id="speech-processing"><a class="anchor" aria-hidden="true" tabindex="-1" href="#speech-processing"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Speech Processing</h2><ul>
<li><strong>Streaming ASR</strong> – Real-time streaming speech recognition engine for iOS. Uses Fast Conformer + CTC, runs fully on device.<br /><a href="https://github.com/Otosaku/OtosakuStreamingASR-iOS/releases" rel="noopener noreferrer">Download</a> | <a href="https://github.com/Otosaku/OtosakuStreamingASR-iOS" rel="noopener noreferrer">Demo</a> | <a href="https://github.com/Otosaku/OtosakuStreamingASR-iOS" rel="noopener noreferrer">Reference</a></li>
<li><strong>Keyword Spotting (KWS)</strong> – On-device keyword spotting engine using lightweight CRNN architecture, optimized for mobile devices.<br /><a href="https://github.com/Otosaku/OtosakuKWS-iOS/releases" rel="noopener noreferrer">Download</a> | <a href="https://github.com/Otosaku/OtosakuKWS-iOS" rel="noopener noreferrer">Demo</a> | <a href="https://github.com/Otosaku/OtosakuKWS-iOS" rel="noopener noreferrer">Reference</a></li>
</ul>
<h1 id="visualization-tools"><a class="anchor" aria-hidden="true" tabindex="-1" href="#visualization-tools"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Visualization Tools</h1><p><em>Tools that help visualize CoreML Models</em></p>
<ul>
<li><a href="https://lutzroeder.github.io/Netron" rel="noopener noreferrer">Netron</a></li>
</ul>
<h1 id="supported-formats"><a class="anchor" aria-hidden="true" tabindex="-1" href="#supported-formats"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Supported formats</h1><p><em>List of model formats that could be converted to Core ML with examples</em></p>
<ul>
<li><a href="https://apple.github.io/coremltools/generated/coremltools.converters.caffe.convert.html" rel="noopener noreferrer">Caffe</a></li>
<li><a href="https://apple.github.io/coremltools/generated/coremltools.converters.keras.convert.html" rel="noopener noreferrer">Keras</a></li>
<li><a href="https://apple.github.io/coremltools/generated/coremltools.converters.xgboost.convert.html" rel="noopener noreferrer">XGBoost</a></li>
<li><a href="https://apple.github.io/coremltools/generated/coremltools.converters.sklearn.convert.html" rel="noopener noreferrer">Scikit-learn</a></li>
<li><a href="https://aws.amazon.com/blogs/ai/bring-machine-learning-to-ios-apps-using-apache-mxnet-and-apple-core-ml/" rel="noopener noreferrer">MXNet</a></li>
<li><a href="https://apple.github.io/coremltools/generated/coremltools.converters.libsvm.convert.html" rel="noopener noreferrer">LibSVM</a></li>
<li><a href="https://github.com/prisma-ai/torch2coreml" rel="noopener noreferrer">Torch7</a></li>
</ul>
<h1 id="the-gold"><a class="anchor" aria-hidden="true" tabindex="-1" href="#the-gold"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The Gold</h1><p><em>Collections of machine learning models that could be converted to Core ML</em></p>
<ul>
<li><a href="https://github.com/BVLC/caffe/wiki/Model-Zoo" rel="noopener noreferrer">Caffe Model Zoo</a> - Big list of models in Caffe format.</li>
<li><a href="https://github.com/tensorflow/models" rel="noopener noreferrer">TensorFlow Models</a> - Models for TensorFlow.</li>
<li><a href="https://github.com/tensorflow/models/tree/master/research/slim/README.md" rel="noopener noreferrer">TensorFlow Slim Models</a> - Another collection of TensorFlow Models.</li>
<li><a href="https://mxnet.incubator.apache.org/model_zoo/" rel="noopener noreferrer">MXNet Model Zoo</a> - Collection of MXNet models.</li>
</ul>
<p><em>Individual machine learning models that could be converted to Core ML. We'll keep adjusting the list as they become converted.</em></p>
<ul>
<li><a href="https://github.com/MiyainNYC/Visual-Memorability-through-Caffe" rel="noopener noreferrer">LaMem</a> Score the memorability of pictures.</li>
<li><a href="https://github.com/BestiVictory/ILGnet" rel="noopener noreferrer">ILGnet</a> The aesthetic evaluation of images.</li>
<li><a href="https://github.com/richzhang/colorization" rel="noopener noreferrer">Colorization</a> Automatic colorization using deep neural networks.</li>
<li><a href="https://github.com/rezoo/illustration2vec" rel="noopener noreferrer">Illustration2Vec</a> Estimating a set of tags and extracting semantic feature vectors from given illustrations.</li>
<li><a href="https://github.com/tianzhi0549/CTPN" rel="noopener noreferrer">CTPN</a> Detecting text in natural image.</li>
<li><a href="https://github.com/msracver/Deep-Image-Analogy" rel="noopener noreferrer">Image Analogy</a> Find semantically-meaningful dense correspondences between two input images.</li>
<li><a href="https://github.com/twerkmeister/iLID" rel="noopener noreferrer">iLID</a> Automatic spoken language identification.</li>
<li><a href="https://github.com/liuziwei7/fashion-detection" rel="noopener noreferrer">Fashion Detection</a> Cloth detection from images.</li>
<li><a href="https://github.com/imatge-upc/saliency-2016-cvpr" rel="noopener noreferrer">Saliency</a> The prediction of salient areas in images has been traditionally addressed with hand-crafted features.</li>
<li><a href="https://github.com/DolotovEvgeniy/DeepPyramid" rel="noopener noreferrer">Face Detection</a> Detect face from image.</li>
<li><a href="https://github.com/CongWeilin/mtcnn-caffe" rel="noopener noreferrer">mtcnn</a> Joint Face Detection and Alignment.</li>
<li><a href="https://github.com/scottworkman/deephorizon" rel="noopener noreferrer">deephorizon</a> Single image horizon line estimation.</li>
</ul>
<h1 id="contributing-and-license"><a class="anchor" aria-hidden="true" tabindex="-1" href="#contributing-and-license"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributing and License</h1><ul>
<li><a href="https://github.com/likedan/Awesome-CoreML-Models/blob/master/.github/CONTRIBUTING.md" rel="noopener noreferrer">See the guide</a></li>
<li>Distributed under the MIT license. See LICENSE for more information.</li>
</ul>


    </main>
  </body>
</html>
