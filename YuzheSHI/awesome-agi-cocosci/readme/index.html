<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <title>Awesome Agi Cocosci (YuzheSHI/awesome-agi-cocosci) Overview - Track Awesome List</title>
    <meta property="og:url" content="https://www.trackawesomelist.com/YuzheSHI/awesome-agi-cocosci/readme/" />
    <meta property="og:type" content="summary" />
    <meta property="og:title" content="Awesome Agi Cocosci Overview" />
    <meta property="og:description" content="An awesome &amp; curated list for Artificial General Intelligence, an emerging inter-discipline field that combines artificial intelligence and computational cognitive sciences." />
    <meta property="og:site_name" content="Track Awesome List" />
    <style>
      main {
        max-width: 1024px;
        margin: 0 auto;
        padding: 0 0.5em;
      }
      :root,[data-color-mode=light][data-light-theme=light],[data-color-mode=dark][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}[data-color-mode=light][data-light-theme=dark],[data-color-mode=dark][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}.markdown-body{word-wrap:break-word;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;font-size:16px;line-height:1.5}.markdown-body:before{content:"";display:table}.markdown-body:after{clear:both;content:"";display:table}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:var(--color-danger-fg)}.markdown-body .anchor{float:left;margin-left:-20px;padding-right:4px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body p,.markdown-body blockquote,.markdown-body ul,.markdown-body ol,.markdown-body dl,.markdown-body table,.markdown-body pre,.markdown-body details{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;background-color:var(--color-border-default);border:0;margin:24px 0;padding:0}.markdown-body blockquote{color:var(--color-fg-muted);border-left:.25em solid var(--color-border-default);padding:0 1em}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:var(--color-fg-default);vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 tt,.markdown-body h1 code,.markdown-body h2 tt,.markdown-body h2 code,.markdown-body h3 tt,.markdown-body h3 code,.markdown-body h4 tt,.markdown-body h4 code,.markdown-body h5 tt,.markdown-body h5 code,.markdown-body h6 tt,.markdown-body h6 code{font-size:inherit;padding:0 .2em}.markdown-body h1{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:2em}.markdown-body h2{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{color:var(--color-fg-muted);font-size:.85em}.markdown-body summary h1,.markdown-body summary h2,.markdown-body summary h3,.markdown-body summary h4,.markdown-body summary h5,.markdown-body summary h6{display:inline-block}.markdown-body summary h1 .anchor,.markdown-body summary h2 .anchor,.markdown-body summary h3 .anchor,.markdown-body summary h4 .anchor,.markdown-body summary h5 .anchor,.markdown-body summary h6 .anchor{margin-left:-40px}.markdown-body summary h1,.markdown-body summary h2{border-bottom:0;padding-bottom:0}.markdown-body ul,.markdown-body ol{padding-left:2em}.markdown-body ul.no-list,.markdown-body ol.no-list{padding:0;list-style-type:none}.markdown-body ol[type="1"]{list-style-type:decimal}.markdown-body ol[type=a]{list-style-type:lower-alpha}.markdown-body ol[type=i]{list-style-type:lower-roman}.markdown-body div>ol:not([type]){list-style-type:decimal}.markdown-body ul ul,.markdown-body ul ol,.markdown-body ol ol,.markdown-body ol ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{margin-top:16px;padding:0;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{margin-bottom:16px;padding:0 16px}.markdown-body table{width:100%;width:-webkit-max-content;width:-webkit-max-content;width:max-content;max-width:100%;display:block;overflow:auto}.markdown-body table th{font-weight:600}.markdown-body table th,.markdown-body table td{border:1px solid var(--color-border-default);padding:6px 13px}.markdown-body table tr{background-color:var(--color-canvas-default);border-top:1px solid var(--color-border-muted)}.markdown-body table tr:nth-child(2n){background-color:var(--color-canvas-subtle)}.markdown-body table img{background-color:rgba(0,0,0,0)}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:var(--color-canvas-default)}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:rgba(0,0,0,0)}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{float:left;width:auto;border:1px solid var(--color-border-default);margin:13px 0 0;padding:7px;display:block;overflow:hidden}.markdown-body span.frame span img{float:left;display:block}.markdown-body span.frame span span{clear:both;color:var(--color-fg-default);padding:5px 0 0;display:block}.markdown-body span.align-center{clear:both;display:block;overflow:hidden}.markdown-body span.align-center>span{text-align:center;margin:13px auto 0;display:block;overflow:hidden}.markdown-body span.align-center span img{text-align:center;margin:0 auto}.markdown-body span.align-right{clear:both;display:block;overflow:hidden}.markdown-body span.align-right>span{text-align:right;margin:13px 0 0;display:block;overflow:hidden}.markdown-body span.align-right span img{text-align:right;margin:0}.markdown-body span.float-left{float:left;margin-right:13px;display:block;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{float:right;margin-left:13px;display:block;overflow:hidden}.markdown-body span.float-right>span{text-align:right;margin:13px auto 0;display:block;overflow:hidden}.markdown-body code,.markdown-body tt{background-color:var(--color-neutral-muted);border-radius:6px;margin:0;padding:.2em .4em;font-size:85%}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{-webkit-text-decoration:inherit;-webkit-text-decoration:inherit;text-decoration:inherit}.markdown-body samp{font-size:85%}.markdown-body pre{word-wrap:normal}.markdown-body pre code{font-size:100%}.markdown-body pre>code{word-break:normal;white-space:pre;background:0 0;border:0;margin:0;padding:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{word-break:normal;margin-bottom:0}.markdown-body .highlight pre,.markdown-body pre{background-color:var(--color-canvas-subtle);border-radius:6px;padding:16px;font-size:85%;line-height:1.45;overflow:auto}.markdown-body pre code,.markdown-body pre tt{max-width:auto;line-height:inherit;word-wrap:normal;background-color:rgba(0,0,0,0);border:0;margin:0;padding:0;display:inline;overflow:visible}.markdown-body .csv-data td,.markdown-body .csv-data th{text-align:left;white-space:nowrap;padding:5px;font-size:12px;line-height:1;overflow:hidden}.markdown-body .csv-data .blob-num{text-align:right;background:var(--color-canvas-default);border:0;padding:10px 8px 9px}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{background:var(--color-canvas-subtle);border-top:0;font-weight:600}.markdown-body [data-footnote-ref]:before{content:"["}.markdown-body [data-footnote-ref]:after{content:"]"}.markdown-body .footnotes{color:var(--color-fg-muted);border-top:1px solid var(--color-border-default);font-size:12px}.markdown-body .footnotes ol{padding-left:16px}.markdown-body .footnotes li{position:relative}.markdown-body .footnotes li:target:before{pointer-events:none;content:"";border:2px solid var(--color-accent-emphasis);border-radius:6px;position:absolute;top:-8px;bottom:-8px;left:-24px;right:-8px}.markdown-body .footnotes li:target{color:var(--color-fg-default)}.markdown-body .footnotes .data-footnote-backref g-emoji{font-family:monospace}.markdown-body{background-color:var(--color-canvas-default);color:var(--color-fg-default)}.markdown-body a{color:var(--color-accent-fg);text-decoration:none}.markdown-body a:hover{text-decoration:underline}.markdown-body iframe{background-color:#fff;border:0;margin-bottom:16px}.markdown-body svg.octicon{fill:currentColor}.markdown-body .anchor>.octicon{display:inline}.markdown-body .highlight .token.keyword,.gfm-highlight .token.keyword{color:var(--color-prettylights-syntax-keyword)}.markdown-body .highlight .token.tag .token.class-name,.markdown-body .highlight .token.tag .token.script .token.punctuation,.gfm-highlight .token.tag .token.class-name,.gfm-highlight .token.tag .token.script .token.punctuation{color:var(--color-prettylights-syntax-storage-modifier-import)}.markdown-body .highlight .token.operator,.markdown-body .highlight .token.number,.markdown-body .highlight .token.boolean,.markdown-body .highlight .token.tag .token.punctuation,.markdown-body .highlight .token.tag .token.script .token.script-punctuation,.markdown-body .highlight .token.tag .token.attr-name,.gfm-highlight .token.operator,.gfm-highlight .token.number,.gfm-highlight .token.boolean,.gfm-highlight .token.tag .token.punctuation,.gfm-highlight .token.tag .token.script .token.script-punctuation,.gfm-highlight .token.tag .token.attr-name{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.function,.gfm-highlight .token.function{color:var(--color-prettylights-syntax-entity)}.markdown-body .highlight .token.string,.gfm-highlight .token.string{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.comment,.gfm-highlight .token.comment{color:var(--color-prettylights-syntax-comment)}.markdown-body .highlight .token.class-name,.gfm-highlight .token.class-name{color:var(--color-prettylights-syntax-variable)}.markdown-body .highlight .token.regex,.gfm-highlight .token.regex{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.regex .regex-delimiter,.gfm-highlight .token.regex .regex-delimiter{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.tag .token.tag,.markdown-body .highlight .token.property,.gfm-highlight .token.tag .token.tag,.gfm-highlight .token.property{color:var(--color-prettylights-syntax-entity-tag)}
    </style>
  </head>
  <body>
    <main data-color-mode="light" data-light-theme="light" data-dark-theme="dark" class="markdown-body">
      <h1>Awesome Agi Cocosci Overview</h1>
<p>An awesome & curated list for Artificial General Intelligence, an emerging inter-discipline field that combines artificial intelligence and computational cognitive sciences.</p>
<p><a href="/">🏠 Home</a><span> · </span><a href="https://www.trackawesomelist.com/YuzheSHI/awesome-agi-cocosci/rss.xml">🔥 Feed</a><span> · </span><a href="https://trackawesomelist.us17.list-manage.com/subscribe?u=d2f0117aa829c83a63ec63c2f&id=36a103854c">📮 Subscribe</a><span> · </span><a href="https://github.com/sponsors/theowenyoung">❤️  Sponsor</a><span> · </span><a href="https://github.com/SHI-Yu-Zhe/awesome-agi-cocosci">😺 YuzheSHI/awesome-agi-cocosci</a><span> · </span><span>⭐ 336</span><span> · </span><span>🏷️ Theory</span></p>
<p><span>[ </span><a href="/YuzheSHI/awesome-agi-cocosci/">Daily</a><span> / </span><a href="/YuzheSHI/awesome-agi-cocosci/week/">Weekly</a><span> / </span><span>Overview</span><span> ]</span></p>
<div>
    <img width="400" height="253" src="https://github.com/SHI-Yu-Zhe/awesome-agi-cocosci/raw/master/assets/abd_map.png" alt="Roadmap of studying Abduction" />
</div>

<h1 id="awesome-artificial-general-intelligence-and-computational-cognitive-sciences-awesome"><a class="anchor" aria-hidden="true" tabindex="-1" href="#awesome-artificial-general-intelligence-and-computational-cognitive-sciences-awesome"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Awesome Artificial General Intelligence and Computational Cognitive Sciences <a href="https://awesome.re" rel="noopener noreferrer"><img src="https://awesome.re/badge.svg" alt="Awesome" /></a></h1><p>An <strong>awesome &amp; curated</strong> list for <strong>Artificial General Intelligence</strong>, an emerging inter-discipline field that combines artificial intelligence and computational cognitive sciences as majority, alone with probability and statistics, formal logic, cognitive and developmental psychology, computational philosophy, cognitive neuroscience, and computational sociology. We are promoting high-level machine intelligence by getting inspirations from the way that human learns and thinks, while obtaining a deeper understanding of human cognition simultaneously. We believe that this kind of reciprocative research is a potential way towards our big picture: building human-level intelligent systems with capabilities such as abstracting, explaining, learning, planning, and making decisions. And such intelligence may generally help people improve scientific research, engineering, and the arts, which are the hallmarks of human intelligence.</p>
<p><em><strong>Awesome AGI &amp; CoCoSci</strong></em> is an all-in-one collection, consisting of recources from basic courses and tutorials, to papers and books around diverse topics in mutiple perspectives. Both junior and senior researchers, whether learning, working on, or working around AGI and CoCoSci, meet their interest here.</p>
<h2 id="contributing"><a class="anchor" aria-hidden="true" tabindex="-1" href="#contributing"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributing</h2><p>Contributions are greatly welcomed! Please refer to <a href="https://github.com/SHI-Yu-Zhe/awesome-agi-cocosci/blob/master/README.md/Contributing.md" rel="noopener noreferrer">Contribution Guidelines</a> before taking any actions.</p>
<p><span></span></p>
<h2 id="contents"><a class="anchor" aria-hidden="true" tabindex="-1" href="#contents"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contents</h2><ul>
<li><a href="#papers">Papers</a><ul>
<li><a href="#abduction">Abduction</a><ul>
<li><a href="#explanation">Explanation</a></li>
<li><a href="#scientific-discovery">Scientific Discovery</a></li>
<li><a href="#rationalization">Rationalization</a></li>
<li><a href="#applications-in-ai">Applications in AI</a></li>
</ul>
</li>
<li><a href="#bayesian-modeling">Bayesian Modeling</a><ul>
<li><a href="#bayesian-induction">Bayesian Induction</a></li>
<li><a href="#generative-model">Generative Model</a></li>
<li><a href="#nonparametric-model">Nonparametric Model</a></li>
<li><a href="#bayesian-optimization">Bayesian Optimization</a></li>
</ul>
</li>
<li><a href="#concepts">Concepts</a><ul>
<li><a href="#theory-of-concepts">Theory of Concepts</a></li>
<li><a href="#human-concept-representation">Human Concept Represenataion</a></li>
<li><a href="#ai-concept-representation">AI Concept Representation</a></li>
</ul>
</li>
<li><a href="#complexity--information-theory">Complexity &amp; Information Theory</a><ul>
<li><a href="#theory">Theory</a></li>
<li><a href="#dimensionality-reduction">Dimensionality Reduction</a></li>
<li><a href="#visual-complexity">Visual Complexity</a></li>
</ul>
</li>
<li><a href="#communications">Communications</a><ul>
<li><a href="#non-verbal-communication">Non-Verbal Communication</a></li>
<li><a href="#pragmatics">Pragmatics</a></li>
<li><a href="#language-compositionality">Language Compositionality</a></li>
<li><a href="#coordination">Coordination</a></li>
</ul>
</li>
<li><a href="#domain-specific-language">Domain Specific Language</a><ul>
<li><a href="#design-theory">Design Theory</a></li>
<li><a href="#design-practises">Design Practises</a></li>
<li><a href="#design-automation">Design Automation</a></li>
<li><a href="#imperative-dsl-applications">Imperative DSL Applications</a></li>
<li><a href="#declarative-dsl-applications">Declarative DSL Applications</a></li>
<li><a href="#logic-dsl-applications">Logic DSL Applications</a></li>
<li><a href="#dsl-program-synthesis">DSL Program Synthesis</a></li>
<li><a href="#cognitive-foundations">Cognitive Foundations</a></li>
</ul>
</li>
<li><a href="#problem-solving">Problem Solving</a><ul>
<li><a href="#human-level-problem-solving">Human-Level Problem Solving</a></li>
<li><a href="#planning">Planning</a></li>
<li><a href="#intrinsic-motivation">Intrinsic Motivation</a></li>
<li><a href="#reinforcement-learning">Reinforcement Learning</a></li>
<li><a href="#inverse-reinforcement-learning">Inverse Reinforcement Learning</a></li>
</ul>
</li>
<li><a href="#system-1--system-2">System 1 &amp; System 2</a><ul>
<li><a href="#dual-coding-theory">Dual-Coding Theory</a></li>
<li><a href="#neural-symbolic-ai">Neural-Symbolic AI</a></li>
</ul>
</li>
<li><a href="#explainability">Explainability</a><ul>
<li><a href="#trustworthy-ai">Trustworthy AI</a></li>
<li><a href="#strong-machine-learning">Strong Machine Learning</a></li>
<li><a href="#explainable-deep-learning">Explainable Deep Learning</a></li>
</ul>
</li>
<li><a href="#embodied-intelligence">Embodied Intelligence</a></li>
<li><a href="#evolutionary-intelligence">Evolutionary Intelligence</a></li>
<li><a href="#methodologies-for-experiments">Methodologies for Experiments</a><ul>
<li><a href="#quantitative-analysis">Quantitative Analysis</a></li>
<li><a href="#scaling-up-behavioral-studies">Scaling Up Behavioral Studies</a></li>
<li><a href="#decision-making">Decision Making</a></li>
<li><a href="#question-answering">Question Answering</a></li>
<li><a href="#human-machine-comparison">Human-Machine Comparison</a></li>
<li><a href="#association-test">Association Test</a></li>
<li><a href="#virtual-reality">Virtual Reality</a></li>
</ul>
</li>
<li><a href="#meta-level-considerations">Meta-Level Considerations</a><ul>
<li><a href="#meta-learning">Meta Learning</a></li>
<li><a href="#marrs-levels-of-analysis">Marr's Levels of Analysis</a></li>
<li><a href="#gestalt">Gestalt</a></li>
<li><a href="#the-aha-moment">The Aha! Moment</a></li>
<li><a href="#rationality">Rationality</a></li>
<li><a href="#cognitive-architecture">Cognitive Architecture</a></li>
</ul>
</li>
<li><a href="#science-logology">Science Logology</a><ul>
<li><a href="#philosophy-of-science">Philosophy of Science</a></li>
<li><a href="#science-of-science">Science of Science</a></li>
<li><a href="#literature-mining">Literature Mining</a></li>
<li><a href="#scientific-writing">Scientific Writing</a></li>
<li><a href="#science-education">Science Education</a></li>
<li><a href="#democratization-of-science">Democratization of Science</a></li>
<li><a href="#laboratory-automation">Laboratory Automation</a></li>
<li><a href="#ai-assisted-research">AI Assisted Research</a></li>
</ul>
</li>
<li><a href="#theory-of-mind">Theory of Mind</a></li>
<li><a href="#analogy">Analogy</a></li>
<li><a href="#causality">Causality</a></li>
<li><a href="#commonsense">Commonsense</a><ul>
<li><a href="#intuitive-physics">Intuitive Physics</a></li>
<li><a href="#ai-commonsense-reasoning">AI Commonsense Reasoning</a></li>
<li><a href="#commonsense-knowledgebase">Commonsense Knowledgebase</a></li>
</ul>
</li>
<li><a href="#inductive-logic--program-synthesis">Inductive Logic &amp; Program Synthesis</a></li>
<li><a href="#knowledge-representation">Knowledge Representation</a></li>
<li><a href="#cognitive-development">Cognitive Development</a></li>
<li><a href="#learning-in-the-open-world">Learning in the Open World</a></li>
<li><a href="#learning-with-cognitive-plausibility">Learning with Cognitive Plausibility</a>  </li>
</ul>
</li>
<li><a href="#academic-tools">Academic Tools</a><ul>
<li><a href="#courses">Courses</a></li>
<li><a href="#programming">Programming</a></li>
<li><a href="#paper-writing">Paper Writing</a></li>
<li><a href="#paper-reading">Paper Reading</a></li>
<li><a href="#literature-management">Literature Management</a></li>
<li><a href="#knowledge-management">Knowledge Management</a></li>
</ul>
</li>
<li><a href="#institute--researcher">Institute &amp; Researcher</a><ul>
<li><a href="#mit">MIT</a></li>
<li><a href="#stanford">Stanford</a></li>
<li><a href="#princeton">Princeton</a></li>
<li><a href="#harvard">Harvard</a></li>
<li><a href="#ucla">UCLA</a></li>
<li><a href="#uc-berkeley">UC Berkeley</a></li>
<li><a href="#bnu">BNU</a></li>
<li><a href="#pku">PKU</a></li>
<li><a href="#ucsd">UCSD</a></li>
<li><a href="#nyu">NYU</a></li>
<li><a href="#jhu">JHU</a></li>
<li><a href="#sit">SIT</a></li>
</ul>
</li>
<li><a href="#people--book">People &amp; Book</a><ul>
<li><a href="#john-hopcroft">John Hopcroft</a></li>
<li><a href="#ulf-grenander">Ulf Grenander</a></li>
<li><a href="#david-marr">David Marr</a></li>
<li><a href="#michael-tomasello">Michael Tomasello</a></li>
<li><a href="#judea-pearl">Judea Pearl</a></li>
<li><a href="#susan-carey">Susan Carey</a></li>
<li><a href="#daniel-kahneman">Daniel Kahneman</a></li>
<li><a href="#karl-popper">Karl Popper</a></li>
</ul>
</li>
<li><a href="#about">About</a></li>
</ul>
<h2 id="papers"><a class="anchor" aria-hidden="true" tabindex="-1" href="#papers"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Papers</h2><h3 id="abduction"><a class="anchor" aria-hidden="true" tabindex="-1" href="#abduction"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Abduction</h3><h4 id="explanation"><a class="anchor" aria-hidden="true" tabindex="-1" href="#explanation"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Explanation</h4><ul>
<li><p><a href="https://plato.stanford.edu/entries/abduction/index.html" rel="noopener noreferrer">Abduction</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Abduction, one of the three thinking patterns besides Induction and Deduction, being unique for its potential to introduce new ideas into current knowledge.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/scientific-explanation/" rel="noopener noreferrer">Scientific Explanation</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Scientific Explanation, a canonical application of Abduction.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/scientific-reduction/" rel="noopener noreferrer">Scientific Reduction</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Scientific Reduction, which comes with no explicit boundary with Explanation.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/logic-nonmonotonic/" rel="noopener noreferrer">Non-monotonic Logic</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Non-monotonic Logic, a family of formal frameworks devised to capture and represent defeasible inference.</p>
</li>
<li><p><a href="https://4lib.org/book/702071/e8ffe8" rel="noopener noreferrer">Philosophical Writings of Peirce</a> - <em><strong>Courier Corporation</strong></em>, 1955. [<a href="https://scholar.google.com/scholar?cluster=3917019015464129592" rel="noopener noreferrer">All Versions</a>]. Original writings by C. S. Peirce, the philosopher who first introduces the concept of Abduction.</p>
</li>
<li><p><a href="https://www.hps.cam.ac.uk/files/lipton-inference.pdf" rel="noopener noreferrer">Inference to the Best Explanation</a> - <em><strong>Routledge</strong></em>, 1991. [<a href="https://scholar.google.com/scholar?cluster=5097986614430666854" rel="noopener noreferrer">All Versions</a>]. Lipton's original paper on Inference to the Best Explanation as a specialized condition of Abduction.</p>
</li>
<li><p><a href="https://link.springer.com/book/10.1007/978-94-017-1733-5" rel="noopener noreferrer">Abductive Reasoning and Learning</a> - <em><strong>Springer</strong></em>, 2000. [<a href="https://scholar.google.com/scholar?cluster=12074269365138058159" rel="noopener noreferrer">All Versions</a>]. This book contains leading survey papers on the various aspects of Abduction, both logical and numerical approaches.</p>
</li>
<li><p><a href="https://link.springer.com/book/10.1007%2F978-3-642-03631-6" rel="noopener noreferrer">Abductive Cognition: The Epistemological and Eco-Cognitive Dimensions of Hypothetical Reasoning</a> - <em><strong>Springer</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=8707351442527595188" rel="noopener noreferrer">All Versions</a>]. Most philosophers of science in the twentieth century have concluded that no logic of creative processes exists and, moreover, that a rational model of discovery is impossible. In short, scientific creative inferences are irrational and there is no “reasoning” to hypotheses. On the other hand, some research in the area of artificial intelligence has shown that methods for discovery could be found that are computationally adequate for rediscovering --- or discovering for the first time --- empirical or theoretical laws and theorems.</p>
</li>
<li><p><a href="https://cognition.princeton.edu/sites/default/files/cognition/files/explanation_abductive_inference.pdf" rel="noopener noreferrer">Explanation and Abductive Inference</a> - <em><strong>The Oxford Handbook of Thinking and Reasoning</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=16126850654692681562" rel="noopener noreferrer">All Versions</a>]. This chapter reviews evidence from cognitive psychology and cognitive development concerning the structure and function of explanations, with a focus on the role of explanations in learning and inference. The findings highlight the value of understanding explanation and abductive inference both as phenomena in their own right and for the insights they provide concerning foundational aspects of human cognition, such as representation, learning, and inference.</p>
</li>
<li><p><a href="https://www.cell.com/AJHG/fulltext/S1364-6613(06)00132-X" rel="noopener noreferrer">Probabilistic models of cognition: Conceptual foundations</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=12857321660837478492" rel="noopener noreferrer">All Versions</a>]. Remarkable progress in the mathematics and computer science of probability has led to a revolution in the scope of probabilistic models. In particular, ‘sophisticated’ probabilistic methods apply to structured relational systems such as graphs and grammars, of immediate relevance to the cognitive sciences. This review outlines progress in this rapidly developing field, which provides a potentially unifying perspective across a wide range of domains and levels of explanation.</p>
</li>
<li><p><a href="https://cognition.princeton.edu/sites/default/files/cognition/files/tics_explanation.pdf" rel="noopener noreferrer">The structure and function of explanations</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=2849189270394400667" rel="noopener noreferrer">All Versions</a>]. Generating and evaluating explanations is spontaneous, ubiquitous and fundamental to our sense of understanding. Recent evidence suggests that in the course of an individual's reasoning, engaging in explanation can have profound effects on the probability assigned to causal claims, on how properties are generalized and on learning. These effects follow from two properties of the structure of explanations: explanations accommodate novel information in the context of prior beliefs, and do so in a way that fosters generalization.</p>
</li>
<li><p><a href="https://scholar.princeton.edu/sites/default/files/cognition/files/explanatory_prefs_tics.pdf" rel="noopener noreferrer">Explanatory Preferences Shape Learning and Inference</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=2040551538203889465" rel="noopener noreferrer">All Versions</a>]. People often learn by seeking explanations, and they assess the viability of hypotheses by considering how well they explain the data. An emerging body of work reveals that both children and adults have strong and systematic intuitions about what constitutes a good explanation, and that these explanatory preferences have a systematic impact on explanation-based processes. In particular, people favor explanations that are simple and broad, with the consequence that engaging in explanation can shape learning and inference by leading people to seek patterns and favor hypotheses that support broad and simple explanations.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0010027715000955" rel="noopener noreferrer">The Role of Explanatory Considerations in Updating</a> - <em><strong>Cognition</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=3089358487428261042" rel="noopener noreferrer">All Versions</a>]. This paper investigates experimentally controversy in philosophy about the connection between explanation and inference, of whether judgments of the explanatory goodness of hypotheses do play a role when people revise their degrees of belief in those hypotheses upon the receipt of new evidence.</p>
</li>
<li><p><a href="https://www.tandfonline.com/doi/full/10.1080/20445911.2016.1230122" rel="noopener noreferrer">Explanation, updating, and accuracy</a> - <em><strong>Journal of Cognitive Psychology</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=967127146748155733" rel="noopener noreferrer">All Versions</a>]. There is evidence that people update their credences partly on the basis of explanatory considerations. Philosophers have recently argued that to minimise the inaccuracy of their credences, people's updates also ought to be partly based on such considerations. However, there are many ways in which explanatory considerations can factor into updating, not all of which minimise inaccuracy. It is an open question whether in their updating, people take explanatory considerations into account in a way that philosophers would deem recommendable. To address this question, the authors re-analyse data from an experiment reported in Douven and Schupbach, “The role of explanatory considerations in updating”.</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/2018-03972-001" rel="noopener noreferrer">Best, second-best, and good-enough explanations: How they matter to reasoning</a> - <em><strong>Journal of Experimental Psychology</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=3067550385175104201" rel="noopener noreferrer">All Versions</a>]. There is a wealth of evidence that people’s reasoning is influenced by explanatory considerations. Three experiments investigate the descriptive adequacy of a precise proposal to be found in the philosophical literature, to wit, that we should infer to the best explanation, provided certain additional conditions are met. The main conslusions are that (a) the quality of an explanation is a good predictor of people’s willingness to accept that explanation, and a better predictor than the prior probability of the explanation, and (b) if more than one possible explanation is given, people are the less willing to infer the best explanation the better they deem the second-best explanation.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S1364661321001790" rel="noopener noreferrer">How explanation guides belief change</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15240531165875981526" rel="noopener noreferrer">All Versions</a>]. Philosophers have argued that people ought to change their graded beliefs via Bayes’ rule. Recent work in psychology indicates that people sometimes violate that rule by attending to explanatory factors. Results from computational modeling suggest that such violations may actually be rational.</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2506_2" rel="noopener noreferrer">Use of current explanations in multicausal abductive reasoning</a> - <em><strong>Cognitive Science</strong></em>, 2001. [<a href="https://scholar.google.com/scholar?cluster=7816050625957759346&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.pnas.org/content/110/42/16766.short" rel="noopener noreferrer">Kinematic mental simulations in abduction and deduction</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=11864820390497230588" rel="noopener noreferrer">All Versions</a>]. This paper presents a theory, and its computer implementation, of how mental simulations underlie the abductions of informal algorithms and deductions from these algorithms. Three experiments tested the theory’s predictions, using an environment of a single railway track and a siding. The results corroborated the use of a kinematic mental model in creating and testing informal algorithms and showed that individuals differ reliably in the ability to carry out these tasks.</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s11229-007-9223-4" rel="noopener noreferrer">Patterns of abduction</a> - <em><strong>Synthese</strong></em>, 2007. [<a href="https://scholar.google.com/scholar?cluster=15230540023076470385&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A categorization for Abduction in the account of pure philosophy.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S1570868314000895" rel="noopener noreferrer">Abduction: A categorical characterization</a> - <em><strong>Journal of Applied Logic</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=17834260152484836885&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.journals.uchicago.edu/doi/abs/10.1086/392744" rel="noopener noreferrer">Defending Abduction</a> - <em><strong>Philosophy of Science</strong></em>, 1999. [<a href="https://scholar.google.com/scholar?cluster=13895790050138832555&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s11229-009-9709-3" rel="noopener noreferrer">On the distinction between Peirce's abduction and Lipton's Inference to the best explanation</a> - <em><strong>Synthese</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?cluster=7865291004729010145&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s11229-019-02337-z" rel="noopener noreferrer">Abduction − the context of discovery + underdetermination = inference to the best explanation</a> - <em><strong>Synthese</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4261649938116694095&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007%2F3-540-45004-1_14" rel="noopener noreferrer">Towards an Architecture for Cognitive Vision Using Qualitative Spatio-temporal Representations and Abduction</a> - <em><strong>Spatial Cognition</strong></em>, 2002. [<a href="https://scholar.google.com/scholar?cluster=8072265283930278310&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s11229-018-1824-6" rel="noopener noreferrer">Abductive inference within a pragmatic framework</a> - <em><strong>Synthese</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=10285954503043361393&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s00354-019-00059-x" rel="noopener noreferrer">Disjunctive Abduction</a> - <em><strong>New Generation Computing</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=6664745483675209831&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00459/full" rel="noopener noreferrer">Probabilistic alternatives to Bayesianism: the case of explanationism</a> - <em><strong>Frontiers in Psychology</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=9016714668469830914&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A non-Bayesian account of Abduction.</p>
</li>
<li><p><a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0010195405620571" rel="noopener noreferrer">A Probabilistic Theory of Abductive Reasoning</a> - <em><strong>ICAART</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=450937566244876051&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A probabilistic perspective for interpreting Abductive Reasoning.</p>
</li>
<li><p><a href="https://www.tandfonline.com/doi/full/10.1080/09528130600558141?scroll=top&amp;needAccess=true" rel="noopener noreferrer">The order effect in human abductive reasoning: an empirical and computational study</a> - <em><strong>Journal of Experimental &amp; Theoretical Artificial Intelligence</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=3803536062463585043&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007%2F978-3-642-15223-8_5" rel="noopener noreferrer">Abduction, Induction, and Analogy</a> - <em><strong>Model-Based Reasoning in Science and Technology</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=14979764682921693390&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The distinctions and relations between Abduction, Induction, and Analogy.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0010027718301094" rel="noopener noreferrer">Remembrance of inferences past: Amortization in human hypothesis generation</a> - <em><strong>Cognition</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=190340622765037472&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>]. A rational account of human hypothesis generation.</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01142.x" rel="noopener noreferrer">The AHA! Experience: Creativity Through Emergent Binding in Neural Networks</a> - <em><strong>Cognitive Science</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=10006889101167052798&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S2352154620300851" rel="noopener noreferrer">Explanation-seeking curiosity in childhood</a> - <em><strong>Current Opinion in Behavioral Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=4167956555501133663&amp;hl=en&amp;as_sdt=2005" rel="noopener noreferrer">All Versions</a>]. A piece of developmental pshchological evidence for Abduction in young children.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.14267" rel="noopener noreferrer">A Grammar of Hypotheses for Visualization, Data, and Analysis</a> - 2022. [<a href="https://scholar.google.com/scholar?cluster=10321469321980973246" rel="noopener noreferrer">All Versions</a>]. This work presents a grammar for expressing hypotheses in visual data analysis to formalize the previously abstract notion of "analysis tasks." Through the lens of this grammar, the authors lay the groundwork for how a user's data analysis questions can be operationalized and automated as a set of hypotheses (a hypothesis space). The authors demonstrate that the grammar-based approach for analysis tasks can provide a systematic method towards unifying three disparate spaces in visualization research: the hypotheses a dataset can express (a data hypothesis space), the hypotheses a user would like to refine or verify through analysis (an analysis hypothesis space), and the hypotheses a visualization design is capable of supporting (a visualization hypothesis space). The authors illustrate how the formalization of these three spaces can inform future research in visualization evaluation, knowledge elicitation, analytic provenance, and visualization recommendation by using a shared language for hypotheses. Finally, the authors compare the proposed grammar-based approach with existing visual analysis models and discuss the potential of a new hypothesis-driven theory of visual analytics.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="scientific-discovery"><a class="anchor" aria-hidden="true" tabindex="-1" href="#scientific-discovery"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Scientific Discovery</h4><ul>
<li><p><a href="https://plato.stanford.edu/entries/scientific-discovery/" rel="noopener noreferrer">Scientific Discovery</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Scientific Discovery, the process or product of successful scientific inquiry, sometimes an Abduction-like (Explanation) thinking pattern.</p>
</li>
<li><p><a href="https://hk1lib.org/book/2241843/c5d7b3?id=2241843&amp;secret=c5d7b3" rel="noopener noreferrer">Models of Discovery: And Other Topics in the Methods of Science</a> - <em><strong>Springer</strong></em>, 1977. [<a href="https://scholar.google.com/scholar?cluster=9932701864897299105&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original book on search as scientific thinking.</p>
</li>
<li><p><a href="https://hk1lib.org/book/970300/6b0ff7?id=970300&amp;secret=6b0ff7" rel="noopener noreferrer">Scientific discovery: Computational explorations of the creative processes</a> - <em><strong>MIT Press</strong></em>, 1987. [<a href="https://scholar.google.com/scholar?cluster=11327000316248254911" rel="noopener noreferrer">All Versions</a>]. The book is divided into four parts. Part I introduces the subject of discovery, defines the scope of our work, and discusses some of the issues that have surrounded and still surround our topic. Parts II and III contain the main body of our results, largely in the form of accounts of the performance of computer programs that simulate human thought processes to make scientific discoveries. Part II is devoted largely to the processes for inducing quantitative theories from data. Part III is devoted mainly to the processes for inducing qualitative descriptive and structural theories from data. In Part IV, on the basis of our experience, we discuss at a lower level of precision how the programs described in the preceding chapters could be combined into a single, more general discovery system, and we describe a wide range of the other component processes that enter into scientific discovery.</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/2000-03968-000" rel="noopener noreferrer">Exploring science: The cognition and development of discovery processes</a> - <em><strong>MIT Press</strong></em>, 2000. [<a href="https://scholar.google.com/scholar?cluster=13091264356550286420" rel="noopener noreferrer">All Versions</a>]. In this book, D. Klahr sets out to describe the cognitive and developmental processes that have enabled scientists to make the discoveries that comprise the body of information we call "scientific knowledge." Over the past decade, Klahr and his colleagues have conducted laboratory experiments in which they create discovery contexts, computer-based environments, to evoke the kind of thinking characteristic of scientific discovery in the "real world." In attempting to solve the problems posed by the discovery tasks, experiment participants (from preschoolers to university students) use many of the same higher-order cognitive processes used by practicing scientists. Through his work, Klahr integrates two disparate approaches–the content-based approach and the process-based approach– to present a comprehensive model of the psychology of scientific discovery.</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1201_1" rel="noopener noreferrer">Dual Space Search During Scientific Reasoning</a> - <em><strong>Cognitive Science</strong></em>, 1988. [<a href="https://scholar.google.com/scholar?cluster=17542852673494089523&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on the dual space search as scientific thinking theory.</p>
</li>
<li><p><a href="https://escholarship.org/uc/item/94n547fj" rel="noopener noreferrer">Complexity Management in a Discovery Task</a> - <em><strong>CogSci'92</strong></em>, 1992. [<a href="https://scholar.google.com/scholar?cluster=18138712608977258974" rel="noopener noreferrer">All Versions</a>]. Previous psychological research about scientific discovery has often focused on subjects' heuristics for discovering simple concepts with one relevant dimension or a few relevant dimensions with simple two-way interactions. This paper presents results from an experiment in which subjects had to discover a concept involving complex three-way interactions on a multi-valued output by running experiments in a computerized microworld. Twenty-two CMU undergraduates attempted the task, of which sixteen succeeded, in an average of 85 minutes. The analyses focus on three strategies used to regulate task complexity. First, subjects preferred depth-first to breadth-first search, with successful subjects regulating the number of features varied from experiment to experiment most effectively. Second, subjects systematically regulated the length of their experiments. Third, a new explicit search heuristic (Put Upon Stack Heuristic) used by successful subjects is described.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S1071581996900324" rel="noopener noreferrer">A dual-space model of iteratively deepening exploratory learning</a> - <em><strong>International Journal of Human-Computer Studies</strong></em>, 1996. [<a href="https://scholar.google.com/scholar?cluster=17337189265334825678" rel="noopener noreferrer">All Versions</a>]. This paper describes a cognitive model of exploratory learning, which covers both trial-and-error and instruction-taking activities. The model, implemented in Soar, is grounded in empirical data of subjects in a task-oriented, trial-and-error exploratory learning situation. A key empirical finding reflected in the model is the repeated scanning of a subset of the available menu items, with increased attention to items on each successive scan. This is explained in terms of dual search spaces, the external interface and the user's internal knowledge, both of which must be tentatively explored with attention to changing costs and benefits.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0010028583710030" rel="noopener noreferrer">Heuristics for Scientific Experimentation: A Developmental Study</a> - <em><strong>Cognitive Psychology</strong></em>, 1993. [<a href="https://scholar.google.com/scholar?cluster=2469515962071844494&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>]. A piece of evidence on children have basic scientific thinking skills.</p>
</li>
<li><p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.645.248&amp;rep=rep1&amp;type=pdf" rel="noopener noreferrer">A 4-Space Model of Scientific Discovery</a> - <em><strong>CogSci'95</strong></em>, 1995. [<a href="https://scholar.google.com/scholar?cluster=1063157789682040473&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>]. Extending the dual space search.</p>
</li>
<li><p><a href="https://link.springer.com/article/10.3758/BF03201090" rel="noopener noreferrer">When to trust the data: Further investigations of system error in a scientific reasoning task</a> - <em><strong>Memory &amp; Cognition</strong></em>, 1996. [<a href="https://scholar.google.com/scholar?cluster=3131191372086488656" rel="noopener noreferrer">All Versions</a>]. When evaluating experimental evidence, how do people deal with the possibility that some of the feedback is erroneous? The potential for error means that evidence evaluation must include decisions about when to “trust the data.” This paper presents two studies that focus on subjects’ responses to erroneous feedback in a hypothesis testing situation—a variant of Wason’s (1960) 2–4–6 rule discovery task in which some feedback was subject tosystem error: “hits” were reported as “misses” and vice versa. Results show that, in contrast to previous research, people are equally adept at identifying false negatives and false positives; further, successful subjects were less likely to use a positive test strategy (Klayman &amp; Ha, 1987) than were unsuccessful subjects. Finally, although others have found that generating possible hypotheses prior to experimentation increases success and task efficiency, such a manipulation did little to mitigate the effects of system error.</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/1987-20689-001" rel="noopener noreferrer">Confirmation, disconfirmation, and information in hypothesis testing</a> - <em><strong>Psychological Review</strong></em>, 1987. [<a href="https://scholar.google.com/scholar?cluster=1954141597807453515&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A psychological account on hypothesis testing.</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/2010-22980-001" rel="noopener noreferrer">Hypothesis generation, sparse categories, and the positive test strategy</a> - <em><strong>Psychological Review</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?cluster=4329636480235863472&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/1990-03504-001" rel="noopener noreferrer">Children and adults as intuitive scientists</a> - <em><strong>Psychological Review</strong></em>, 1989. [<a href="https://scholar.google.com/scholar?cluster=9577945454476127070&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>]. A perspective against search as scientific thinking.</p>
</li>
<li><p><a href="https://link.springer.com/content/pdf/10.1007/s11229-019-02127-7.pdf" rel="noopener noreferrer">Abduction and styles of scientific thinking</a> - <em><strong>Synthese</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=9336871656706514469&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A computational philosophy account connecting Abduction and scientific thinking.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="rationalization"><a class="anchor" aria-hidden="true" tabindex="-1" href="#rationalization"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Rationalization</h4><ul>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0885201414000744" rel="noopener noreferrer">Imagination and the generation of new ideas</a> - <em><strong>Cognitive Development</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=16920774374067505248" rel="noopener noreferrer">All Versions</a>]. A variety of theories have been put forth to explain the function of imagination, most notably that imagination engages and develops children's theory of mind and counterfactual reasoning. This work proposes that a primary role for imagination is as a cognitive mechanism for efficiently generating new ideas without observing new evidence. Learners must generate hypotheses before they can assess the truth of these hypotheses. Given infinite possibilities, how do learners constrain the process of hypothesis generation? The authors suggest that learners represent abstract criteria for the solution to a problem and generate solutions that, if true, would solve the problem. As a preliminary test of this idea, the authors show that, in the absence of any fact of the matter (i.e., when neither prior knowledge nor statistical data distinguishes competing hypotheses), 4–6-year-olds (mean: 63 months) systematically converge on solutions to problems, consistent with an ability to imagine the abstract properties of causal problems and their solutions.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S1364661319302311" rel="noopener noreferrer">How We Know What Not To Think</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=13106919756521743226" rel="noopener noreferrer">All Versions</a>]. Humans often represent and reason about unrealized possible actions---the vast infinity of things that were not (or have not yet been) chosen. This capacity is central to the most impressive of human abilities: causal reasoning, planning, linguistic communication, moral judgment, etc. Nevertheless, how do we select possible actions that are worth considering from the infinity of unrealized actions that are better left ignored? This work reviews research across the cognitive sciences, and find that the possible actions considered by default are those that are both likely to occur and generally valuable. This paper then offers a unified theory of why. The authors propose that (i) across diverse cognitive tasks, the possible actions we consider are biased towards those of general practical utility, and (ii) a plausible primary function for this mechanism resides in decision making.</p>
</li>
<li><p><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/rationalization-is-rational/2A13B99ED09BD802C0924D3681FEC55B" rel="noopener noreferrer">Rationalization is rational</a> - <em><strong>Behavioral and Brain Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5165464589274056844" rel="noopener noreferrer">All Versions</a>]. [<a href="https://bpb-us-e1.wpmucdn.com/websites.harvard.edu/dist/0/59/files/2022/03/rationalization_is_rational.pdf" rel="noopener noreferrer">Preprint</a>]. Rationalization occurs when a person has performed an action and then concocts the beliefs and desires that would have made it rational. Then, people often adjust their own beliefs and desires to match the concocted ones. While many studies demonstrate rationalization, and a few theories describe its underlying cognitive mechanisms, we have little understanding of its function. Why is the mind designed to construct post hoc rationalizations of its behavior, and then to adopt them? This may accomplish an important task: transferring information between the different kinds of processes and representations that influence our behavior. Human decision making does not rely on a single process; it is influenced by reason, habit, instinct, norms, and so on. Several of these influences are not organized according to rational choice (i.e., computing and maximizing expected value). Rationalization extracts implicit information – true beliefs and useful desires – from the influence of these non-rational systems on behavior.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S1364661321001480" rel="noopener noreferrer">Rationalizing constraints on the capacity for cognitive control</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=13060297961922073063" rel="noopener noreferrer">All Versions</a>]. Humans are remarkably limited in: (i) how many control-dependent tasks they can execute simultaneously, and (ii) how intensely they can focus on a single task. These limitations are universal assumptions of most theories of cognition. Yet, a rationale for why humans are subject to these constraints remains elusive. This feature review draws on recent insights from psychology, neuroscience, and machine learning, to suggest that constraints on cognitive control may result from a rational adaptation to fundamental, computational dilemmas in neural architectures. The reviewed literature implies that limitations in multitasking may result from a trade-off between learning efficacy and processing efficiency and that limitations in the intensity of commitment to a single task may reflect a trade-off between cognitive stability and flexibility.</p>
</li>
<li><p><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/why-imaginary-worlds/CA2AB4B1E1EDD8FE965E6DDB4A047B35" rel="noopener noreferrer">Why Imaginary Worlds? The psychological foundations and cultural evolution of fictions with imaginary worlds</a> - <em><strong>Behavioral and Brain Sciences</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=16985691366494688837" rel="noopener noreferrer">All Versions</a>]. Imaginary worlds are extremely successful. The most popular fictions produced in the last few decades contain such a fictional world. They can be found in all fictional media, from novels (e.g., Lord of The Rings and Harry Potter) to films (e.g., Star Wars and Avatar), video games (e.g., The Legend of Zelda and Final Fantasy), graphic novels (e.g., One Piece and Naruto), and TV series (e.g., Star Trek and Game of Thrones), and they date as far back as ancient literature (e.g., the Cyclops Islands in The Odyssey, 850 BCE). Why such a success? Why so much attention devoted to non-existent worlds? In this paper, the authors propose that imaginary worlds co-opt our preferences for exploration, which have evolved in humans and nonhuman animals alike, to propel individuals toward new environments and new sources of reward. Humans would find imaginary worlds very attractive for the very same reasons, and under the same circumstances, as they are lured by unfamiliar environments in real life. After reviewing research on exploratory preferences in behavioral ecology, environmental esthetics, neuroscience, and evolutionary and developmental psychology, the authors focus on the sources of their variability across time and space, which they argue can account for the variability of the cultural preference for imaginary worlds. This hypothesis can, therefore, explain the way imaginary worlds evolved culturally, their shape and content, their recent striking success, and their distribution across time and populations.</p>
</li>
<li><p><a href="https://escholarship.org/uc/item/5f64z7d7" rel="noopener noreferrer">Coalescing the Vapors of Human Experience into a Viable and Meaningful Comprehension</a> - <em><strong>CogSci'16</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=5460385008324352958" rel="noopener noreferrer">All Versions</a>]. Models of concept learning and theory acquisition often invoke a stochastic search process, in which learners generate hypotheses through some structured random process and thenevaluate them on some data measuring their quality or value. To be successful within a reasonable time-frame, these models need ways of generating good candidate hypotheses evenbefore the data are considered. Schulz (2012a) has proposed that studying the origins of new ideas in more everyday contexts, such as how we think up new names for things, can provide insight into the cognitive processes that generate good hypotheses for learning. We propose a simple generative model for how people might draw on their experience to propose new names in everyday domains such as pub names or action movies, and show that it captures surprisingly well the names that people actually imagine. We discuss the role for an analogous hypothesis-generation mechanism in enabling and constraining causal theory learning.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="applications-in-ai"><a class="anchor" aria-hidden="true" tabindex="-1" href="#applications-in-ai"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Applications in AI</h4><ul>
<li><p><a href="https://www.nature.com/articles/nature02236" rel="noopener noreferrer">Functional genomic hypothesis generation and experimentation by a robot scientist</a> - <em><strong>Nature</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=17461972625475533182" rel="noopener noreferrer">All Versions</a>]. This paper describes a physically implemented robotic system that applies techniques from artificial intelligence to carry out cycles of scientific experimentation. The system automatically originates hypotheses to explain observations, devises experiments to test these hypotheses, physically runs the experiments using a laboratory robot, interprets the results to falsify hypotheses inconsistent with the data, and then repeats the cycle. The system is applied to the determination of gene function using deletion mutants of yeast (Saccharomyces cerevisiae) and auxotrophic growth experiments. The authors built and tested a detailed logical model (involving genes, proteins and metabolites) of the aromatic amino acid synthesis pathway.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/0004370293900154?via%3Dihub" rel="noopener noreferrer">Interpretation as abduction</a> - <em><strong>Artificial Intelligence</strong></em>, 1993. [<a href="https://scholar.google.com/scholar?cluster=12658433318211361322" rel="noopener noreferrer">All Versions</a>]. Abduction is inference to the best explanation. The authors have developed an approach to abductive inference, called “weighted abduction”, that has resulted in a significant simplification of how the problem of interpreting texts is conceptualized. The interpretation of a text is the minimal explanation of why the text would be true. More precisely, to interpret a text, one must prove the logical form of the text from what is already mutually known, allowing for coercions, merging redundancies where possible, and making assumptions where necessary. It is shown how such “local pragmatics” problems as reference resolution, the interpretation of compound nominals, the resolution of syntactic ambiguity and metonymy, and schema recognition can be solved in this manner. Moreover, this approach of “interpretation as abduction” can be combined with the older view of “parsing as deduction” to produce an elegant and thorough integration of syntax, semantics, and pragmatics, one that spans the range of linguistic phenomena from phonology to discourse structure.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/000437029390061F?via%3Dihub" rel="noopener noreferrer">Probabilistic Horn abduction and Bayesian networks</a> - <em><strong>Artificial Intelligence</strong></em>, 1993. [<a href="https://scholar.google.com/scholar?cluster=7728248035489349629" rel="noopener noreferrer">All Versions</a>]. This paper presents a simple framework for Horn-clause abduction, with probabilities associated with hypotheses. The framework incorporates assumptions about the rule base and independence assumptions amongst hypotheses. It is shown how any probabilistic knowledge representable in a discrete Bayesian belief network can be represented in this framework. The main contribution is in finding a relationship between logical and probabilistic notions of evidential reasoning. This provides a useful representation language in its own right, providing a compromise between heuristic and epistemic adequacy.</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-3-540-39879-0_6" rel="noopener noreferrer">Abductive Inference in Bayesian Networks: A Review</a> - <em><strong>Advances in Bayesian Networks</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=8502276402734843212" rel="noopener noreferrer">All Versions</a>]. The goal of this paper is to serve as a survey for the problem of abductive inference (or belief revision) in Bayesian networks. Thus, the problem is introduced in its two variants: total abduction (or MPE) and partial abduction (or MAP) . Also, the problem is formulated in its general case, that is, looking for the K best explanations. Then, a (non exhaustive) review of exact and approximate algorithms for dealing with both abductive inference problems is carried out. Finally, the authors collect the main complexity results appeared in the literature for both problems (MPE and MAP).</p>
</li>
<li><p><a href="https://academic.oup.com/logcom/article-abstract/2/6/719/942121" rel="noopener noreferrer">Abductive Logic Programming</a> - <em><strong>Journal of Logic Computation</strong></em>, 1992. [<a href="https://scholar.google.com/scholar?cluster=18119357517656745518" rel="noopener noreferrer">All Versions</a>]. This paper is a survey and critical overview of recent work on the extension of logic programming to perform abductive reasoning (abductive logic programming). The authors outline the general framework of abduction and its applications to knowledge assimilation and default reasoning; and they introduce an argumentation-theoretic approach to the use of abduction as an interpretation for negation as failure.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0743106699000758" rel="noopener noreferrer">ACLP: Abductive Constraint Logic Programming</a> - <em><strong>The Journal of Logic Programming</strong></em>, 1999. [<a href="https://scholar.google.com/scholar?cluster=14319574550421192429" rel="noopener noreferrer">All Versions</a>]. This paper presents the framework of Abductive Constraint Logic Programming (ACLP), which integrates Abductive Logic Programming (ALP) and Constraint Logic Programming (CLP). In ACLP, the task of abduction is supported and enhanced by its non-trivial integration with constraint solving. This integration of constraint solving into abductive reasoning facilitates a general form of constructive abduction and enables the application of abduction to computationally demanding problems. The paper studies the formal declarative and operational semantics of the ACLP framework together with its application to various problems.</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007/3-540-45628-7_16" rel="noopener noreferrer">Abduction in Logic Programming</a> - <em><strong>Computational Logic</strong></em>, 2002. [<a href="https://scholar.google.com/scholar?cluster=902643678163312237" rel="noopener noreferrer">All Versions</a>]. [<a href="https://web.stanford.edu/class/cs227/Readings/Abudction%20in%20LP.pdf" rel="noopener noreferrer">Preprint</a>]. Abduction in Logic Programming started in the late 80s, early 90s, in an attempt to extend logic programming into a framework suitable for a variety of problems in Artificial Intelligence and other areas of Computer Science. This paper aims to chart out the main developments of the field over the last ten years and to take a critical view of these developments from several perspectives: logical, epistemological, computational and suitability to application. The paper attempts to expose some of the challenges and prospects for the further development of the field.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.5555/2283696.2283887" rel="noopener noreferrer">Bayesian Abductive Logic Programs: A Probabilistic Logic for Abductive Reasoning</a> - <em><strong>IJCAI'11</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?cluster=4453424083730209198" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.cs.utexas.edu/~ml/papers/raghavan.starai10.pdf" rel="noopener noreferrer">Preprint</a>]. This work introduces Bayesian Abductive Logic Programs (BALP), a probabilistic logic that adapts Bayesian Logic Programs (BLPs) for abductive reasoning. Like BLPs, BALPs also combine first-order logic and Bayes nets. However, unlike BLPs, which use deduction to construct Bayes nets, BALPs employ logical abduction. As a result, BALPs are more suited for problems like plan/activity recognition that require abductive reasoning.</p>
</li>
<li><p><a href="https://www.cs.utexas.edu/~ml/papers/raghavan.ecml11.pdf" rel="noopener noreferrer">Abductive Plan Recognition by Extending Bayesian Logic Programs</a> - <em><strong>ECML'11</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?cluster=7276511797197017483" rel="noopener noreferrer">All Versions</a>]. Plan recognition is the task of predicting an agent’s top-level plans based on its observed actions. It is an abductive reasoning task that involves inferring cause from effect. Most existing approaches to plan recognition use either first-order logic or probabilistic graphical models. While the former cannot handle uncertainty, the latter cannot handle structured representations. In order to overcome these limitations, this work develops an approach to plan recognition using Bayesian Logic Programs (BLPs), which combine first-order logic and Bayesian networks. Since BLPs employ logical deduction to construct the networks, they cannot be used effectively for plan recognition. Therefore, the authors extend BLPs to use logical abduction to construct Bayesian networks and call the resulting model Bayesian Abductive Logic Programs (BALPs). The authors learn the parameters in BALPs using the Expectation Maximization algorithm adapted for BLPs. Finally, the authors present an experimental evaluation of BALPs on three benchmark data sets and compare its performance with the state-of-the-art for plan recognition.</p>
</li>
<li><p><a href="https://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/6624/6619" rel="noopener noreferrer">An Approach to Abductive Reasoning in Equational Logic</a> - <em><strong>IJCAI'13</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=686895264429811190" rel="noopener noreferrer">All Versions</a>]. Abduction has been extensively studied in propositional logic because of its many applications in artificial intelligence. However, its intrinsic complexity has been a limitation to the implementation of abductive reasoning tools in more expressive logics. The authors have devised such a tool in ground flat equational logic, in which literals are equations or disequations between constants. The tool is based on the computation of prime implicates. It uses a relaxed paramodulation calculus, designed to generate all prime implicates of a formula, together with a carefully defined data structure storing the implicates and able to efficiently detect, and remove, redundancies.</p>
</li>
<li><p><a href="https://ojs.aaai.org//index.php/AAAI/article/view/3964" rel="noopener noreferrer">Abduction-Based Explanations for Machine Learning Models</a> - <em><strong>AAAI'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7355960657107994022" rel="noopener noreferrer">All Versions</a>]. The growing range of applications of Machine Learning (ML) in a multitude of settings motivates the ability of computing small explanations for predictions made. Small explanations are generally accepted as easier for human decision makers to understand. Most earlier work on computing explanations is based on heuristic approaches, providing no guarantees of quality, in terms of how close such solutions are from cardinality- or subset-minimal explanations. This paper develops a constraint-agnostic solution for computing explanations for any ML model. The proposed solution exploits abductive reasoning, and imposes the requirement that the ML model can be represented as sets of constraints using some target constraint reasoning system for which the decision problem can be answered with some oracle. The experimental results, obtained on well-known datasets, validate the scalability of the proposed approach as well as the quality of the computed solutions.</p>
</li>
<li><p><a href="https://www.ijcai.org/proceedings/2021/0424.pdf" rel="noopener noreferrer">Probabilistic Sufficient Explanations</a> - <em><strong>IJCAI'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=1874102360688341104" rel="noopener noreferrer">All Versions</a>]. Understanding the behavior of learned classifiers is an important task, and various black-box explanations, logical reasoning approaches, and model-specific methods have been proposed. This paper introduces probabilistic sufficient explanations, which formulate explaining an instance of classification as choosing the "simplest" subset of features such that only observing those features is "sufficient" to explain the classification. That is, sufficient to give us strong probabilistic guarantees that the model will behave similarly when all features are observed under the data distribution. In addition, the authors leverage tractable probabilistic reasoning tools such as probabilistic circuits and expected predictions to design a scalable algorithm for finding the desired explanations while keeping the guarantees intact. The experiments demonstrate the effectiveness of the algorithm in finding sufficient explanations, and showcase its advantages compared to Anchors and logical explanations.</p>
</li>
<li><p><a href="https://www.aclweb.org/anthology/H91-1024.pdf" rel="noopener noreferrer">Machine Translation Using Abductive Inference</a> - <em><strong>COLING</strong></em>, 1990. [<a href="https://scholar.google.com/scholar?cluster=15275163177548183539" rel="noopener noreferrer">All Versions</a>]. Many existing approaches to machine translation take for granted that the information presented in the output is found somewhere in the input, and, moreover, that such information should be expressed at a single representational level, say, in terms of the parse trees or of "semantic" assertions. Languages, however, not only express the equivalent information by drastically different linguistic means, but also often disagree in what distinctions should be expressed linguistically at all. For example, in translating from Japanese to English, it is often necessary to supply determiners for noun phrases, and this in general cannot be done without deep understanding of the source text. Similarly, in translating from English to Japanese, politeness considerations, which in English are implicit in the social situation and explicit in very diffuse ways in, for example, the heavy use of hypotheticals, must be realized grammatically in Japanese. Machine translation therefore requires that the appropriate inferences be drawn and that the text be interpreted to some depth. Recently, an elegant approach to inference in discourse interpretation has been developed at a number of sites, all based on the notion of abduction, and the authors have begun to explore its potential application to machine translation. The authors argue that this approach provides the possibility of deep reasoning and of mapping between the languages at a variety of levels.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2105.07758" rel="noopener noreferrer">Automated Biodesign Engineering by Abductive Meta-Interpretive Learning</a> - <em><strong>AAAI Spring Symposium Series 2021 on Artificial Intelligence for Synthetic Biology</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=543730388062329581" rel="noopener noreferrer">All Versions</a>]. This work proposes an automated biodesign engineering framework empowered by Abductive Meta-Interpretive Learning (MetaAbd), a novel machine learning approach that combines symbolic and sub-symbolic machine learning, to further enhance the design-build-test-learn cycle by enabling the learning machine to 1) exploit domain knowledge and learn human-interpretable models that are expressed by formal languages such as first-order logic; 2) simultaneously optimise the structure and parameters of the models to make accurate numerical predictions; 3) reduce the cost of experiments and effort on data annotation by actively generating hypotheses and examples.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2308.12740" rel="noopener noreferrer">Human Comprehensible Active Learning of Genome-Scale Metabolic Networks</a> - <em><strong>AAAI Spring Symposium Series 2023 on Computational Scientific Discovery</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=10875437066608527790" rel="noopener noreferrer">All Versions</a>]. [<a href="http://cogsys.org/symposium/discovery-2023/abstracts/Abstract_3169.pdf" rel="noopener noreferrer">Extended Abstract</a>]. [<a href="http://cogsys.org/symposium/discovery-2023/talks/Ai.pdf" rel="noopener noreferrer">Slides</a>]. This work introduces a novel machine learning framework ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive logical reasoning and actively learns from training examples. The ILP-iML1515 framework 1) allows high-throughput simulations and 2) actively selects experiments that reduce the experimental cost of learning gene functions in comparison to randomly selected experiments.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="bayesian-modeling"><a class="anchor" aria-hidden="true" tabindex="-1" href="#bayesian-modeling"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Bayesian Modeling</h3><h4 id="bayesian-induction"><a class="anchor" aria-hidden="true" tabindex="-1" href="#bayesian-induction"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Bayesian Induction</h4><ul>
<li><p><a href="https://plato.stanford.edu/entries/epistemology-bayesian/" rel="noopener noreferrer">Bayesian Epistemology</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on the nature of uncertainty modeling in Bayesian Epistemology.</p>
</li>
<li><p><a href="https://www.nature.com/articles/nature14541" rel="noopener noreferrer">Probabilistic machine learning and artificial intelligence</a> - <em><strong>Nature</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=1783282361269717744" rel="noopener noreferrer">All Versions</a>]. Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.</p>
</li>
<li><p><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/generalization-similarity-and-bayesian-inference/595CAA321C9C56270C624057021DE77A" rel="noopener noreferrer">Generalization, similarity, and Bayesian inference</a> - <em><strong>Behavioral and Brain Sciences</strong></em>, 2001. [<a href="https://scholar.google.com/scholar?cluster=14074987155133342565" rel="noopener noreferrer">All Versions</a>]. [<a href="http://web.mit.edu/cocosci/archive/Papers/tenenbaum_griffiths01.pdf" rel="noopener noreferrer">Preprint</a>]. Shepard has argued that a universal law should govern generalization across different domains of perception and cognition, as well as across organisms from different species or even different planets. Starting with some basic assumptions about natural kinds, he derived an exponential decay function as the form of the universal generalization gradient, which accords strikingly well with a wide range of empirical data. However, his original formulation applied only to the ideal case of generalization from a single encountered stimulus to a single novel stimulus, and for stimuli that can be represented as points in a continuous metric psychological space. The authors recast Shepard's theory in a more general Bayesian framework and show how this naturally extends his approach to the more realistic situation of generalizing from multiple consequential stimuli with arbitrary representational structure. This framework also subsumes a version of Tversky's set-theoretic model of similarity, which is conventionally thought of as the primary alternative to Shepard's continuous metric space model of similarity and generalization.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper_files/paper/1998/hash/d010396ca8abf6ead8cacc2c2f2f26c7-Abstract.html" rel="noopener noreferrer">Bayesian modeling of human concept learning</a> - <em><strong>NeurIPS'98</strong></em>, 1998. [<a href="https://scholar.google.com/scholar?cluster=3772493362518191863" rel="noopener noreferrer">All Versions</a>]. [<a href="http://web.mit.edu/cocosci/archive/Papers/bayes.pdf" rel="noopener noreferrer">Preprint</a>]. This work considers the problem of learning concepts from small numbers of positive examples, a feat which humans perform routinely but which computers are rarely capable of. Bridging machine learning and cognitive science perspectives, this work presents both theoretical analysis and an empirical study with human subjects for the simple task oflearning concepts corresponding to axis-aligned rectangles in a multidimensional feature space. Existing learning models, when applied to this task, cannot explain how subjects generalize from only a few examples of the concept. The author proposes a principled Bayesian model based on the assumption that the examples are a random sample from the concept to be learned. The model gives precise fits to human behavior on this simple task and provides qualitati ve insights into more complex, realistic cases of concept learning.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/1999/hash/86d7c8a08b4aaa1bc7c599473f5dddda-Abstract.html" rel="noopener noreferrer">Rules and Similarity in Concept Learning</a> - <em><strong>NeurIPS'99</strong></em>, 1999. [<a href="https://scholar.google.com/scholar?cluster=10968021160883668417" rel="noopener noreferrer">All Versions</a>]. [<a href="http://web.mit.edu/cocosci/archive/Papers/nips99preprint.pdf" rel="noopener noreferrer">Preprint</a>]. This paper argues that two apparently distinct modes of generalizing concepts - abstracting rules and computing similarity to exemplars - should both be seen as special cases of a more general Bayesian learning framework. Bayes explains the specific workings of these two modes - which rules are abstracted, how similarity is measured - as well as why generalization should appear rule- or similarity-based in different situations. This analysis also suggests why the rules/similarity distinction, even if not computationally fundamental, may still be useful at the algorithmic level as part of a principled approximation to fully Bayesian learning.</p>
</li>
<li><p><a href="https://www.cell.com/AJHG/fulltext/S1364-6613(06)00134-3" rel="noopener noreferrer">Theory-based Bayesian models of inductive learning and reasoning</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=6741344960992898446" rel="noopener noreferrer">All Versions</a>]. [<a href="http://www.charleskemp.com/papers/TenenbaumGK06.pdf" rel="noopener noreferrer">Preprint</a>]. Inductive inference allows humans to make powerful generalizations from sparse data when learning about word meanings, unobserved properties, causal relationships, and many other aspects of the world. Traditional accounts of induction emphasize either the power of statistical learning, or the importance of strong constraints from structured domain knowledge, intuitive theories or schemas. This paper argues that both components are necessary to explain the nature, use and acquisition of human knowledge, and the authors introduce a theory-based Bayesian framework for modeling inductive learning and reasoning as statistical inferences over structured knowledge representations.</p>
</li>
<li><p><a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2F0033-295X.114.2.245" rel="noopener noreferrer">Word learning as Bayesian inference</a> - <em><strong>Psychological Review</strong></em>, 2007. [<a href="https://scholar.google.com/scholar?cluster=5476233692839102256" rel="noopener noreferrer">All Versions</a>]. [<a href="https://tallinzen.net/media/readings/xu_tenenbaum_2007.pdf" rel="noopener noreferrer">Preprint</a>]. The authors present a Bayesian framework for understanding how adults and children learn the meanings of words. The theory explains how learners can generalize meaningfully from just one or a few positive examples of a novel word's referents, by making rational inductive inferences that integrate prior knowledge about plausible word meanings with the statistical structure of the observed examples. The theory addresses shortcomings of the two best known approaches to modeling word learning, based on deductive hypothesis elimination and associative learning. Three experiments with adults and children test the Bayesian account's predictions in the context of learning words for object categories at multiple levels of a taxonomic hierarchy. Results provide strong support for the Bayesian account over competing accounts, in terms of both quantitative model fits and the ability to explain important qualitative phenomena. Several extensions of the basic theory are discussed, illustrating the broader potential for Bayesian models of word learning.</p>
</li>
<li><p><a href="https://www.science.org/doi/full/10.1126/science.1192788" rel="noopener noreferrer">How to Grow a Mind: Statistics, Structure, and Abstraction</a> - <em><strong>Science</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?cluster=2667398573353002097" rel="noopener noreferrer">All Versions</a>]. [<a href="https://cocosci.princeton.edu/tom/papers/growamind.pdf" rel="noopener noreferrer">Preprint</a>]. This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?</p>
</li>
<li><p><a href="https://www.science.org/doi/full/10.1126/science.aab3050" rel="noopener noreferrer">Human-level concept learning through probabilistic program induction</a> - <em><strong>Science</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=11844685101409624506" rel="noopener noreferrer">All Versions</a>]. [<a href="https://ai6034.mit.edu/wiki/images/LakeDec2015.pdf" rel="noopener noreferrer">Preprint</a>]. [<a href="https://cims.nyu.edu/~brenden/LakeEtAl2015Science_supp.pdf" rel="noopener noreferrer">Supplementary Material</a>]. People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. This work presents a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.</p>
</li>
<li><p><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993" rel="noopener noreferrer">Building Machines That Learn and Think Like People</a> - <em><strong>Behavioral and Brain Sciences</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=8504723689348856287" rel="noopener noreferrer">All Versions</a>]. [<a href="https://leylaroksancaglar.github.io/Caglar_Hanson_2017.pdf" rel="noopener noreferrer">Preprint</a>]. Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. The authors review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, the authors argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. The authors suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41562-024-01991-9" rel="noopener noreferrer">Building machines that learn and think with people</a> - <em><strong>Nature Human Behavior</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=4420595706578245444" rel="noopener noreferrer">All Versions</a>]. [<a href="https://arxiv.org/abs/2408.03943" rel="noopener noreferrer">Preprint</a>]. This perspective shows how the science of collaborative cognition can be put to work to engineer systems that really can be called ‘thought partners’, systems built to meet humans' expectations and complement humans' limitations. The authors lay out several modes of collaborative thought in which humans and artificial intelligence thought partners can engage, and they propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, this work motivates an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the constructed partners actively build and reason over models of the human and world.</p>
</li>
<li><p><a href="http://web.mit.edu/cocosci/archive/Papers/cogsci01_final.pdf" rel="noopener noreferrer">The rational basis of representativeness</a> - <em><strong>CogSci'01</strong></em>, 2001. [<a href="https://scholar.google.com/scholar?cluster=11464039134248091466&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2011/hash/2c89109d42178de8a367c0228f169bf8-Abstract.html" rel="noopener noreferrer">Testing a Bayesian Measure of Representativeness Using a Large Image Database</a> - <em><strong>NeurIPS'11</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?cluster=8576570792794301292&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://cocosci.princeton.edu/tom/papers/abbott_cogsci2012_wordnet.pdf" rel="noopener noreferrer">Constructing a hypothesis space from the Web for large-scale Bayesian word learning</a> - <em><strong>CogSci'12</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=9266416266046851766&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://escholarship.org/content/qt1md755ng/qt1md755ng.pdf" rel="noopener noreferrer">Modeling rules and similarity in colexification</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=11578380234126546169" rel="noopener noreferrer">All Versions</a>]. Rule- and similarity-based generalization in colexification.</p>
</li>
<li><p><a href="https://www.science.org/doi/full/10.1126/sciadv.adg2488" rel="noopener noreferrer">Human-level few-shot concept induction through minimax entropy learning</a> - <em><strong>Science Advances</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?&amp;cluster=9084477652494351940" rel="noopener noreferrer">All Versions</a>]. This paper introduces a computational model designed to emulate human inductive reasoning on abstract reasoning tasks, such as those in IQ tests, using a minimax entropy approach. This method combines identifying the most effective constraints on data via minimum entropy with determining the best combination of them via maximum entropy.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="generative-model"><a class="anchor" aria-hidden="true" tabindex="-1" href="#generative-model"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Generative Model</h4><ul>
<li><p><a href="https://github.com/YuzheSHI/generative-modeling-explained" rel="noopener noreferrer">Generative Modeling Explained (⭐60)</a> - <em><strong>Statistical Machine Learning Tutorials</strong></em>, 2022. This tutorial on generative modeling is in part of Statistical Machine Learning Tutorial by Ying Nian Wu at UCLA Statistics. The tutorial goes over the key equations and algorithms for learning recent generative models, including energy-based models, diffusion/score-based models, autoregressive/flow-based models, VAEs, and GANs, and explains the connections between these models.</p>
</li>
<li><p><a href="https://www.taylorfrancis.com/books/mono/10.1201/9780429258411/bayesian-data-analysis-andrew-gelman-donald-rubin-john-carlin-hal-stern" rel="noopener noreferrer">Bayesian Data Analysis</a> - <em><strong>Chapman and Hall/CRC</strong></em>, 1995. [<a href="https://scholar.google.com/scholar?cluster=5067275302121330689&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Don Rubin's introductory book on Bayesian models.</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1023/A:1007925832420" rel="noopener noreferrer">Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling</a> - <em><strong>International Journal of Computer Vision</strong></em>, 1998. [<a href="https://scholar.google.com/scholar?cluster=11604954524863138240" rel="noopener noreferrer">All Versions</a>]. [<a href="https://dash.harvard.edu/bitstream/handle/1/3637117/Mumford_FRAME.pdf?sequence=1" rel="noopener noreferrer">Preprint</a>]. This article presents a statistical theory for texture modeling. This theory combines filtering theory and Markov random field modeling through the maximum entropy principle, and interprets and clarifies many previous concepts and methods for texture analysis and synthesis from a unified point of view. The theory characterizes the ensemble of images I with the same texture appearance by a probability distribution f(I) on a random field, and the objective of texture modeling is to make inference about f(I), given a set of observed texture examples.</p>
</li>
<li><p><a href="https://www.annualreviews.org/content/journals/10.1146/annurev.psych.55.090902.142005" rel="noopener noreferrer">Object Perception as Bayesian Inference</a> - <em><strong>Annual Review of Psychology</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=1611451804975333652" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.cs.jhu.edu/~ayuille/pubs/ucla/A189_dkersten_ARP2004.pdf" rel="noopener noreferrer">Preprint</a>]. We perceive the shapes and material properties of objects quickly and reliably despite the complexity and objective ambiguities of natural images. Typical images are highly complex because they consist of many objects embedded in background clutter. Moreover, the image features of an object are extremely variable and ambiguous owing to the effects of projection, occlusion, background clutter, and illumination. The very success of everyday vision implies neural mechanisms, yet to be understood, that discount irrelevant information and organize ambiguous or noisy local image features into objects and surfaces. Recent work in Bayesian theories of visual perception has shown how complexity may be managed and ambiguity resolved through the task-dependent, probabilistic integration of prior object knowledge with image features.</p>
</li>
<li><p><a href="https://www.ams.org/journals/qam/2019-77-02/S0033-569X-2018-01528-5/home.html" rel="noopener noreferrer">A tale of three probabilistic families: Discriminative, descriptive, and generative models</a> - <em><strong>Quarterly of Applied Mathematics</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=6129609629126793774" rel="noopener noreferrer">All Versions</a>]. [<a href="http://www.stat.ucla.edu/~ywu/QAM2018.pdf" rel="noopener noreferrer">Preprint</a>]. The pattern theory of Grenander is a mathematical framework where patterns are represented by probability models on random variables of algebraic structures. In this paper, the authors review three families of probability models, namely, the discriminative models, the descriptive models, and the generative models. A discriminative model is in the form of a classifier. It specifies the conditional probability of the class label given the input signal. A descriptive model specifies the probability distribution of the signal, based on an energy function defined on the signal. A generative model assumes that the signal is generated by some latent variables via a transformation. The authors shall review these models within a common framework and explore their connections, and shall also review the recent developments that take advantage of the high approximation capacities of deep neural networks.</p>
</li>
<li><p><a href="https://www.jstor.org/stable/43638808?seq=1" rel="noopener noreferrer">From information scaling of natural images to regimes of statistical models</a> - <em><strong>Quarterly of Applied Mathematics</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=17387130978932998303" rel="noopener noreferrer">All Versions</a>]. [<a href="http://www.stat.ucla.edu/~sczhu/papers/Quarterly_final.pdf" rel="noopener noreferrer">Preprint</a>].  One fundamental property of natural image data that distinguishes vision from other sensory tasks such as speech recognition is that scale plays a profound role in image formation and interpretation. Specifically, visual objects can appear at a wide range of scales in the images due to the change of viewing distance as well as camera resolution. The same objects appearing at different scales produce different image data with different statistical properties. In particular, this work shows that the entropy rate of the image data changes over scale. Moreover, the inferential uncertainty changes over scale too. The authors call these changes information scaling. They then examine both empirically and theoretically two prominent and yet largely isolated classes of image models, namely, wavelet sparse coding models and Markov random field models. The results indicate that the two classes of models are appropriate for two different entropy regimes: sparse coding targets low entropy regimes, whereas Markov random fields are appropriate for high entropy regimes. Because information scaling connects different entropy regimes, both sparse coding and Markov random fields are necessary for representing natural image data, and information scaling triggers transitions between these two regimes.</p>
</li>
<li><p><a href="https://proceedings.mlr.press/v48/xiec16.html" rel="noopener noreferrer">A Theory of Generative ConvNet</a> - <em><strong>ICML'16</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=11062907630625111054" rel="noopener noreferrer">All Versions</a>]. The authors show that a generative random field model, which they call generative ConvNet, can be derived from the commonly used discriminative ConvNet, by assuming a ConvNet for multi-category classification and assuming one of the category is a base category generated by a reference distribution. For a further assumption that the non-linearity in the ConvNet is Rectified Linear Unit (ReLU) and the reference distribution is Gaussian white noise, then a generative ConvNet model that is unique among energy-based models is obtained: The model is piecewise Gaussian, and the means of the Gaussian pieces are defined by an auto-encoder, where the filters in the bottom-up encoding become the basis functions in the top-down decoding, and the binary activation variables detected by the filters in the bottom-up convolution process become the coefficients of the basis functions in the top-down deconvolution process.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/8519332" rel="noopener noreferrer">Cooperative Training of Descriptor and Generator Networks</a> - <em><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=18202808849093155435" rel="noopener noreferrer">All Versions</a>]. This paper studies the cooperative training of two generative models for image modeling and synthesis. Both models are parametrized by convolutional neural networks (ConvNets). The first model is a deep energy-based model, whose energy function is defined by a bottom-up ConvNet, which maps the observed image to the energy. We call it the descriptor network. The second model is a generator network, which is a non-linear version of factor analysis. It is defined by a top-down ConvNet, which maps the latent factors to the observed image. The maximum likelihood learning algorithms of both models involve MCMC sampling such as Langevin dynamics. This work observes that the two learning algorithms can be seamlessly interwoven into a cooperative learning algorithm that can train both models simultaneously. Specifically, within each iteration of the cooperative learning algorithm, the generator model generates initial synthesized examples to initialize a finite-step MCMC that samples and trains the energy-based descriptor model. After that, the generator model learns from how the MCMC changes its synthesized examples. That is, the descriptor model teaches the generator model by MCMC, so that the generator model accumulates the MCMC transitions and reproduces them by direct ancestral sampling.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2020/hash/fa3060edb66e6ff4507886f9912e1ab9-Abstract.html" rel="noopener noreferrer">Learning Latent Space Energy-Based Prior Model</a> - <em><strong>NeurIPS'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=9945264852135249894" rel="noopener noreferrer">All Versions</a>]. [<a href="https://bpucla.github.io/latent-space-ebm-prior-project/" rel="noopener noreferrer">Project</a>]. [<a href="https://github.com/bpucla/latent-space-EBM-prior" rel="noopener noreferrer">Code (⭐36)</a>]. A milestone paper on Latent Energy-Based Model.</p>
</li>
<li><p><a href="https://openreview.net/forum?id=v_1Soh8QUNc" rel="noopener noreferrer">Learning Energy-Based Models by Diffusion Recovery Likelihood</a> - <em><strong>ICLR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=4399294843209736764" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/ruiqigao/recovery_likelihood" rel="noopener noreferrer">Code (⭐51)</a>].</p>
</li>
<li><p><a href="https://openreview.net/forum?id=PxTIG12RRHS&amp;utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" rel="noopener noreferrer">Score-Based Generative Modeling through Stochastic Differential Equations</a> - <em><strong>ICLR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=14592788616550656262" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://proceedings.mlr.press/v119/li20i.html" rel="noopener noreferrer">Latent Space Factorisation and Manipulation via Matrix Subspace Projection</a> - <em><strong>ICML'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9592355331559392684" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/6796444" rel="noopener noreferrer">Minimax entropy principle and its application to texture modeling</a> - <em><strong>Neural Computing</strong></em>, 1997. [<a href="https://scholar.google.com/scholar?cluster=407872717119429940" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.dam.brown.edu/people/mumford/vision/papers/1997e--MinimaxEntropy-NC.pdf" rel="noopener noreferrer">Preprint</a>]. This article proposes a general theory and methodology, called the minimax entropy principle, for building statistical models for images (or signals) in a variety of applications. This principle consists of two parts. The first is the maximum entropy principle for feature binding (or fusion): for a given set of observed feature statistics, a distribution can be built to bind these feature statistics together by maximizing the entropy over all distributions that reproduce them. The second part is the minimum entropy principle for feature selection: among all plausible sets of feature statistics, we choose the set whose maximum entropy distribution has the minimum entropy. Computational and inferential issues in both parts are addressed; in particular, a feature pursuit procedure is proposed for approximately selecting the optimal set of features. The minimax entropy principle is then corrected by considering the sample variation in the observed feature statistics, and an information criterion for feature pursuit is derived. The minimax entropy principle is applied to texture modeling, where a novel Markov random field (MRF) model, called FRAME (filter, random field, and minimax entropy), is derived, and encouraging results are obtained in experiments on a variety of texture images.</p>
</li>
<li><p><a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10473879" rel="noopener noreferrer">Parameter Expansion for Data Augmentation</a> - <em><strong>Journal of the American Statistical Association</strong></em>, 1999. [<a href="https://scholar.google.com/scholar?cluster=15342818142955984734" rel="noopener noreferrer">All Versions</a>]. [<a href="http://www.stat.ucla.edu/~ywu/research/papers/PXDA.pdf" rel="noopener noreferrer">Preprint</a>]. Viewing the observed data of a statistical model as incomplete and augmenting its missing parts are useful for clarifying concepts and central to the invention of two well-known statistical algorithms: expectation-maximization (EM) and data augmentation. Recently, the authors demonstrated that expanding the parameter space along with augmenting the missing data is useful for accelerating iterative computation in an EM algorithm. The main purpose of this article is to rigorously define a parameter expanded data augmentation (PX-DA) algorithm and to study its theoretical properties. The PX-DA is a special way of using auxiliary variables to accelerate Gibbs sampling algorithms and is closely related to reparameterization techniques.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/1000239" rel="noopener noreferrer">Image segmentation by data-driven markov chain monte carlo</a> - <em><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong></em>, 2002. [<a href="https://scholar.google.com/scholar?cluster=3461400072144667491" rel="noopener noreferrer">All Versions</a>]. [<a href="http://www.stat.ucla.edu/~sczhu/papers/DDMCMC_reprint.pdf" rel="noopener noreferrer">Preprint</a>]. This paper presents a computational paradigm called Data-Driven Markov Chain Monte Carlo (DDMCMC) for image segmentation in the Bayesian statistical framework. The paper contributes to image segmentation in four aspects. First, it designs efficient and well-balanced Markov Chain dynamics to explore the complex solution space and, thus, achieves a nearly global optimal solution independent of initial segmentations. Second, it presents a mathematical principle and a K-adventurers algorithm for computing multiple distinct solutions from the Markov chain sequence and, thus, it incorporates intrinsic ambiguities in image segmentation. Third, it utilizes data-driven (bottom-up) techniques, such as clustering and edge detection, to compute importance proposal probabilities, which drive the Markov chain dynamics and achieve tremendous speedup in comparison to the traditional jump-diffusion methods. Fourth, the DDMCMC paradigm provides a unifying framework in which the role of many existing segmentation algorithms, such as, edge detection, clustering, region growing, split-merge, snake/balloon, and region competition, are revealed as either realizing Markov chain dynamics or computing importance proposal probabilities. Thus, the DDMCMC paradigm combines and generalizes these segmentation methods in a principled way.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2006/file/87f4d79e36d68c3031ccf6c55e9bbd39-Paper.pdf" rel="noopener noreferrer">Efficient Learning of Sparse Representations with an Energy-Based Model</a> - <em><strong>NeurIPS'06</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=2247668190782691760" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://yann.lecun.com/exdb/publis/orig/lecun-06.pdf" rel="noopener noreferrer">A Tutorial on Energy-Based Learning</a> - <em><strong>Predicting Structured Data, MIT Press</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=8819502341081664768&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versiosn</a>]. Yann LeCun's tutorial on energy-based learning.</p>
</li>
<li><p><a href="https://arxiv.org/abs/1511.06434" rel="noopener noreferrer">Unsupervised Representaton Learning with Deep Convolutional Generative Adversarial Networks</a> - <em><strong>ICLR'16</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=3321343160055675528&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.jmlr.org/papers/v20/18-173.html" rel="noopener noreferrer">Analysis of Langevin Monte Carlo via Convex Optimization</a> - <em><strong>Journal of Machine Learning Research</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=5305860199396047317" rel="noopener noreferrer">All Versions</a>]. This paper provides new insights on the Unadjusted Langevin Algorithm. The authors show that this method can be formulated as the first order optimization algorithm for an objective functional defined on the Wasserstein space of order $2$. Using this interpretation and techniques borrowed from convex optimization, the authors give a non-asymptotic analysis of this method to sample from log-concave smooth target distribution on $\mathbb{R}^d$. Based on this interpretation, the authors propose two new methods for sampling from a non-smooth target distribution. These new algorithms are natural extensions of the Stochastic Gradient Langevin Dynamics (SGLD) algorithm, which is a popular extension of the Unadjusted Langevin Algorithm for largescale Bayesian inference. Using the optimization perspective, the authors provide non-asymptotic convergence analysis for the newly proposed methods.</p>
</li>
<li><p><a href="https://www.science.org/doi/full/10.1126/science.aag2612" rel="noopener noreferrer">A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs</a> - <em><strong>Science</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=1478382321633671444" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.cs.jhu.edu/~ayuille/JHUcourses/ProbabilisticModelsOfVisualCognition2020/Lec22/GeorgeCAPCHAS.pdf" rel="noopener noreferrer">Preprint</a>]. Learning from a few examples and generalizing to markedly different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, this work introduces a probabilistic generative model for vision in which message-passing–based inference handles recognition, segmentation, and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) by generatively segmenting characters without CAPTCHA-specific heuristics.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0010028516302766" rel="noopener noreferrer">Where do hypotheses come from?</a> - <em><strong>Cognitive Psychology</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=17480320046655923235" rel="noopener noreferrer">All Versions</a>]. [<a href="https://gershmanlab.com/pubs/Dasgupta17.pdf" rel="noopener noreferrer">Preprint</a>]. Why are human inferences sometimes remarkably close to the Bayesian ideal and other times systematically biased? In particular, why do humans make near-rational inferences in some natural domains where the candidate hypotheses are explicitly available, whereas tasks in similar domains requiring the self-generation of hypotheses produce systematic deviations from rational inference. This work proposes that these deviations arise from algorithmic processes approximating Bayes’ rule. Specifically in our account, hypotheses are generated stochastically from a sampling process, such that the sampled hypotheses form a Monte Carlo approximation of the posterior.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="nonparametric-model"><a class="anchor" aria-hidden="true" tabindex="-1" href="#nonparametric-model"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Nonparametric Model</h4><ul>
<li><p><a href="https://www.jstor.org/stable/2958008?seq=1" rel="noopener noreferrer">A Bayesian Analysis of Some Non-parametric Problems</a> - <em><strong>The Annals of Statistics</strong></em>, 1973. [<a href="https://scholar.google.com/scholar?cluster=3969163427460060902" rel="noopener noreferrer">All Versions</a>]. [<a href="https://people.stat.sc.edu/hansont/stat740/Ferguson1973.pdf" rel="noopener noreferrer">Preprint</a>]. A classic review on non-parametric problems.</p>
</li>
<li><p><a href="https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/antoniak.pdf" rel="noopener noreferrer">Mixtures of Dirichlet Process with Applications to Bayesian Nonparametric Problems</a> - <em><strong>The Annals of Statistics</strong></em>, 1974. [<a href="https://scholar.google.com/scholar?cluster=17937202534282344046&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on Dirichlet Process modeling for non-parametric problems.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0022000000917112" rel="noopener noreferrer">Latent Semantic Indexing: A Probabilistic Analysis</a> - <em><strong>Journal of Computer and System Sciences</strong></em>, 2000. [<a href="https://scholar.google.com/scholar?cluster=7296120469860429813&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on hierarchical topic model.</p>
</li>
<li><p><a href="https://projecteuclid.org/journals/statistical-science/volume-19/issue-1/Nonparametric-Bayesian-Data-Analysis/10.1214/088342304000000017.full" rel="noopener noreferrer">Nonparametric Bayesian Data Analysis</a> - <em><strong>Statistical Science</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=13476170780072319995" rel="noopener noreferrer">All Versions</a>]. This paper reviews the current state of nonparametric Bayesian inference. The discussion follows a list of important statistical inference problems, including density estimation, regression, survival analysis, hierarchical models and model validation. For each inference problem the authors review relevant nonparametric Bayesian models and approaches including Dirichlet process (DP) models and variations, Pólya trees, wavelet based models, neural network models, spline regression, CART, dependent DP models and model validation with DP and Pólya tree extensions of parametric models.</p>
</li>
<li><p><a href="https://www.pnas.org/doi/abs/10.1073/pnas.0307752101" rel="noopener noreferrer">Finding scientific topics</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=17382767110929995134" rel="noopener noreferrer">All Versions</a>]. A first step in identifying the content of a document is determining which topics that document addresses. This paper describes a generative model for documents, in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. The authors then present a Markov chain Monte Carlo algorithm for inference in this model. The authors use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. This work shows that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying “hot topics” by examining temporal dynamics and tagging abstracts to illustrate semantic content.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2003/file/7b41bfa5085806dfa24b8c9de0ce567f-Paper.pdf" rel="noopener noreferrer">Hierarchical topic models and the nested Chinese restaurant process</a> - <em><strong>NeurIPS'03</strong></em>, 2003. [<a href="https://scholar.google.com/scholar?cluster=15040818675282958700&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper for nested Chinese restaurant process.</p>
</li>
<li><p><a href="https://www.aaai.org/Papers/AAAI/2006/AAAI06-061.pdf" rel="noopener noreferrer">Learning Systems of Concepts with an Infinite Relational Model</a> - <em><strong>AAAI'06</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=3207350432755252565&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/1667053.1667056" rel="noopener noreferrer">The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies</a> - <em><strong>Journal of the ACM</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=8216933258869737505&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://mlg.eng.cam.ac.uk/zoubin/papers/ibptr.pdf" rel="noopener noreferrer">Infinite Latent Feature Models and the Indian Buffet Process</a> - <em><strong>Gatsby Computational Neuroscience Unit Technical Report 2005-001</strong></em>, 2005. [<a href="https://scholar.google.com/scholar?cluster=13180738480564152907&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://jmlr.org/papers/v12/griffiths11a.html" rel="noopener noreferrer">The Indian Buffet Process: An Introduction and Review</a> - <em><strong>Journal of Machine Learning Research</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?cluster=6301314251995890943" rel="noopener noreferrer">All Versions</a>]. The Indian buffet process is a stochastic process defining a probability distribution over equivalence classes of sparse binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. This work gives a detailed derivation of this distribution, and illustrate its use as a prior in an infinite latent feature model. The authors then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.5555/3020336.3020347" rel="noopener noreferrer">Nonparametric Bayesian Logic</a> - <em><strong>UAI'05</strong></em>, 2005. [<a href="https://scholar.google.com/scholar?cluster=18267211625980322095" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.cs.ubc.ca/~nando/papers/npblog.pdf" rel="noopener noreferrer">Preprint</a>]. The Bayesian Logic (BLOG) language was recently developed for defining first-order probability models over worlds with unknown numbers of objects. It handles important problems in AI, including data association and population estimation. This paper extends BLOG by adopting generative processes over function spaces — known as nonparametrics in the Bayesian literature. This work introduces syntax for reasoning about arbitrary collections of objects, and their properties, in an intuitive manner. By exploiting exchangeability, distributions over unknown objects and their attributes are cast as Dirichlet processes, which resolve difficulties in model selection and inference caused by varying numbers of objects.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.5555/3020419.3020485" rel="noopener noreferrer">Infinite Hidden Relational Models</a> - <em><strong>UAI'06</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=2143172296528388141" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.dbs.ifi.lmu.de/~yu_k/uai06_relation.pdf" rel="noopener noreferrer">Preprint</a>]. Relational learning analyzes the probabilistic constraints between the attributes of entities and relationships. This work extends the expressiveness of relational models by introducing for each entity (or object) an infinite-dimensional latent variable as part of a Dirichlet process (DP) mixture model. This work discusses inference in the model, which is based on a DP Gibbs sampler, i.e., the Chinese restaurant process. The authors extended the Chinese restaurant process to be applicable to relational modeling.</p>
</li>
<li><p><a href="https://alchemy.cs.washington.edu/papers/kok07/kok07.pdf" rel="noopener noreferrer">Statistical Predicate Invention</a> - <em><strong>ICML'07</strong></em>, 2007. [<a href="https://scholar.google.com/scholar?cluster=17009312281859401704" rel="noopener noreferrer">All Versions</a>]. This work proposes statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. This work proposes an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. The proposed approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the objects they relate).</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="bayesian-optimization"><a class="anchor" aria-hidden="true" tabindex="-1" href="#bayesian-optimization"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Bayesian Optimization</h4><ul>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/7352306" rel="noopener noreferrer">Taking the Human Out of the Loop: A Review of Bayesian Optimization</a> - <em><strong>Proceedings of the IEEE</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=2039456143890648437" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.cs.princeton.edu/~rpa/pubs/shahriari2016loop.pdf" rel="noopener noreferrer">Preprint</a>]. Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html" rel="noopener noreferrer">Practical Bayesian Optimization of Machine Learning Algorithms</a> - <em><strong>NeurIPS'12</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=14442949298925775705" rel="noopener noreferrer">All Versions</a>]. The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a “black art” requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. This work considers this problem through the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process (GP). The authors show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. The authors describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. These proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including Latent Dirichlet Allocation, Structured SVMs and convolutional neural networks.</p>
</li>
<li><p><a href="https://arxiv.org/abs/1807.02811" rel="noopener noreferrer">A Tutorial on Bayesian Optimization</a> - 2018. [<a href="https://scholar.google.com/scholar?cluster=7971934771645047583" rel="noopener noreferrer">All Versions</a>]. Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. This tutorial describes how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. The authors then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. The authors conclude with a discussion of Bayesian optimization software and future research directions in the field. This tutorial provides a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="concepts"><a class="anchor" aria-hidden="true" tabindex="-1" href="#concepts"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Concepts</h3><h4 id="theory-of-concepts"><a class="anchor" aria-hidden="true" tabindex="-1" href="#theory-of-concepts"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Theory of Concepts</h4><ul>
<li><p><a href="https://plato.stanford.edu/entries/concepts/" rel="noopener noreferrer">Concepts</a> - <em><strong>Plato Stanford</strong></em>. A collection of the computational philosophical debates about the concepts.</p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/Theory-theory" rel="noopener noreferrer">Theory-theory</a> - <em><strong>Wikipedia</strong></em>. Wikipedia for the Theory theory, a perspective that contextualizes concepts in theoretical (or empirical) systems.</p>
</li>
<li><p><a href="https://hk1lib.org/book/3659332/11fa44" rel="noopener noreferrer">Conceptual Change in Childhood</a> - <em><strong>MIT Press</strong></em>, 1985. [<a href="https://scholar.google.com/scholar?cluster=11720022076481483465" rel="noopener noreferrer">All Versions</a>]. Susan Carey's book on the theory theory of concepts in child development.</p>
</li>
<li><p><a href="http://library.lol/main/6A8215E9BAEB77F198C98CD75C517E02" rel="noopener noreferrer">Words, thoughts, and theories</a> - <em><strong>MIT Press</strong></em>, 1997. [<a href="https://scholar.google.com/scholar?cluster=16726462136203686735&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Alison Gopnik's book that articulates and defends the "theory theory" of cognitive and semantic development, the idea that infants and young children, like scientists, learn about the world by forming and revising theories-a view of the origins of knowledge and meaning that has broad implications for cognitive science.</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/1994-97940-009" rel="noopener noreferrer">The Theory Theory</a> - <em><strong>Mapping the mind: Domain specificity in cognition and culture, Cambridge University Press</strong></em>, 1994. [<a href="https://scholar.google.com/scholar?cluster=9397889700764191662&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Alison Gopnik's original paper on the theory theory.</p>
</li>
<li><p><a href="https://hk1lib.org/book/844457/42178f?id=844457&amp;secret=42178f" rel="noopener noreferrer">The Origin of Concepts</a> - <em><strong>Oxford University Press</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=11493102398422813821&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Susan Carey's extended book on the theory theory of concepts in child development.</p>
</li>
<li><p><a href="https://osf.io/preprints/psyarxiv/xrnb2" rel="noopener noreferrer">What we mean when we say semantic: A Consensus statement on the nomenclature of semantic memory</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=7464626532716945232" rel="noopener noreferrer">All Versions</a>]. The aim of this multidisciplinary workgroup was to establish consensus definitions for some of the major recurring constructs in semantic research (e.g., concept, amodal, abstract). These efforts yielded a glossary consisting of succinct definitions, agreement, subjective confidence ratings, relevant theoretical background, and principled dissenting views. These core definitions will potentially yield benchmarks for aligning perspectives and improving cross-disciplinary communication in semantic research.</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/2012-12791-001" rel="noopener noreferrer">Reconstructing constructivism: Causal models, Bayesian learning mechanisms, and the theory theory</a> - <em><strong>Psychological Bulletin</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=11218217347365817167&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Alison Gopnik's review on the constructivism idea of developmental research, including the theory theory of concepts.</p>
</li>
<li><p><a href="https://groups.psych.northwestern.edu/gentner/newpdfpapers/MedinGoldstoneGentner90.pdf" rel="noopener noreferrer">Similarity involving attributes and relations: Judgments of similarity and difference are not inverses</a> - <em><strong>Psychological Science</strong></em>, 1990. [<a href="https://scholar.google.com/scholar?cluster=13205938250772079784" rel="noopener noreferrer">All Versions</a>]. Theory on similarity judgement by attributes and relations.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="human-concept-representation"><a class="anchor" aria-hidden="true" tabindex="-1" href="#human-concept-representation"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Human Concept Representation</h4><ul>
<li><p><a href="https://www.science.org/doi/full/10.1126/science.aaf0941" rel="noopener noreferrer">Organizing conceptual knowledge in humans with a gridlike code</a> - <em><strong>Science</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=10995575332310321503" rel="noopener noreferrer">All Versions</a>]. [<a href="http://behavioralhealth2000.com/wp-content/uploads/2017/01/Organizing-conceptual-knowledge-in-humans-with-a-gridlike-code.pdf" rel="noopener noreferrer">Preprint</a>]. It has been hypothesized that the brain organizes concepts into a mental map, allowing conceptual relationships to be navigated in a manner similar to that of space. Grid cells use a hexagonally symmetric code to organize spatial representations and are the likely source of a precise hexagonal symmetry in the functional magnetic resonance imaging signal. Humans navigating conceptual two-dimensional knowledge showed the same hexagonal signal in a set of brain regions markedly similar to those activated during spatial navigation. This gridlike signal is consistent across sessions acquired within an hour and more than a week apart. This work's findings suggest that global relational codes may be used to organize nonspatial conceptual representations and that these codes may have a hexagonal gridlike pattern when conceptual knowledge is laid out in two continuous dimensions.</p>
</li>
<li><p><a href="https://www.science.org/doi/full/10.1126/science.aat6766" rel="noopener noreferrer">Navigating cognition: Spatial codes for human thinking</a> - <em><strong>Science</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=1407237757770081862" rel="noopener noreferrer">All Versions</a>]. [<a href="https://doellerlab.com/wp-content/uploads/2018/11/Bellmund_et_al_2018_Science_Navigating-cognition.pdf" rel="noopener noreferrer">Preprint</a>]. The hippocampal formation has long been suggested to underlie both memory formation and spatial navigation. This work discusses how neural mechanisms identified in spatial navigation research operate across information domains to support a wide spectrum of cognitive functions. In the proposed framework, place and grid cell population codes provide a representational format to map variable dimensions of cognitive spaces. This highly dynamic mapping system enables rapid reorganization of codes through remapping between orthogonal representations across behavioral contexts, yielding a multitude of stable cognitive spaces at different resolutions and hierarchical levels. Action sequences result in trajectories through cognitive space, which can be simulated via sequential coding in the hippocampus. In this way, the spatial representational format of the hippocampal formation has the capacity to support flexible cognition and behavior.</p>
</li>
<li><p><a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(20)30250-3" rel="noopener noreferrer">Structuring Knowledge with Cognitive Maps and Cognitive Graphs</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7196012353183004425" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.sas.upenn.edu/psych/epsteinlab/pdfs/Peer%20Brunec%20Newcombe%20Epstein%20TiCS%202020%20Cog%20maps%20and%20cog%20graphs.pdf" rel="noopener noreferrer">Preprint</a>]. Humans and animals use mental representations of the spatial structure of the world to navigate. The classical view is that these representations take the form of Euclidean cognitive maps, but alternative theories suggest that they are cognitive graphs consisting of locations connected by paths. The authors review evidence suggesting that both map-like and graph-like representations exist in the mind/brain that rely on partially overlapping neural systems. Maps and graphs can operate simultaneously or separately, and they may be applied to both spatial and nonspatial knowledge. By providing structural frameworks for complex information, cognitive maps and cognitive graphs may provide fundamental organizing schemata that allow us to navigate in physical, social, and conceptual spaces.</p>
</li>
<li><p><a href="https://www.nature.com/articles/nature17637" rel="noopener noreferrer">Natural speech reveals the semantic maps that tile human cerebral cortex</a> - <em><strong>Nature</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=14997953800741854188" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.polyu.edu.hk/cbs/rclcn/images/cdl_articles/H/Huth_et_al._2016.pdf" rel="noopener noreferrer">Preprint</a>]. [<a href="https://github.com/HuthLab/speechmodeltutorial" rel="noopener noreferrer">Code &amp; Tutorial (⭐34)</a>]. The meaning of language is represented in regions of the cerebral cortex collectively known as the ‘semantic system’. However, little of the semantic system has been mapped comprehensively, and the semantic selectivity of most regions is unknown. This work systematically maps semantic selectivity across the cortex using voxel-wise modelling of functional MRI (fMRI) data collected while subjects listened to hours of narrative stories. This work shows that the semantic system is organized into intricate patterns that seem to be consistent across individuals. The authors then use a novel generative model to create a detailed semantic atlas. The results suggest that most areas within the semantic system represent information about specific semantic domains, or groups of related concepts, and the atlas shows which domains are represented in each area. This study demonstrates that data-driven methods---commonplace in studies of human neuroanatomy and functional connectivity---provide a powerful and efficient means for mapping functional representations in the brain.</p>
</li>
<li><p><a href="https://journals.sagepub.com/doi/full/10.1177/09567976211003877" rel="noopener noreferrer">Idiosyncratic Tower of Babel: Individual differences in word-meaning representation increase as word abstractness increases</a> - <em><strong>Psychological Science</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=18214600097352809308" rel="noopener noreferrer">All Versions</a>]. [<a href="http://bilab.bnu.edu.cn/paper/2021/Wang_2021_Psychology%20Science.pdf" rel="noopener noreferrer">All Versions</a>]. Humans primarily rely on language to communicate, on the basis of a shared understanding of the basic building blocks of communication: words. Do we mean the same things when we use the same words? Although cognitive neural research on semantics has revealed the common principles of word-meaning representation, the factors underlying the potential individual variations in word meanings are unknown. This work empirically characterized the intersubject consistency of 90 words across 20 adult subjects (10 female) using both behavioral measures (rating-based semantic-relationship patterns) and neuroimaging measures (word-evoked brain activity patterns). Across both the behavioral and neuroimaging experiments, this work showed that the magnitude of individual disagreements on word meanings could be modeled on the basis of how much language or sensory experience is associated with a word and that this variation increases with word abstractness. Uncovering the cognitive and neural origins of word-meaning disagreements across individuals has implications for potential mechanisms to modulate such disagreements.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41562-022-01316-8" rel="noopener noreferrer">Semantic projection recovers rich human knowledge of multiple object features from word embeddings</a> - <em><strong>Nature Human Behavior</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=2499199921371106654" rel="noopener noreferrer">All Versions</a>]. [<a href="https://cap.csail.mit.edu/sites/default/files/research-pdfs/Semantic%20projection%20recovers%20rich%20human%20knowledge%20of%20multiple%20object%20features%20from%20word%20embeddings.pdf" rel="noopener noreferrer">Preprint</a>]. How is knowledge about word meaning represented in the mental lexicon? Current computational models infer word meanings from lexical co-occurrence patterns. They learn to represent words as vectors in a multidimensional space, wherein words that are used in more similar linguistic contexts—that is, are more semantically related—are located closer together. However, whereas inter-word proximity captures only overall relatedness, human judgements are highly context dependent. For example, dolphins and alligators are similar in size but differ in dangerousness. This work proposes a domain-general method to extract context-dependent relationships from word embeddings: ‘semantic projection’ of word-vectors onto lines that represent features such as size (the line connecting the words ‘small’ and ‘big’) or danger (‘safe’ to ‘dangerous’), analogous to ‘mental scales’. This method recovers human judgements across various object categories and properties. Thus, the geometry of word embeddings explicitly represents a wealth of context-dependent world knowledge.</p>
</li>
<li><p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00385/full" rel="noopener noreferrer">Using a high-dimensional graph of semantic space to model relationships among words</a> - <em><strong>Frontiers in Psychology</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=472523411548302295" rel="noopener noreferrer">All Versions</a>]. The GOLD model (Graph Of Language Distribution) is a network model constructed based on co-occurrence in a large corpus of natural language that may be used to explore what information may be present in a graph-structured model of language, and what information may be extracted through theoretically-driven algorithms as well as standard graph analysis methods. The present study will employ GOLD to examine two types of relationship between words: semantic similarity and associative relatedness. Semantic similarity refers to the degree of overlap in meaning between words, while associative relatedness refers to the degree to which two words occur in the same schematic context. It is expected that a graph structured model of language constructed based on co-occurrence should easily capture associative relatedness, because this type of relationship is thought to be present directly in lexical co-occurrence. However, it is hypothesized that semantic similarity may be extracted from the intersection of the set of first-order connections, because two words that are semantically similar may occupy similar thematic or syntactic roles across contexts and thus would co-occur lexically with the same set of nodes. Two versions the GOLD model that differed in terms of the co-occurence window, bigGOLD at the paragraph level and smallGOLD at the adjacent word level, were directly compared to the performance of a well-established distributional model, Latent Semantic Analysis (LSA). The superior performance of the GOLD models (big and small) suggest that a single acquisition and storage mechanism, namely co-occurrence, can account for associative and conceptual relationships between words and is more psychologically plausible than models using singular value decomposition (SVD).</p>
</li>
<li><p><a href="https://academic.oup.com/cercor/article/33/15/9280/7190929" rel="noopener noreferrer">Simple shape feature computation across modalities: convergence and divergence between the ventral and dorsal visual streams</a> - <em><strong>Cerebral Cortex</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=5977822802446917081" rel="noopener noreferrer">All Versions</a>]. [<a href="http://bilab.bnu.edu.cn/paper/2023/Tian_2023_CC.pdf" rel="noopener noreferrer">Preprints</a>]. Shape processing, whether by seeing or touching, is pivotal to object recognition and manipulation. Although the low-level signals are initially processed by different modality-specific neural circuits, multimodal responses to object shapes have been reported along both ventral and dorsal visual pathways. To understand this transitional process, the authors conducted visual and haptic shape perception fMRI experiments to test basic shape features (i.e. curvature and rectilinear) across the visual pathways. Using a combination of region-of-interest-based support vector machine decoding analysis and voxel selection method, the authors found that the top visual-discriminative voxels in the left occipital cortex (OC) could also classify haptic shape features, and the top haptic-discriminative voxels in the left posterior parietal cortex (PPC) could also classify visual shape features. Furthermore, these voxels could decode shape features in a cross-modal manner, suggesting shared neural computation across visual and haptic modalities. In the univariate analysis, the top haptic-discriminative voxels in the left PPC showed haptic rectilinear feature preference, whereas the top visual-discriminative voxels in the left OC showed no significant shape feature preference in either of the two modalities. Together, these results suggest that mid-level shape features are represented in a modality-independent manner in both the ventral and dorsal streams.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41597-019-0341-x" rel="noopener noreferrer">The Database of Cross-Linguistic Colexifications, reproducible analysis of cross-linguistic polysemies</a> - <em><strong>Scientific Data</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=4039754406289857135" rel="noopener noreferrer">All Versions</a>]. [<a href="https://clics.clld.org/" rel="noopener noreferrer">Project</a>]. Advances in computer-assisted linguistic research have been greatly influential in reshaping linguistic research. With the increasing availability of interconnected datasets created and curated by researchers, more and more interwoven questions can now be investigated. Such advances, however, are bringing high requirements in terms of rigorousness for preparing and curating datasets. This work presents CLICS, a Database of Cross-Linguistic Colexifications (CLICS). CLICS tackles interconnected interdisciplinary research questions about the colexifcation of words across semantic categories in the world’s languages, and show-cases best practices for preparing data for cross-linguistic research.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S001002772300183X" rel="noopener noreferrer">Locating what comes to mind in empirically derived representational spaces</a> - <em><strong>Cognition</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=57834483230365927" rel="noopener noreferrer">All Versions</a>]. An evidence-based study concluding that people call category members to mind according to their location in representational space, specifically based on the predicted usefulness of considering category members with particular features.</p>
</li>
<li><p><a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(24)00171-2" rel="noopener noreferrer">Why concepts are (probably) vectors</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=4315363807034184312" rel="noopener noreferrer">All Versions</a>]. For decades, cognitive scientists have debated what kind of representation might characterize human concepts. Whatever the format of the representation, it must allow for the computation of varied properties, including similarities, features, categories, definitions, and relations. It must also support the development of theories, ad hoc categories, and knowledge of procedures. Here, the authors discuss why vector-based representations provide a compelling account that can meet all these needs while being plausibly encoded into neural architectures. This view has become especially promising with recent advances in both large language models and vector symbolic architectures. These innovations show how vectors can handle many properties traditionally thought to be out of reach for neural models, including compositionality, definitions, structures, and symbolic computational processes.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="ai-concept-representation"><a class="anchor" aria-hidden="true" tabindex="-1" href="#ai-concept-representation"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>AI Concept Representation</h4><ul>
<li><p><a href="https://www.science.org/doi/full/10.1126/science.ade4401" rel="noopener noreferrer">A principal odor map unifies diverse tasks in olfactory perception</a> - <em><strong>Science</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=17847258457660438418" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/osmoai/publications/tree/main/lee_et_al_2023" rel="noopener noreferrer">Code (⭐28)</a>]. [<a href="https://www.kaggle.com/datasets/aryanamitbarsainyan/multi-labelled-smiles-odors-dataset" rel="noopener noreferrer">Data (Reproduced)</a>]. [<a href="https://centaur.reading.ac.uk/113304/1/Mayhew%20et%20al%20for%20Centaur.pdf" rel="noopener noreferrer">Preprint</a>]. [<a href="https://www.thegoodscentscompany.com/" rel="noopener noreferrer">GoodScents Database</a>]. [<a href="http://www.leffingwell.com/bacispmp.htm" rel="noopener noreferrer">Leffingwell Database</a>]. Mapping molecular structure to odor perception is a key challenge in olfaction. This work used graph neural networks to generate a principal odor map (POM) that preserves perceptual relationships and enables odor quality prediction for previously uncharacterized odorants. The model was as reliable as a human in describing odor quality: On a prospective validation set of 400 out-of-sample odorants, the model-generated odor profile more closely matched the trained panel mean than did the median panelist. By applying simple, interpretable, theoretically rooted transformations, the POM outperformed chemoinformatic models on several other odor prediction tasks, indicating that the POM successfully encoded a generalized map of structure-odor relationships. This approach broadly enables odor prediction and paves the way toward digitizing odors.</p>
</li>
<li><p><a href="https://elifesciences.org/articles/82502" rel="noopener noreferrer">Metabolic activity organizes olfactory representations</a> - <em><strong>eLife</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=8857896396450033667" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/osmoai/publications/tree/main/qian_et_al_2023" rel="noopener noreferrer">Code &amp; Data (⭐28)</a>]. Odorous compounds with similar POM representations are more likely to co-occur within a substance and be metabolically closely related; metabolic reaction sequences also follow smooth paths in POM despite large jumps in molecular structure.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9136877" rel="noopener noreferrer">A Review of Tactile Information: Perception and Action Through Touch</a> - <em><strong>IEEE Transactions on Robotics</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=15493221881484741343" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.researchgate.net/profile/Qiang-Li-110/publication/342797645_A_Review_of_Tactile_Information_Perception_and_Action_Through_Touch/links/602f95bc92851c4ed5806e9f/A-Review-of-Tactile-Information-Perception-and-Action-Through-Touch.pdf" rel="noopener noreferrer">Preprint</a>]. Tactile sensing is a key sensor modality for robots interacting with their surroundings. These sensors provide a rich and diverse set of data signals that contain detailed information collected from contacts between the robot and its environment. The data are however not limited to individual contacts and can be used to extract a wide range of information about the objects in the environment as well as the actions of the robot during the interactions. This article provides an overview of tactile information and its applications in robotics. The authors present a hierarchy consisting of raw, contact, object, and action levels to structure the tactile information, with higher-level information often building upon lower-level information. The authors discuss different types of information that can be extracted at each level of the hierarchy. The article also includes an overview of different types of robot applications and the types of tactile information that they employ. Finally the article ends with a discussion for future tactile applications which are still beyond the current capabilities of robots.</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.html" rel="noopener noreferrer">ImageBind: One Embedding Space To Bind Them All</a> - <em><strong>CVPR'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=1657173986906232916" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/facebookresearch/ImageBind" rel="noopener noreferrer">Project (⭐8.7k)</a>]. This work presents ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. The authors show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and this work sets a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, the authors show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.</p>
</li>
<li><p><a href="https://escholarship.org/uc/item/44s454ng" rel="noopener noreferrer">Semantic features of object concepts generated with GPT-3</a> - <em><strong>CogSci'22</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=16958563995984242923" rel="noopener noreferrer">All Versions</a>]. Semantic features have been playing a central role in investigating the nature of our conceptual representations. Yet the enormous time and effort required to empirically sample and norm features from human raters has restricted their use to a limited set of manually curated concepts. Given recent promising developments with transformer-based language models, here the authors asked whether it was possible to use such models to automatically generate meaningful lists of properties for arbitrary object concepts and whether these models would produce features similar to those found in humans. To this end, the authors probed a GPT-3 model to generate semantic features for 1,854 objects and compared automatically-generated features to existing human feature norms. GPT-3 generated many more features than humans, yet showed a similar distribution in the types of generated features. Generated feature norms rivaled human norms in predicting similarity, relatedness, and category membership, while variance partitioning demonstrated that these predictions were driven by similar variance in humans and GPT-3. Together, these results highlight the potential of large language models to capture important facets of human knowledge and yield a new approach for automatically generating interpretable feature sets, thus drastically expanding the potential use of semantic features in psychological and linguistic studies.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/8953737" rel="noopener noreferrer">Connecting Touch and Vision via Cross-Modal Prediction</a> - <em><strong>CVPR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=17326564895972374001" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/YunzhuLi/VisGel" rel="noopener noreferrer">Project (⭐74)</a>]. Humans perceive the world using multi-modal sensory inputs such as vision, audition, and touch. This work investigates the cross-modal connection between vision and touch. The main challenge in this cross-domain modeling task lies in the significant scale discrepancy between the two: while our eyes perceive an entire visual scene at once, humans can only feel a small region of an object at any given moment. To connect vision and touch, this work introduces new tasks of synthesizing plausible tactile signals from visual inputs as well as imagining how we interact with objects given tactile data as input. To accomplish the goals, the authors first equip robots with both visual and tactile sensors and collect a large-scale dataset of corresponding vision and tactile image sequences. To close the scale gap, the authors present a new conditional adversarial model that incorporates the scale and location information of the touch. Human perceptual studies demonstrate that the model can produce realistic visual images from tactile data and vice versa.</p>
</li>
<li><p><a href="https://aclanthology.org/2022.tacl-1.69/" rel="noopener noreferrer">Unit Testing for Concepts in Neural Networks</a> - <em><strong>Transactions of the Association for Computational Linguistics</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=3036662275506971282&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Testing the concept representation by neural networks through Fodor's theory of concepts.</p>
</li>
<li><p><a href="https://aclanthology.org/2024.acl-long.820/" rel="noopener noreferrer">Do Llamas Work in English? On the Latent Language of Multilingual Transformers</a> - <em><strong>ACL'24</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=5847238732288003106" rel="noopener noreferrer">All Versions</a>]. A preliminary work empirically showing that the intermediate embeddings of multilingual Transformers (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an input-language-specific region of the embedding space. Also, the embedding of abstract concept space lies closer to English than to other languages.</p>
</li>
<li><p><a href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(24)00035-4" rel="noopener noreferrer">From task structures to world models: what do LLMs know?</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=14836877410607949822" rel="noopener noreferrer">All Versions</a>]. [<a href="http://cncl.yale.edu/sites/default/files/pub-downloads/yildirim-paul-llms-knowledge.pdf" rel="noopener noreferrer">Preprint</a>]. In what sense does a large language model (LLM) have knowledge? The authors answer by granting LLMs ‘instrumental knowledge’: knowledge gained by using next-word generation as an instrument. The authors then ask how instrumental knowledge is related to the ordinary, ‘worldly knowledge’ exhibited by humans, and explore this question in terms of the degree to which instrumental knowledge can be said to incorporate the structured world models of cognitive science. The authors discuss ways LLMs could recover degrees of worldly knowledge and suggest that such recovery will be governed by an implicit, resource-rational tradeoff between world models and tasks. The authors' answer to this question extends beyond the capabilities of a particular AI system and challenges assumptions about the nature of knowledge and intelligence.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="complexity--information-theory"><a class="anchor" aria-hidden="true" tabindex="-1" href="#complexity--information-theory"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Complexity &amp; Information Theory</h3><h4 id="theory"><a class="anchor" aria-hidden="true" tabindex="-1" href="#theory"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Theory</h4><ul>
<li><p><a href="http://www.cs.yale.edu/homes/yry/readings/general/shannon1948.pdf" rel="noopener noreferrer">A Mathematical Theory of Communication</a> - <em><strong>The Bell System Technical Journal</strong></em>, 1948. [<a href="https://scholar.google.com/scholar?cluster=8313213127749369813" rel="noopener noreferrer">All Versions</a>]. Shannon's original paper on Information Theory.</p>
</li>
<li><p><a href="https://link.springer.com/content/pdf/10.1007/978-3-030-11298-1.pdf" rel="noopener noreferrer">An introduction to Kolmogorov complexity and its applications</a> - <em><strong>Springer</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=8746708322477453221" rel="noopener noreferrer">All Versions</a>]. The introductory book for Algorithmic Information Theory, especially the Kolmogorov complexity theory.</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/1973-01647-001" rel="noopener noreferrer">Complexity and the representation of patterned sequences of symbols</a> - <em><strong>Psychological Review</strong></em>, 1972. [<a href="https://scholar.google.com/scholar?cluster=3426861135318645138" rel="noopener noreferrer">All Versions</a>]. Herbert Simon's review on subjective complexity.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/1057698" rel="noopener noreferrer">Visual Pattern Discrimination</a> - <em><strong>IRE Transactions on Information Theory</strong></em>, 1962. [<a href="https://scholar.google.com/scholar?cluster=10729525966103382864" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5390997" rel="noopener noreferrer">Algorithmic Information Theory</a> - <em><strong>IBM Journal of Research and Development</strong></em>, 1977. [<a href="https://scholar.google.com/scholar?cluster=14735710867906424793" rel="noopener noreferrer">All Versions</a>]. Chaitin's original paper on Algorithmic Information Theory.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2003/hash/b06b5541a62ed438f956b662b4e1ec28-Abstract.html" rel="noopener noreferrer">From Algorithmic to Subjective Randomness</a> - <em><strong>NeurIPS'03</strong></em>, 2003. [<a href="https://scholar.google.com/scholar?cluster=14721764738308036578" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.mlr.press/v202/shi23i.html" rel="noopener noreferrer">On the Complexity of Bayesian Generalization</a> - <em><strong>ICML'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=5817813824878811147" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/SHI-Yu-Zhe/bayesian-generalization-complexity" rel="noopener noreferrer">Project (⭐8)</a>]. [<a href="https://drive.google.com/file/d/1eCuFqBYN8kuiAmoVtXWedXW0r0TdY55W/view" rel="noopener noreferrer">Models</a>]. This work examines concept generalization at a large scale in the natural visual spectrum. Established computational modes (i.e., rule-based or similarity-based) are primarily studied isolated, focusing on confined and abstract problem spaces. This work studies these two modes when the problem space scales up and when the complexity of concepts becomes diverse. At the representational level, the authors investigate how the complexity varies when a visual concept is mapped to the representation space. Prior literature has shown that two types of complexities build an inverted-U relation. Leveraging Representativeness of Attribute (RoA), the authors computationally confirm: Models use attributes with high RoA to describe visual concepts, and the description length falls in an inverted-U relation with the increment in visual complexity. At the computational level, the authors examine how the complexity of representation affects the shift between the rule- and similarity-based generalization. The authors hypothesize that category-conditioned visual modeling estimates the co-occurrence frequency between visual and categorical attributes, thus potentially serving as the prior for the natural visual world. Experimental results show that representations with relatively high subjective complexity outperform those with relatively low subjective complexity in rule-based generalization, while the trend is the opposite in similarity-based generalization.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="dimensionality-reduction"><a class="anchor" aria-hidden="true" tabindex="-1" href="#dimensionality-reduction"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Dimensionality Reduction</h4><ul>
<li><p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1084.4695&amp;rep=rep1&amp;type=pdf" rel="noopener noreferrer">A global geometric framework for nonlinear dimensionality reduction</a> - <em><strong>Science</strong></em>, 2000. [<a href="https://scholar.google.com/scholar?cluster=14602426245887619907" rel="noopener noreferrer">All Versions</a>]. The original paper on spectrum clustering.</p>
</li>
<li><p><a href="https://asset-pdf.scinapse.io/prod/2100495367/2100495367.pdf" rel="noopener noreferrer">Reducing the dimensionality of data with neural networks</a> - <em><strong>Science</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=15344645275208957628" rel="noopener noreferrer">All Versions</a>]. The original paper on Variational Autoencoder.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1206.5538.pdf" rel="noopener noreferrer">Representation Learning: A Review and New Perspectives</a> - <em><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=559463397382443088" rel="noopener noreferrer">All Versions</a>]. Yoshua Bengio's review on representation learning.</p>
</li>
<li><p><a href="http://www.stat.ucla.edu/~jxie/personalpage_file/publications/representation_learning_Review.pdf" rel="noopener noreferrer">Representation Learning: A Statistical Perspective</a> - <em><strong>Annual Review of Statistics and Its Application</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=14358027809538175293" rel="noopener noreferrer">All Versions</a>]. Song-Chun Zhu and Ying Nian Wu's review on representation learning, in an account of statistics.</p>
</li>
<li><p><a href="http://robotics.caltech.edu/wiki/images/8/8f/DeepLearningBottleneck.pdf" rel="noopener noreferrer">Deep Learning and the Information Bottleneck Principle</a> - <em><strong>IEEE Information Theory Workshop'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=13152354842433826281" rel="noopener noreferrer">All Versions</a>]. The first paper identifying the problem of information bottleneck in representation learning.</p>
</li>
<li><p><a href="https://artemyk.github.io/assets/pdf/papers/Saxe%20et%20al_2019_On%20the%20information%20bottleneck%20theory%20of%20deep%20learning.pdf" rel="noopener noreferrer">On the information bottleneck theory of deep learning</a> - <em><strong>Journal of Statistical Mechanics: Theory and Experiment</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=12271240925674881982" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="visual-complexity"><a class="anchor" aria-hidden="true" tabindex="-1" href="#visual-complexity"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Visual Complexity</h4><ul>
<li><p><a href="https://www.researchgate.net/profile/Don-Donderi-2/publication/7337589_Visual_Complexity_A_Review/links/5f0875ed45851550509a3a7a/Visual-Complexity-A-Review.pdf" rel="noopener noreferrer">Visual complexity: a review</a> - <em><strong>Psychological Bulletin</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=10747901143387624939" rel="noopener noreferrer">All Versions</a>]. [<a href="https://psycnet.apa.org/record/2006-00818-005" rel="noopener noreferrer">APA</a>]. A psychological account on visual complexity.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0141938205000120" rel="noopener noreferrer">Compressed File Length Predicts Search Time and Errors on Visual Displays</a> - <em><strong>Displays</strong></em>, 2005. [<a href="https://scholar.google.com/scholar?cluster=15600966633648834042" rel="noopener noreferrer">All Versions</a>]. Compressed file size, an objective, easily obtained measure of display complexity, predicts both subjective complexity judgments and objective search performance. It is analogous to algorithmic complexity, a theoretical but impractical measure of bit string complexity. The data suggest that it may be possible to use the compressed file size measure to predict display performance in applied tasks.</p>
</li>
<li><p><a href="https://stefan.winklerbros.net/Publications/qomex2013si.pdf" rel="noopener noreferrer">Image complexity and spatial information</a> - <em><strong>International Workshop on Quality of Multimedia Experience</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=16011036229039693102" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://perception.jhu.edu/files/PDFs/21_Complexity_Speaking/SunFirestone_SpeakingSeeing_2021_JEPG.pdf" rel="noopener noreferrer">Seeing and speaking: How verbal “description length” encodes visual complexity</a> - <em><strong>Journal of Experimental Psychology</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=246820603191585233" rel="noopener noreferrer">All Versions</a>]. [<a href="https://psycnet.apa.org/record/2021-83037-001" rel="noopener noreferrer">APA</a>]. Empirical evidencs showing the relation between visual complexity and description length.</p>
</li>
<li><p><a href="https://pure.mpg.de/rest/items/item_3380375/component/file_3383568/content" rel="noopener noreferrer">How variability shapes learning and generalization</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=10940775338620708972" rel="noopener noreferrer">All Versions</a>]. A comprehensive review on the trade-off between variability and generalization ability.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2205.05666.pdf" rel="noopener noreferrer">Identifying concept libraries from language about object structure</a> - <em><strong>CogSci'22</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=4019205027627496528" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0010027722003158" rel="noopener noreferrer">Show or tell? Exploring when (and why) teaching with language outperforms demonstration</a> - <em><strong>Cognition</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11837154580063293174" rel="noopener noreferrer">All Versions</a>]. The findings of this paper suggest that language communicates complex concepts by directly transmitting abstract rules. In contrast, demonstrations transmit examples, requiring the learner to infer the rules.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="communications"><a class="anchor" aria-hidden="true" tabindex="-1" href="#communications"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Communications</h3><h4 id="non-verbal-communication"><a class="anchor" aria-hidden="true" tabindex="-1" href="#non-verbal-communication"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Non-Verbal Communication</h4><ul>
<li><p><a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1551-6709.2009.01090.x" rel="noopener noreferrer">The Interactive Evolution of Human Communication Systems</a> - <em><strong>Cognitive Science</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=6689941517686043970" rel="noopener noreferrer">All Versions</a>]. Nicolas Fay's original paper on iconicity.</p>
</li>
<li><p><a href="https://benjamins.com/catalog/pc.22.2.05fay" rel="noopener noreferrer">Iconicity: From sign to system in human communication and language</a> - <em><strong>Pragmatics &amp; Cognition</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=8525760321117094567" rel="noopener noreferrer">All Versions</a>]. This paper explores the role of iconicity in spoken language and other human communication systems.</p>
</li>
<li><p><a href="https://journals.sagepub.com/doi/abs/10.1177/108835769400900301" rel="noopener noreferrer">The Picture Exchange Communication System</a> - <em><strong>Behavior Modification</strong></em>, 1994. [<a href="https://scholar.google.com/scholar?cluster=18113491434570143349&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/full/10.1080/15326900701221363" rel="noopener noreferrer">Graphical Language Games: Interactional Constraints on Representational Form</a> - <em><strong>Cognitive Science</strong></em>, 2007. [<a href="https://scholar.google.com/scholar?cluster=280214578402050136&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The first paper introducing the graphical language game.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0378216614001830" rel="noopener noreferrer">A multimodal discourse theory of visual narrative</a> - <em><strong>Journal of Pragmatics</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=912273653379961242&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ayankumarbhunia.github.io/pixelor/image/pixelor.pdf" rel="noopener noreferrer">Pixelor: A Competitive Sketching AI Agent. So you think you can beat me?</a> - <em><strong>ACM SIGGRAPH'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=6676723059377806081&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="http://sketchx.ai/pixelor" rel="noopener noreferrer">Project</a>]. Rationality in feature sketching.</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s42113-019-00058-7" rel="noopener noreferrer">Pragmatic Inference and Visual Abstraction Enable Contextual Flexibility During Visual Communication</a> - <em><strong>Computational Brain &amp; Behavior</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=17971107104483505071&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A computational account on the rational behavior in graphical language games.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/550ff553efc2c58410f277c667d12786-Abstract-Conference.html" rel="noopener noreferrer">Emergent Graphical Conventions in a Visual Communication Game</a> - <em><strong>NeurIPS</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=17122070906194572150" rel="noopener noreferrer">All Versions</a>]. A computational account on the emergence of iconic language.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/3610591.3616427" rel="noopener noreferrer">AI Nüshu: An Exploration of Language Emergence in Sisterhood Through the Lens of Computational Linguistics</a> - <em><strong>ACM SIGGRAPH Asia'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=6849286654402017109&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. By continually observing their environment and communicating, AI agents trained in the Chinese dictionary and the Nüshu corpus collaborate towards creating a standard writing system to encode Chinese.</p>
</li>
<li><p><a href="https://www.pnas.org/content/118/12/e2016569118" rel="noopener noreferrer">Communicating artificial neural networks develop efficient color-naming systems</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=1640459156303560508&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Simulating the emergence of code as the communication bottleneck in color learning task.</p>
</li>
<li><p><a href="https://escholarship.org/content/qt9p70d5s9/qt9p70d5s9.pdf" rel="noopener noreferrer">Bridging cultural and cognitive perspectives on similarity reasoning</a> - <em><strong>CogSci'22</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Bridging+cultural+and+cognitive+perspectives+on+similarity+reasoning&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.eva.mpg.de/documents/Elsevier/Liszkowski_Twelve_Cognition_2008_1554509.pdf" rel="noopener noreferrer">Twelve-month-olds communicate helpfully and appropriately for knowledgeable and ignorant partners</a> - <em><strong>Cognition</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=8202048572661677635&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on child pointing.</p>
</li>
<li><p><a href="https://pure.mpg.de/rest/items/item_64467_4/component/file_64468/content" rel="noopener noreferrer">12- and 18-Month-Olds Point to Provide Information for Others</a> - <em><strong>Journal of Cognition and Development</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=7322772656439413984&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s10115-006-0062-2" rel="noopener noreferrer">Toward understanding the importance of gesture in distributed scientific collaboration</a> - <em><strong>Knowledge and Information Systems</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=3145646721897130511" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="pragmatics"><a class="anchor" aria-hidden="true" tabindex="-1" href="#pragmatics"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pragmatics</h4><ul>
<li><p><a href="https://plato.stanford.edu/entries/pragmatics/" rel="noopener noreferrer">Pragmatics</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account of Pragmatics, whilch studies utterances in specific contexts.</p>
</li>
<li><p><a href="https://www.science.org/doi/abs/10.1126/science.1218633" rel="noopener noreferrer">Predicting Pragmatic Reasoning in Language Games</a> - <em><strong>Science</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=15533081031935746054" rel="noopener noreferrer">All Versions</a>]. [<a href="https://langcog.stanford.edu/papers_new/frank-2012-science.pdf" rel="noopener noreferrer">Preprint</a>]. One of the most astonishing features of human language is its capacity to convey information efficiently in context. Many theories provide informal accounts of communicative inference, yet there have been few successes in making precise, quantitative predictions about pragmatic reasoning. This work examined judgments about simple referential communication games, modeling behavior in these games by assuming that speakers attempt to be informative and that listeners use Bayesian inference to recover speakers’ intended referents. The model provides a close, parameter-free fit to human judgments, suggesting that the use of information-theoretic tools to predict pragmatic reasoning may lead to more effective formal models of communication.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S136466131630122X" rel="noopener noreferrer">Pragmatic Language Interpretation as Probabilistic Inference</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=11393505968563356130" rel="noopener noreferrer">All Versions</a>]. Understanding language requires more than the use of fixed conventions and more than decoding combinatorial structure. Instead, comprehenders make exquisitely sensitive inferences about what utterances mean given their knowledge of the speaker, language, and context. Building on developments in game theory and probabilistic modeling, the authors describe the rational speech act (RSA) framework for pragmatic reasoning. RSA models provide a principled way to formalize inferences about meaning in context; they have been used to make successful quantitative predictions about human behavior in a variety of different tasks and situations, and they explain why complex phenomena, such as hyperbole and vagueness, occur. More generally, they provide a computational framework for integrating linguistic structure, world knowledge, and context in pragmatic language understanding.</p>
</li>
<li><p><a href="http://cocolab.stanford.edu/papers/BergenLevyGoodman-LexUnc.pdf" rel="noopener noreferrer">Pragmatic Reasoning through Semantic Inference</a> - <em><strong>Semantics &amp; Pragmatics</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=1433855075217315997" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://semantics.uchicago.edu/kennedy/docs/processing.pdf" rel="noopener noreferrer">Processing gradable adjectives in context: A visual world study</a> - <em><strong>Semantics and Linguistic Theory</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=13426776838629402579" rel="noopener noreferrer">All Versions</a>]. Adjective understanding as a rational inference in the context.</p>
</li>
<li><p><a href="https://transacl.org/index.php/tacl/article/view/1142" rel="noopener noreferrer">Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding</a> - <em><strong>Transactions of the Association for Computational Linguistics</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=11119271811833503059" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://compdevlab.yale.edu/docs/2019/2019_ChildDev_Pragmatics.pdf" rel="noopener noreferrer">Social Pragmatics: Preschoolers Rely on Commonsense Psychology to Resolve Referential Underspecification</a> - <em><strong>Child Development</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=16352913537004112920" rel="noopener noreferrer">All Versions</a>]. A piece of evidence for children's capability on social pragmatics.</p>
</li>
<li><p><a href="http://cocolab.stanford.edu/papers/CohnGordonEtAl2018_NAACL.pdf" rel="noopener noreferrer">Pragmatically Informative Image Captioning with Character-Level Inference</a> - <em><strong>NAACL'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=1670953084401884599" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://aclanthology.org/2020.findings-emnlp.173/" rel="noopener noreferrer">Pragmatic Issue-Sensitive Image Captioning</a> - <em><strong>EMNLP Findings'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=10608257248144445301" rel="noopener noreferrer">All Versions</a>]. Application of Rational Speech Act to Image Captioning.</p>
</li>
<li><p><a href="https://cogsci.mindmodeling.org/2019/papers/0091/0091.pdf" rel="noopener noreferrer">Disentangling contributions of visual information and interaction history in the formation of graphical conventions</a> - <em><strong>CogSci'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15046353579508199394&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41562-021-01145-1" rel="noopener noreferrer">How young children integrate information sources to infer the meaning of words</a> - <em><strong>Nature Human Behavior</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=10144794357802769844" rel="noopener noreferrer">All Versions</a>]. Before formal education begins, children typically acquire a vocabulary of thousands of words. This learning process requires the use of many different information sources in their social environment, including their current state of knowledge and the context in which they hear words used. This paper specifies a developmental model according to which children consider information sources in an age-specific way and integrate them via Bayesian inference. This work presents a developmental theory of information integration during language learning and illustrates how formal models can be used to make a quantitative test of the predictive and explanatory power of competing theories.</p>
</li>
<li><p><a href="https://semprag.org/index.php/sp/article/view/sp.5.6/pdf" rel="noopener noreferrer">Information Structure in Discourse: Towards an Integrated Formal Theory of Pragmatics</a> - <em><strong>Semantics and Pragmatics</strong></em>, 1998. [<a href="https://scholar.google.com/scholar?cluster=9127222314768938599&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s11098-020-01490-3" rel="noopener noreferrer">When Lingens meets Frege: communication without common ground</a> - <em><strong>Philosophical Studies</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=10912415595149303257&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2307.07871" rel="noopener noreferrer">The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents</a> - <em><strong>ICML'23 Workshop on Theory-of-Mind</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11933410239580707313&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://sites.google.com/view/socialai-school" rel="noopener noreferrer">Project</a>].</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41599-020-0404-9" rel="noopener noreferrer">Language as shaped by the environment: linguistic construal in a collaborative spatial task</a> - <em><strong>Humanities and Social Sciences Communications</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7842508027049437987" rel="noopener noreferrer">All Versions</a>]. [<a href="https://osf.io/sxtaq" rel="noopener noreferrer">Code &amp; Data</a>]. [<a href="https://dialoguetoolkit.github.io/chattool/" rel="noopener noreferrer">Dialogue Experimental Toolkit(DiET)</a>]. The present study sets out to experimentally investigate how environmental factors come to shape the emergence of linguistic conventions. To this end, the authors adapt the classical Maze Game task to test the hypothesis that participants routinise different linguistic strategies to communicate positions in the maze contingent on particular environmental affordances (i.e. structure of the mazes). The results confirm that subtle environmental motivations drive the emergence of different communicative conventions in an otherwise identical task, suggesting that linguistic adaptations are highly sensitive to factors of the shared task environment.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2008.12142" rel="noopener noreferrer">Exploring Urban Form Through Openstreetmap Data: A Visual Introduction</a> - <em><strong>Urban Experience and Design: Contemporary Perspectives on Improving the Public Realm</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7094530618542001733&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/gboeing/osmnx" rel="noopener noreferrer">OSMnx Tool (⭐5.2k)</a>]. [<a href="https://www.openstreetmap.org/" rel="noopener noreferrer">OpenStreetMap Website</a>].</p>
</li>
<li><p><a href="https://www.speech.kth.se/~edlund/bielefeld/references/garrod-and-anderson-1987.pdf" rel="noopener noreferrer">Saying what you mean in dialogue: A study in conceptual and semantic co-ordination</a> - <em><strong>Cognition</strong></em>, 1987. [<a href="https://scholar.google.com/scholar?cluster=15377075954534820544&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://www.sfs.uni-tuebingen.de/~gjaeger/lehre/ws0708/spieltheorie/garrod.pdf" rel="noopener noreferrer">Conversation, co-ordination and convention: an empirical investigation of how groups establish linguistic conventions</a> - <em><strong>Cognition</strong></em>, 1994. [<a href="https://scholar.google.com/scholar?cluster=3784850469297049700&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="language-compositionality"><a class="anchor" aria-hidden="true" tabindex="-1" href="#language-compositionality"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Language Compositionality</h4><ul>
<li><p><a href="https://plato.stanford.edu/entries/compositionality/" rel="noopener noreferrer">Compositionality</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on compositionality, one of the distinctive feature of language.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-024-07522-w" rel="noopener noreferrer">Language is primarily a tool for communication rather than thought</a> - <em><strong>Nature</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=13724799649075764503" rel="noopener noreferrer">All Versions</a>]. This perspective brings recent evidence from neuroscience and allied disciplines to argue that in modern humans, language is a tool for communication, contrary to a prominent view that we use language for thinking. The authors begins by introducing the brain network that supports linguistic ability in humans. They then review evidence for a double dissociation between language and thought, and discuss several properties of language that suggest that it is optimized for communication. This perspective concludes that although the emergence of language has unquestionably transformed human culture, language does not appear to be a prerequisite for complex thought, including symbolic thought. Instead, language is a powerful tool for the transmission of cultural knowledge; it plausibly co-evolved with humans' thinking and reasoning capacities, and only reflects, rather than gives rise to, the signature sophistication of human cognition.</p>
</li>
<li><p><a href="https://link.springer.com/content/pdf/10.1007/BF00763644.pdf" rel="noopener noreferrer">The Principle of Semantic Compositionality</a> - <em><strong>Topoi</strong></em>, 1994. [<a href="https://scholar.google.com/scholar?cluster=10899040818001759322&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on the principle of semantic compositionality.</p>
</li>
<li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.3235" rel="noopener noreferrer">On The Emergence Of Compositionality</a> - <em><strong>Proceedings of the Evolution of Language Conference'06</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=16315741180717951222&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on the emergence of compositionality.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1612.07182.pdf" rel="noopener noreferrer">Multi-Agent Cooperation and the Emergence of (Natural) Language</a> - <em><strong>ICLR'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=1931070702879918446&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on the emergence of language in multi-agent reinforcement learning.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2017/hash/70222949cc0db89ab32c9969754d4758-Abstract.html" rel="noopener noreferrer">Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols</a> - <em><strong>NeurIPS'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=17308624474306270808&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/1804.03980" rel="noopener noreferrer">Emergent communication through negotiation</a> - <em><strong>ICLR'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8825869866742501521&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/2019-07481-001" rel="noopener noreferrer">The language of generalization</a> - <em><strong>Psychological Review</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7723877614160376324&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2004.09124" rel="noopener noreferrer">Compositionality and Generalization in Emergent Languages</a> - <em><strong>ACL'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5792073344743965767&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://escholarship.org/uc/item/5kv636c5" rel="noopener noreferrer">Word formation supports efficient communication: The case of compounds</a> - <em><strong>CogSci'22</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=17465553221758916299&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2311.17227" rel="noopener noreferrer">War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=3598519753107761968&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="coordination"><a class="anchor" aria-hidden="true" tabindex="-1" href="#coordination"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Coordination</h4><ul>
<li><p><a href="https://www.science.org/doi/full/10.1126/scirobotics.abm4183" rel="noopener noreferrer">In situ bidirectional human-robot value alignment</a> - <em><strong>Science Robotics</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=18342339995965564405" rel="noopener noreferrer">All Versions</a>]. [<a href="https://par.nsf.gov/servlets/purl/10351399" rel="noopener noreferrer">Preprint</a>]. This paper proposes an explainable artificial intelligence (XAI) system in which a group of robots predicts users’ values by taking in situ feedback into consideration while communicating their decision processes to users through explanations. To learn from human feedback, the XAI system integrates a cooperative communication model for inferring human values associated with multiple desirable goals. To be interpretable to humans, it simulates human mental dynamics and predicts optimal explanations using graphical models.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2304.14656.pdf" rel="noopener noreferrer">From Explicit Communication to Tacit Cooperation: A Novel Paradigm for Cooperative MARL</a> - <em><strong>AAMAS'24</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=12114270828108588849" rel="noopener noreferrer">All Versions</a>]. Drawing inspiration from human team cooperative learning, this paper proposes a novel paradigm that facilitates a gradual shift from explicit communication to tacit cooperation.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="domain-specific-language"><a class="anchor" aria-hidden="true" tabindex="-1" href="#domain-specific-language"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Domain Specific Language</h3><h4 id="design-theory"><a class="anchor" aria-hidden="true" tabindex="-1" href="#design-theory"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Design Theory</h4><ul>
<li><p><a href="https://en.wikipedia.org/wiki/Domain-specific_language" rel="noopener noreferrer">Domain-Specific Language</a> - <em><strong>Wikipedia</strong></em>. Wikipedia encyclopedia entry on Domain Specific Languages.</p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/Domain_engineering" rel="noopener noreferrer">Domain Engineering</a> - <em><strong>Wikipedia</strong></em>. Wikipedia encyclopedia entry on Domain Engineering.</p>
</li>
<li><p><a href="https://martinfowler.com/books/dsl.html" rel="noopener noreferrer">Domain-Specific Languages</a> - <em><strong>Pearson Education</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=3653365103385845410" rel="noopener noreferrer">All Versions</a>]. [<a href="https://martinfowler.com/dsl.html" rel="noopener noreferrer">Domain-Specific Languages Guide</a>]. When carefully selected and used, Domain-Specific Languages (DSLs) may simplify complex code, promote effective communication with customers, improve productivity, and unclog development bottlenecks. In Domain-Specific Languages, noted software development expert Martin Fowler first provides the information software professionals need to decide if and when to utilize DSLs. Then, where DSLs prove suitable, Fowler presents effective techniques for building them, and guides software engineers in choosing the right approaches for their applications.</p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/Comparison_of_multi-paradigm_programming_languages" rel="noopener noreferrer">Comparison of multi-paradigm programming languages</a> - <em><strong>Wikipedia</strong></em>. Programming languages may support multiple programming paradigms. This Wikipedia encyclopedia entry lists a concise reference for the programming paradigms.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/pdf/10.1145/947955.1083808" rel="noopener noreferrer">Epigrams on programming</a> - <em><strong>ACM SIGPLAN Notices</strong></em>, 1982. [<a href="https://scholar.google.com/scholar?cluster=6439127299132936476" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://tomassetti.me/domain-specific-languages/" rel="noopener noreferrer">The complete guide to (external) Domain Specific Languages</a>. An introduction to Domain Specific Languages (DSL) based on 19 DSL cases.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/1118890.1118892" rel="noopener noreferrer">When and How to Develop Domain-Specific Languages</a> - <em><strong>ACM Computing Surveys</strong></em>, 2005. [<a href="https://scholar.google.com/scholar?cluster=8598236436890577027" rel="noopener noreferrer">All Versions</a>]. [<a href="https://people.cs.ksu.edu/~schmidt/505f14/Lectures/WhenDSL.pdf" rel="noopener noreferrer">Preprint</a>]. Domain-specific languages (DSLs) are languages tailored to a specific application domain. They offer substantial gains in expressiveness and ease of use compared with general-purpose programming languages in their domain of application. DSL development is hard, requiring both domain knowledge and language development expertise. Few people have both. Not surprisingly, the decision to develop a DSL is often postponed indefinitely, if considered at all, and most DSLs never get beyond the application library stage. Although many articles have been written on the development of particular DSLs, there is very limited literature on DSL development methodologies and many questions remain regarding when and how to develop a DSL. To aid the DSL developer, this survey paper identifies patterns in the decision, analysis, design, and implementation phases of DSL development. These patterns improve and extend earlier work on DSL design patterns.</p>
</li>
<li><p><a href="https://arxiv.org/abs/1409.2378" rel="noopener noreferrer">Design Guidelines for Domain Specific Languages</a> - <em><strong>OOPSLA Workshop on Domain-Specific Modeling (DSM' 09)</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=1962567819031018744" rel="noopener noreferrer">All Versions</a>]. Designing a new domain specific language is as any other complex task sometimes error-prone and usually time consuming, especially if the language shall be of high-quality and comfortably usable. Existing tool support focuses on the simplification of technical aspects but lacks support for an enforcement of principles for a good language design. In this paper we investigate guidelines that are useful for designing domain specific languages, largely based on our experience in developing languages as well as relying on existing guidelines on general purpose (GPLs) and modeling languages. This work defined Guidelines to support a DSL developer to achieve better quality of the language design and a better acceptance among its users.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/352029.352035" rel="noopener noreferrer">Domain-specific languages: an annotated bibliography</a> - <em><strong>ACM SIGPLAN Notices</strong></em>, 2000. [<a href="https://scholar.google.com/scholar?cluster=8845429548327315750" rel="noopener noreferrer">All Versions</a>]. A survey on the topic of domain-specific languages as used for the construction and maintenance of software systems. The survey lists a selection of 75 key publications in the area, and provides a summary for each of the papers. Moreover, the survey discusses terminology, risks and benefits, example domain-specific languages, design methodologies, and implementation techniques.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/6511840" rel="noopener noreferrer">Usability Evaluation of Domain-Specific Languages</a> - <em><strong>ICQICT'12</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=3047215455890195199" rel="noopener noreferrer">All Versions</a>]. [<a href="http://www-ctp.di.fct.unl.pt/QUASAR/Resources/Papers/2012/Barisic2012SEDES.pdf" rel="noopener noreferrer">Preprint</a>]. The purpose of this proposal is to contribute to the systematic activity of Software Language Engineering by focusing on the issue of the Usability evaluation of DSLs. Usability evaluation is often skipped, relaxed, or at least omitted from papers reporting development of DSLs. The authors argue that a systematic approach based on User Interface experimental validation techniques should be used to assess the impact of new DSLs. For that purpose, the authors propose to merge common Usability evaluation processes with the DSL development process.</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-3-642-36654-3_6" rel="noopener noreferrer">Domain-Specific Modeling Languages: Requirements Analysis and Design Guidelines</a> - <em><strong>Domain Engineering: Product Lines, Languages, and Conceptual Models</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=15620404537599157753" rel="noopener noreferrer">All Versions</a>]. In recent years, the development of domain-specific modeling languages has gained remarkable attention. This is for good reasons. A domain-specific modeling language incorporates concepts that represent domain-level knowledge. Hence, systems analysts are not forced to reconstruct these concepts from scratch. At the same time, domain-specific modeling languages contribute to model integrity, because they include already constraints that would otherwise have to be added manually. Even though there has been a considerable amount of research on developing and using domain-specific modeling languages, there is still lack of comprehensive methods to guide the design of these languages. With respect to the complexity and risk related to developing a domain-specific modeling language, this is a serious shortfall. This chapter is aimed at a contribution to filling the gap. At first, it presents guidelines for selecting a metamodeling language. Its main focus is on supporting the process from analyzing requirements to specifying and evaluating a domain-specific modeling language.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="design-practises"><a class="anchor" aria-hidden="true" tabindex="-1" href="#design-practises"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Design Practises</h4><ul>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0164121214002799" rel="noopener noreferrer">Quantifying usability of domain-specific languages: An empirical study on software maintenance</a> - <em><strong>Journal of Systems and Software</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=3450893039446010260" rel="noopener noreferrer">All Versions</a>]. A DSL aims to support software development by offering abstractions to a particular domain. It is expected that DSLs improve the maintainability of artifacts otherwise produced with general-purpose languages. However, the maintainability of the DSL artifacts and, hence, their adoption in mainstream development, is largely dependent on the usability of the language itself. Unfortunately, it is often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. Usability is a multi-faceted quality characteristic, which is challenging to quantify beforehand by DSL stakeholders. There is even less support on how to quantitatively evaluate the usability of DSLs used in maintenance tasks. In this context, this paper reports a study to compare the usability of textual DSLs under the perspective of software maintenance. A usability measurement framework was developed based on the cognitive dimensions of notations. The framework was evaluated both qualitatively and quantitatively using two DSLs in the context of two evolving object-oriented systems. The results suggested that the proposed metrics were useful: (1) to early identify DSL usability limitations, (2) to reveal specific DSL features favoring maintenance tasks, and (3) to successfully analyze eight critical DSL usability dimensions.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/2685028" rel="noopener noreferrer">A Taxonomy of Domain-Specific Aspect Languages</a> - <em><strong>ACM Computing Surveys</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=17254174131160041640" rel="noopener noreferrer">All Versions</a>]. Domain-Specific Aspect Languages (DSALs) are Domain-Specific Languages (DSLs) designed to express crosscutting concerns. Compared to DSLs, their aspectual nature greatly amplifies the language design space. This survey structures this space in order to shed light on and compare the different domain-specific approaches to deal with crosscutting concerns. This survey reports on a corpus of 36 DSALs covering the space, discuss a set of design considerations, and provide a taxonomy of DSAL implementation approaches. This work serves as a frame of reference to DSAL and DSL researchers, enabling further advances in the field, and to developers as a guide for DSAL implementations.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9904438" rel="noopener noreferrer">No Grammar to Rule Them All: A Survey of JSON-style DSLs for Visualization</a> - <em><strong>IEEE Transactions on Visualization and Computer Graphics</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=17206818917381447796" rel="noopener noreferrer">All Versions</a>]. There has been substantial growth in the use of JSON-based grammars, as well as other standard data serialization languages, to create visualizations. Each of these grammars serves a purpose: some focus on particular computational tasks (such as animation), some are concerned with certain chart types (such as maps), and some target specific data domains (such as ML). Despite the prominence of this interface form, there has been little detailed analysis of the characteristics of these languages. This study surveys and analyzes the design and implementation of 57 JSON-style DSLs for visualization. The authors analyze these languages supported by a collected corpus of examples for each DSL (consisting of 4395 instances) across a variety of axes organized into concerns related to domain, conceptual model, language relationships, affordances, and general practicalities. The authors identify tensions throughout these areas, such as between formal and colloquial specifications, among types of users, and within the composition of languages. Through this work, the authors seek to support language implementers by elucidating the choices, opportunities, and tradeoffs in visualization DSL design.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/3622851" rel="noopener noreferrer">How Domain Experts Use an Embedded DSL</a> - <em><strong>OOPSLA'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=8416124186663074528" rel="noopener noreferrer">All Versions</a>]. Programming tools are increasingly integral to research and analysis in myriad domains, including specialized areas with no formal relation to computer science. Embedded domain-specific languages (eDSLs) have the potential to serve these programmers while placing relatively light implementation burdens on language designers. However, barriers to eDSL use reduce their practical value and adoption. This work aims to deepen the understanding of how programmers use eDSLs and identify user needs to inform future eDSL designs. The authors performed a contextual inquiry (9 participants) with domain experts using Mimi, an eDSL for climate change economics modeling. A thematic analysis identified five key themes, including: the interaction between the eDSL and the host language has significant and sometimes unexpected impacts on eDSL user experience, and users preferentially engage with domain-specific communities and code templates rather than host language resources.</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-981-96-0780-8_9" rel="noopener noreferrer">Abstract Hardware Grounding Towards the Automated Design of Automation Systems</a> - <em><strong>ICIRA'24</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=3331524500088540378" rel="noopener noreferrer">All Versions</a>]. [<a href="https://arxiv.org/abs/2410.05663" rel="noopener noreferrer">Preprint</a>]. Crafting automation systems tailored for specific domains requires aligning the space of human experts’ semantics with the space of robot executable actions, and scheduling the required resources and system layout accordingly. Regrettably, there are three major gaps, fine-grained domain-specific knowledge injection, heterogeneity between human knowledge and robot instructions, and diversity of users’ preferences, resulting automation system design a case-by-case and labour-intensive effort, thus hindering the democratization of automation. This work refers to this challenging alignment as the abstract hardware grounding problem, where the authors firstly regard the procedural operations in humans’ semantics space as the abstraction of hardware requirements, then the authors ground such abstractions to instantiated hardware devices, subject to constraints and preferences in the real world—optimizing this problem is essentially standardizing and automating the design of automation systems. On this basis, this work develops an automated design framework in a hybrid data-driven and principle-derived fashion. Results on designing self-driving laboratories for enhancing experiment-driven scientific discovery suggest the proposed framework’s potential to produce compact systems that fully satisfy domain-specific and user-customized requirements with no redundancy.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/10766486" rel="noopener noreferrer">Constraint Representation Towards Precise Data-Driven Storytelling</a> - <em><strong>VIS-Gen4DS'24</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=12234019078719898658" rel="noopener noreferrer">All Versions</a>]. [<a href="https://arxiv.org/abs/2410.07535" rel="noopener noreferrer">Preprint</a>]. A position paper on DSL for data-driven storytelling. Data-driven storytelling serves as a crucial bridge for communicating ideas in a persuasive way. However, the manual creation of data stories is a multifaceted, labor-intensive, and case-specific effort, limiting their broader application. As a result, automating the creation of data stories has emerged as a significant research thrust. Despite advances in Artificial Intelligence, the systematic generation of data stories remains challenging due to their hybrid nature: they must frame a perspective based on a seed idea in a top-down manner, similar to traditional storytelling, while coherently grounding insights of given evidence in a bottom-up fashion, akin to data analysis. These dual requirements necessitate precise constraints on the permissible space of a data story. This viewpoint proposes integrating constraints into the data story generation process. Defined upon the hierarchies of interpretation and articulation, constraints shape both narrations and illustrations to align with seed ideas and contextualized evidence. The authors identify the taxonomy and required functionalities of these constraints. Although constraints can be heterogeneous and latent, this position paper explores the potential to represent them in a computation-friendly fashion via Domain-Specific Languages. The authors believe that leveraging constraints will facilitate both artistic and scientific aspects of data story generation.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s44160-024-00649-8" rel="noopener noreferrer">Reproducibility in automated chemistry laboratories using computer science abstractions</a> - <em><strong>Nature Synthesis</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=2583939834455194329" rel="noopener noreferrer">All Versions</a>]. While abstraction is critical for the transferability of automated laboratory science in (bio)chemical and materials sciences, its improper implementation is a technical debt taken against the reproducibility of experimental results. Over the decades, computer science has developed guidelines and strategies for how abstractions are captured in programming languages---particularly concerning the substitutability of implementations of abstracted ideas and the clear definition of the contexts in which abstractions are used. However, few programming languages developed for automated experiments fully leverage the wisdom learned in computer science. To achieve collaborative sharing of scientific knowledge via automated laboratories, the way that experimental protocols are codified and interpreted by machine agents must use abstractions responsibly and with reproducibility, rather than solely transferability, at its core. This Review discusses how computer science principles of abstraction can be translated to create more reproducible automation as an enabler for the acceleration of collaborative research with self-driving laboratories.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="design-automation"><a class="anchor" aria-hidden="true" tabindex="-1" href="#design-automation"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Design Automation</h4><ul>
<li><p><a href="https://aclanthology.org/2024.acl-long.659/" rel="noopener noreferrer">AutoDSL: Automated domain-specific language design for structural representation of procedures with constraints</a> - <em><strong>ACL'24</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=588082932830954126" rel="noopener noreferrer">All Versions</a>]. [<a href="https://arxiv.org/abs/2406.12324" rel="noopener noreferrer">Preprint</a>]. [<a href="https://autodsl.org/procedure/papers/acl24shi.html" rel="noopener noreferrer">Project</a>]. The original paper on the automated design of DSLs, referred to as AutoDSL. Accurate representation of procedures in restricted scenarios, such as non-standardized scientific experiments, requires precise depiction of constraints. Unfortunately, Domain-Specific Language (DSL), as an effective tool to express constraints structurally, often requires case-by-case hand-crafting, necessitating customized, labor-intensive efforts. To overcome this challenge, this paper introduces the AutoDSL framework to automate DSL-based constraint design across various domains. Utilizing domain specified experimental protocol corpora, AutoDSL optimizes syntactic constraints and abstracts semantic constraints. Quantitative and qualitative analyses of the DSLs designed by AutoDSL across five distinct domains highlight its potential as an auxiliary module for language models, aiming to improve procedural planning and execution.</p>
</li>
<li><p><a href="https://openreview.net/forum?id=9nUBh4V6SA" rel="noopener noreferrer">Hierarchically Encapsulated Representation for Protocol Design in Self-Driving Labs</a> - <em><strong>ICLR'25</strong></em>, 2025. [<a href="https://scholar.google.com/scholar?cluster=6090102857833092474" rel="noopener noreferrer">All Versions</a>]. [<a href="https://autodsl.org/procedure/papers/iclr25shi.html" rel="noopener noreferrer">Project</a>]. Self-driving laboratories have begun to replace human experimenters in performing single experimental skills or predetermined experimental protocols. However, as the pace of idea iteration in scientific research has been intensified by Artificial Intelligence, the demand for rapid design of new protocols for new discoveries become evident. Efforts to automate protocol design have been initiated, but the capabilities of knowledge-based machine designers, such as Large Language Models, have not been fully elicited, probably for the absence of a systematic representation of experimental knowledge, as opposed to isolated, flatten pieces of information. To tackle this issue, this work proposes a multi-faceted, multi-scale representation, where instance actions, generalized operations, and product flow models are hierarchically encapsulated using Domain-Specific Languages. The authors further develop a data-driven algorithm based on non-parametric modeling that autonomously customizes these representations for specific domains. The proposed representation is equipped with various machine designers to manage protocol design tasks, including planning, modification, and adjustment. The results demonstrate that the proposed method could effectively complement Large Language Models in the protocol design process, serving as an auxiliary module in the realm of machine-assisted scientific exploration.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="imperative-dsl-applications"><a class="anchor" aria-hidden="true" tabindex="-1" href="#imperative-dsl-applications"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Imperative DSL Applications</h4><ul>
<li><p><a href="https://www.science.org/doi/full/10.1126/science.aav2211" rel="noopener noreferrer">Organic synthesis in a modular robotic system driven by a chemical programming language</a> - <em><strong>Science</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=13920677955690815682" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.chem.gla.ac.uk/cronin/images/pubs/387-Steiner-ScienceJan19.full.pdf" rel="noopener noreferrer">Preprint</a>]. [<a href="https://www.science.org/doi/10.1126/science.aav8816" rel="noopener noreferrer">Perspective: Democratizing synthesis by automation</a>]. This paper develops an autonomous compiler and robotic laboratory platform to synthesize organic compounds on the basis of standardized methods descriptions. The platform comprises conventional equipment such as round-bottom flasks, separatory funnels, and a rotary evaporator to maximize its compatibility with extant literature. The authors showcase the system with short syntheses of three common pharmaceuticals that proceeded comparably to manual synthesis.</p>
</li>
<li><p><a href="https://jbioleng.biomedcentral.com/track/pdf/10.1186/1754-1611-4-13.pdf" rel="noopener noreferrer">Biocoder: A programming language for standardizing and automating biology protocols</a> - <em><strong>Journal of Biological Engineering</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?start=0&amp;hl=en&amp;as_sdt=0,5&amp;cluster=15572197190838916795" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/nmz787/BioCoder" rel="noopener noreferrer">Project (⭐1)</a>]. [<a href="https://www.microsoft.com/en-us/download/details.aspx?id=52556" rel="noopener noreferrer">Microsoft Page</a>] This paper introduces BioCoder, a C++ library that enables biologists to express the exact steps needed to execute a protocol. In addition to being suitable for automation, BioCoder converts the code into a readable, English-language description for use by biologists.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s44160-023-00473-6" rel="noopener noreferrer">Universal chemical programming language for robotic synthesis repeatability</a> - <em><strong>Nature Synthesis</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=3455106495990439366" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.chem.gla.ac.uk/cronin/images/pubs/rauschen-natsynthesisjan24.pdf" rel="noopener noreferrer">Preprint</a>]. This paper presents an approach that uses a universal chemical programming language (χDL) to encode and execute synthesis procedures for a variety of chemical reactions, including reductive amination, ring formation, esterification, carbon–carbon bond formation and amide coupling on four different hardware systems in two laboratories. With around 50 lines of code per reaction, the approach uses abstraction to efficiently compress chemical protocols.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/full/10.1145/3604568" rel="noopener noreferrer">Building an Open Representation for Biological Protocols</a> - <em><strong>ACM Journal on Emerging Technologies in Computing Systems</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=17225405546647782000" rel="noopener noreferrer">All Versions</a>]. Laboratory protocols are critical to biological research and development, yet difficult to communicate and reproduce across projects, investigators, and organizations. While many attempts have been made to address this challenge, there is currently no available protocol representation that is unambiguous enough for precise interpretation and automation, yet simultaneously “human friendly” and abstract enough to enable reuse and adaptation. The Laboratory Open Protocol language (LabOP) is a free and open protocol representation aiming to address this gap, building on a foundation of UML, Autoprotocol, Aquarium, SBOL RDF, and the Provenance Ontology. LabOP provides a linked-data representation both for protocols and for records of their execution and the resulting data, as well as a framework for exporting from LabOP for execution by either humans or laboratory automation. LabOP is currently implemented in the form of an RDF knowledge representation, specification document, and Python library, and supports execution as manual “paper protocols,” by Autoprotocol or by Opentrons. From this initial implementation, LabOP is being further developed as an open community effort.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/3586183.3606789" rel="noopener noreferrer">KnitScript: A Domain-Specific Scripting Language for Advanced Machine Knitting</a> - <em><strong>UIST'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=KnitScript%3A+A+Domain-Specific+Scripting+Language+for+Advanced+Machine+Knitting&amp;btnG=" rel="noopener noreferrer">All Versions</a>]. [<a href="https://pypi.org/project/knit-script/" rel="noopener noreferrer">Project</a>]. This paper presents KnitScript, a domain-specific machine knitting scripting language that supports computationally driven knitting designs. KnitScript provides a comprehensive virtual model of knitting machines, giving access to machine-level capabilities as they are needed while automating a variety of tedious and error-prone details.</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s11119-020-09770-y" rel="noopener noreferrer">A domain‑specifc language framework for farm management information systems in precision agriculture</a> - <em><strong>Precision Agriculture</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1495954486695213496" rel="noopener noreferrer">All Versions</a>]. This paper proposes a domain-specific language framework for the design and development of precision-agriculture FMISs, which copes with challenges on supporting the understandability, enhancing communication and analysis of the design decisions, and the communication among stakeholders.</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Raistrick_Infinite_Photorealistic_Worlds_Using_Procedural_Generation_CVPR_2023_paper.html" rel="noopener noreferrer">Infinite Photorealistic Worlds Using Procedural Generation</a> - <em><strong>CVPR'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11620922717915489091" rel="noopener noreferrer">All Versions</a>]. [<a href="https://infinigen.org/" rel="noopener noreferrer">Website</a>]. [<a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Raistrick_Infinite_Photorealistic_Worlds_CVPR_2023_supplemental.pdf" rel="noopener noreferrer">Supplementary Text</a>]. This paper introduces Infinigen, a procedural generator of photorealistic 3D scenes of the natural world. Infinigen is entirely procedural: every asset, from shape to texture, is generated from scratch via randomized mathematical rules, using no external source and allowing infinite variation and composition.</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Raistrick_Infinigen_Indoors_Photorealistic_Indoor_Scenes_using_Procedural_Generation_CVPR_2024_paper.html" rel="noopener noreferrer">Infinigen Indoors: Photorealistic Indoor Scenes using Procedural Generation</a> - <em><strong>CVPR'24</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=14526967027465419958" rel="noopener noreferrer">All Versions</a>]. This work introduces Infinigen Indoors, a Blender-based procedural generator of photorealistic indoor scenes. It builds upon the existing Infinigen system, which focuses on natural scenes, but expands its coverage to indoor scenes by introducing a diverse library of procedural indoor assets, including furniture, architecture elements, appliances, and other day-to-day objects. It also introduces a constraint-based arrangement system, which consists of a domain-specific language for expressing diverse constraints on scene composition, and a solver that generates scene compositions that maximally satisfy the constraints. The authors provide an export tool that allows the generated 3D objects and scenes to be directly used for training embodied agents in real-time simulators such as Omniverse and Unreal. Infinigen Indoors is open-sourced under the BSD license.</p>
</li>
<li><p><a href="https://fse.studenttheses.ub.rug.nl/25731/" rel="noopener noreferrer">Corel: A DSL for Cooking Recipes</a> - 2021. [<a href="https://scholar.google.com/scholar?cluster=9477049800574267813" rel="noopener noreferrer">All Versions</a>]. [<a href="https://roorda.dev/recipes/0" rel="noopener noreferrer">Corel recipe page</a>]. [<a href="https://www.fao.org/infoods/infoods/tables-and-databases/faoinfoods-databases/en/" rel="noopener noreferrer">International Network of Food Data Systems (INFOODS)</a>]. The Corel DSL for cooking recipes enables understanding of and computation with ingredients, and can construct a nutrition label for the recipe.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/full/10.1145/3613905.3650756" rel="noopener noreferrer">"We Need Structured Output": Towards User-centered Constraints on Large Language Model Output</a> - <em><strong>CHI EA'24</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=12105435542197416648" rel="noopener noreferrer">All Versions</a>]. [<a href="https://research.google/pubs/we-need-structured-output-towards-user-centered-constraints-on-large-language-model-output/" rel="noopener noreferrer">Preprint</a>]. Large language models can produce creative and diverse responses. However, to integrate them into current developer workflows, it is essential to constrain their outputs to follow specific formats or standards. This work surveyed 51 experienced industry professionals to understand the range of scenarios and motivations driving the need for output constraints from a user-centered perspective. The authors identified 134 concrete use cases for constraints at two levels: low-level, which ensures the output adhere to a structured format and an appropriate length, and high-level, which requires the output to follow semantic and stylistic guidelines without hallucination. Critically, applying output constraints could not only streamline the currently repetitive process of developing, testing, and integrating LLM prompts for developers, but also enhance the user experience of LLM-powered features and applications. The authors conclude with a discussion on user preferences and needs towards articulating intended constraints for LLMs, alongside an initial design for a constraint prototyping tool.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="declarative-dsl-applications"><a class="anchor" aria-hidden="true" tabindex="-1" href="#declarative-dsl-applications"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Declarative DSL Applications</h4><ul>
<li><p><a href="https://www.nature.com/articles/nbt.1666" rel="noopener noreferrer">The BioPAX community standard for pathway data sharing</a> - <em><strong>Nature Biotechnology</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=11368332679628594895" rel="noopener noreferrer">All Versions</a>]. [<a href="https://core.ac.uk/download/pdf/216139091.pdf" rel="noopener noreferrer">Preprint</a>]. Biological Pathway Exchange (BioPAX) is a standard language to represent biological pathways at the molecular and cellular level and to facilitate the exchange of pathway data. BioPAX can represent metabolic and signaling pathways, molecular and genetic interactions and gene regulation networks.</p>
</li>
<li><p><a href="https://www.science.org/doi/full/10.1126/science.abd7331" rel="noopener noreferrer">Learning the language of viral evolution and escape</a> - <em><strong>Science</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=13862653184613223515" rel="noopener noreferrer">All Versions</a>]. The ability for viruses to mutate and evade the human immune system and cause infection, called viral escape, remains an obstacle to antiviral and vaccine development. Understanding the complex rules that govern escape could inform therapeutic design. This work modeled viral escape with machine learning algorithms originally developed for human natural language. The authors identified escape mutations as those that preserve viral infectivity but cause a virus to look different to the immune system, akin to word changes that preserve a sentence’s grammaticality but change its meaning. With this approach, language models of influenza hemagglutinin, HIV-1 envelope glycoprotein (HIV Env), and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) Spike viral proteins can accurately predict structural escape patterns using sequence data alone. This study represents a promising conceptual bridge between natural language and viral evolution.</p>
</li>
<li><p><a href="https://www.biorxiv.org/content/10.1101/2022.12.21.521526v1" rel="noopener noreferrer">A high-level programming language for generative protein design</a> - 2022. [<a href="https://scholar.google.com/scholar?cluster=11732741354610784314" rel="noopener noreferrer">All Versions</a>]. Combining a basic set of building blocks into more complex forms is a universal design principle. Most protein designs have proceeded from a manual bottom-up approach using parts created by nature, but top-down design of proteins is fundamentally hard due to biological complexity. This work demonstrates how the modularity and programmability long sought for protein design can be realized through generative artificial intelligence. Advanced protein language models demonstrate emergent learning of atomic resolution structure and protein design principles. The authors leverage these developments to enable the programmable design of de novo protein sequences and structures of high complexity. First, the authors describe a high-level programming language based on modular building blocks that allows a designer to easily compose a set of desired properties. The authors then develop an energy-based generative model, built on atomic resolution structure prediction with a language model, that realizes all-atom structure designs that have the programmed properties. Designing a diverse set of specifications, including constraints on atomic coordinates, secondary structure, symmetry, and multimerization, demonstrates the generality and controllability of the approach. Enumerating constraints at increasing levels of hierarchical complexity shows that the approach can access a combinatorially large design space.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41467-023-39396-3" rel="noopener noreferrer">Artificial intelligence driven design of catalysts and materials for ring opening polymerization using a domain-specific language</a> - <em><strong>Nature Communications</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=6595955912508683146" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/IBM/ibm-materials-notebook" rel="noopener noreferrer">Project (⭐14)</a>]. Advances in machine learning (ML) and automated experimentation are poised to vastly accelerate research in polymer science. Data representation is a critical aspect for enabling ML integration in research workflows, yet many data models impose significant rigidity making it difficult to accommodate a broad array of experiment and data types found in polymer science. This inflexibility presents a significant barrier for researchers to leverage their historical data in ML development. This work shows that a domain specific language, termed Chemical Markdown Language (CMDL), provides flexible, extensible, and consistent representation of disparate experiment types and polymer structures. CMDL enables seamless use of historical experimental data to fine-tune regression transformer (RT) models for generative molecular design tasks. The authors demonstrate the utility of this approach through the generation and the experimental validation of catalysts and polymers in the context of ring-opening polymerization---although the authors provide examples of how CMDL can be more broadly applied to other polymer classes. Critically, this work shows how the CMDL tuned model preserves key functional groups within the polymer structure, allowing for experimental validation. These results reveal the versatility of CMDL and how it facilitates translation of historical data into meaningful predictive and generative models to produce experimentally actionable output.</p>
</li>
<li><p><a href="https://docs.openlaw.io/" rel="noopener noreferrer">OpenLaw</a> - <em><strong>OpenLaw.io</strong></em>. It is now possible to model all or parts of legal agreements using code (smart contracts), decreasing the cost and friction of creating, securing, and generating binding legal agreements. Lawyers lack basic tools to build these dynamic, “smart” contracts in a way that is enforceable and understandable to a legal professional. OpenLaw is a technology stack to help power next generation "smart" legal agreements, with a domain-specific markup language, a integration framework, and a series of general applications.</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s10994-021-06120-5" rel="noopener noreferrer">Scenic: a language for scenario specification and data generation</a> - <em><strong>Machine Learning</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=13790565080942515865" rel="noopener noreferrer">All Versions</a>]. This paper proposes a domain-specific language, Scenic, for describing scenarios that are distributions over scenes and the behaviors of their agents over time. Scenic combines concise, readable syntax for spatiotemporal relationships with the ability to declaratively impose hard and soft constraints over the scenario.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9169399" rel="noopener noreferrer">Domain Specific Language for Smart Contract Development</a> - <em><strong>ICBC'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16998538751745390273" rel="noopener noreferrer">All Versions</a>]. [<a href="http://eprints-dev5.cs.univie.ac.at/6341/1/PID6382125.pdf" rel="noopener noreferrer">Preprint</a>]. This research addresses the understanding hardness raised from the conceptual discrepancy between contractual clauses and corresponding code of the Solidity programming language, by the design and study of a domain-specific smart contract language based on higher level of abstraction that can be automatically transformed to an implementation.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0950584921002081" rel="noopener noreferrer">iContractML 2.0: A domain-specific language for modeling and deploying smart contracts onto multiple blockchain platforms</a> - <em><strong>Information and Software Technology</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=1548144959305241494" rel="noopener noreferrer">All Versions</a>]. Smart contracts play a vital role in many fields. Despite being called smart, the development of smart contracts is a tedious task beyond defining a set of contractual rules. In addition to business knowledge, coding a smart contract requires strong technical knowledge in a multiplex of new and rapidly changing domain-specific languages and blockchain platforms. The goal of this paper is to assist developers in building smart contracts independently from the language or the target blockchain platform. In which, this paper presents the second-generation smart contract language iContractML 2.0. iContractML 2.0 is an extensible framework that empowers developers to model and generate functional smart contract code that can be deployed onto multiple blockchain platforms.</p>
</li>
<li><p><a href="https://proceedings.mlr.press/v130/lew21a.html" rel="noopener noreferrer">PClean: Bayesian Data Cleaning at Scale with Domain-Specific Probabilistic Programming</a> - <em><strong>ICML'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=2892523061439714130" rel="noopener noreferrer">All Versions</a>]. This work presents PClean, a probabilistic programming language (PPL) for leveraging dataset-specific knowledge to automate Bayesian cleaning, automating Bayesian approaches given the diversity of real-world error patterns and the hardness of inference.</p>
</li>
<li><p><a href="http://proceedings.mlr.press/v139/tavares21a.html" rel="noopener noreferrer">A Language for Counterfactual Generative Models</a> - <em><strong>ICML'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=2067748786482591497" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/zenna/Omega.jl" rel="noopener noreferrer">Project (⭐171)</a>]. This paper presents Omega, a probabilistic programming language with support for counterfactual inference. This feature is accomplished by introducing a new operator to probabilistic programming akin to Pearl’s do.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/6030048" rel="noopener noreferrer">Product Line Engineering Using Domain-Specific Languages</a> - <em><strong>ISPLC'11</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?cluster=17589685299346185442" rel="noopener noreferrer">All Versions</a>]. [<a href="https://voelter.de/data/pub/VoelterVisser-PLEusingDSLs.pdf" rel="noopener noreferrer">Preprint</a>]. This paper investigates the application of domain-specific languages in product line engineering (PLE). It starts by analyzing the limits of expressivity of feature models. Feature models correspond to context-free grammars without recursion, which prevents the expression of multiple instances and references. The authors then show how domain-specific languages (DSLs) can serve as a middle ground between feature modeling and programming. They can be used in cases where feature models are too limited, while keeping the separation between problem space and solution space provided by feature models. This work then categorizes useful combinations between configuration with feature model and construction with DSLs and provide an integration of DSLs into the conceptual framework of PLE. Finally the authors show how use of a consistent, unified formalism for models, code, and configuration can yield important benefits for managing variability and trace ability.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/9613674" rel="noopener noreferrer">A Domain-Specific Language for Product-Process-Resource Modeling</a> - <em><strong>ETFA'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=6006131184799036515" rel="noopener noreferrer">All Versions</a>]. This paper presents the design of the PPR-DSL to effectively and efficiently represent Product-Process-Resource (PPR) aspects and evaluate constraints defined for modeling PPR views in the Formalized Process Description standard (VDI 3682).</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s11263-018-1103-5" rel="noopener noreferrer">Configurable 3D Scene Synthesis and 2D Image Rendering with Per-pixel Ground Truth Using Stochastic Grammars</a> - <em><strong>International Journal of Computer Vision</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8301697457354598778" rel="noopener noreferrer">All Versions</a>]. [<a href="https://yzhu.io/publication/scenesynthesis2018ijcv/paper.pdf" rel="noopener noreferrer">Preprint</a>]. This work proposes a systematic learning-based approach to the generation of massive quantities of synthetic 3D scenes and arbitrary numbers of photorealistic 2D images thereof, with associated ground truth information, for the purposes of training, benchmarking, and diagnosing learning-based computer vision and robotics algorithms. In particular, the authors devise a learning-based pipeline of algorithms capable of automatically generating and rendering a potentially infinite variety of indoor scenes by using a stochastic grammar, represented as an attributed Spatial And-Or Graph, in conjunction with state-of-the-art physics-based rendering. The pipeline is capable of synthesizing scene layouts with high diversity, and it is configurable inasmuch as it enables the precise customization and control of important attributes of the generated scenes. It renders photorealistic RGB images of the generated scenes while automatically synthesizing detailed, per-pixel ground truth data, including visible surface depth and normal, object identity, and material information (detailed to object parts), as well as environments (e.g., illuminations and camera viewpoints). The authors demonstrate the value of the synthesized dataset, by improving performance in certain machine-learning-based scene understanding tasks—depth and surface normal prediction, semantic segmentation, reconstruction, etc.---and by providing benchmarks for and diagnostics of trained models by modifying object attributes and scene properties in a controllable manner.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2410.16770" rel="noopener noreferrer">The Scene Language: Representing Scenes with Programs, Words, and Embeddings</a> - <em><strong>CVPR'25</strong></em>, 2025. [<a href="https://scholar.google.com/scholar?cluster=8704845413716059914" rel="noopener noreferrer">All Versions</a>]. [<a href="https://ai.stanford.edu/~yzzhang/projects/scene-language/" rel="noopener noreferrer">Project</a>]. This paper introduces the Scene Language, a visual scene representation that concisely and precisely describes the structure, semantics, and identity of visual scenes. It represents a scene with three key components: a program that specifies the hierarchical and relational structure of entities in the scene, words in natural language that summarize the semantic class of each entity, and embeddings that capture the visual identity of each entity. This representation can be inferred from pre-trained language models via a training-free inference technique, given text or image inputs.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="logic-dsl-applications"><a class="anchor" aria-hidden="true" tabindex="-1" href="#logic-dsl-applications"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Logic DSL Applications</h4><ul>
<li><p><a href="https://en.wikipedia.org/wiki/Situation_calculus" rel="noopener noreferrer">Situation Calculus</a> - <em><strong>Wikipedia</strong></em>. Wikipedia on Situation Calculus, a logic formalism designed for representing and reasoning about dynamical domains.</p>
</li>
<li><p><a href="https://link.springer.com/content/pdf/10.1007/978-3-030-24658-7.pdf" rel="noopener noreferrer">What is Answer Set Programming?</a> - <em><strong>Springer</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=3691841207891991771" rel="noopener noreferrer">All Versions</a>]. [<a href="https://dl.acm.org/doi/abs/10.5555/1620270.1620340" rel="noopener noreferrer">Tutorial on AAAI</a>]. Answer set programming (ASP) is a form of declarative programming oriented towards difficult search problems. As an outgrowth of research on the use of nonmonotonic reasoning in knowledge representation, it is particularly useful in knowledge-intensive applications. ASP programs consist of rules that look like Prolog rules, but the computational mechanisms used in ASP are different: they are based on the ideas that have led to the creation of fast satisfiability solvers for propositional logic.</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007/3-540-46767-x_28" rel="noopener noreferrer">Answer Set Programming</a> - <em><strong>ICLPNR'99</strong></em>, 1999. [<a href="https://scholar.google.com/scholar?cluster=15267370435063454675" rel="noopener noreferrer">All Versions</a>]. [<a href="http://people.sabanciuniv.edu/~esraerdem/teaching/krr06/asp.pdf" rel="noopener noreferrer">Preprint</a>]. The original paper on Answer Set Programming (ASP), a form of declarative programming oriented towards difficult search problems, on the use of nonmonotonic reasoning in knowledge representation. In ASP solutions to a problem are represented by answer sets (known also as stable models), and not by answer substitutions produced in response to a query, as in conventional logic programming.</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007%2F978-3-642-60085-2_16" rel="noopener noreferrer">Action Languages, Answer Sets, and Planning</a> - <em><strong>The Logic Programming Paradigms</strong></em>, 1999. [<a href="https://scholar.google.com/scholar?cluster=2045126541850245645" rel="noopener noreferrer">All Versions</a>]. [<a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=e58359b3dae3141fd2c85ee3f00c566411134929" rel="noopener noreferrer">Preprint</a>]. This is a discussion of some of the achievements and challenges related to representing actions and the design of planners from the perspective of logic programming. The authors talk about recent work on action languages and translating them into logic programming, on representing possible histories of an action domain by answer sets, on efficient implementations of the answer set semantics and their use for generating plans, and on causal logic and its relation to planning algorithms. Recent progress in these areas may lead to the creation of planners which are based on the ideas of logic programming and combine the use of expressive action description languages with efficient computational procedures.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/0004370286900731" rel="noopener noreferrer">Qualitative Simulation</a> - <em><strong>Artificial Intelligence</strong></em>, 1986. [<a href="https://scholar.google.com/scholar?cluster=4945009733425184345&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.cs.utexas.edu/ftp/qsim/papers/Kuipers-aij-86.pdf" rel="noopener noreferrer">Preprint</a>]. This paper presents a precise definition of qualitative structure and behavior descriptions as abstractions of differential equations and continuously differentiable functions. The authors present a new algorithm for qualitative simulation that generalizes the best features of existing algorithms, and allows direct comparisons among alternate approaches. Starting with a set of constraints abstracted from a differential equation, this work proves that the QSIM algorithm is guaranteed to produce a qualitative behavior corresponding to any solution to the original equation. The paper also shows that any qualitative simulation algorithm will sometimes produce spurious qualitative behaviors: ones which do not correspond to any mechanism satisfying the given constraints. These observations suggest specific types of care that must be taken in designing applications of qualitative causal reasoning systems, and in constructing and validating a knowledge base of mechanism descriptions.</p>
</li>
<li><p><a href="https://www.cs.utexas.edu/users/qr/QR-book.html" rel="noopener noreferrer">Qualitative Reasoning: Modeling and Simulation with Incomplete Knowledge</a> - <em><strong>MIT Press</strong></em>, 1994. [<a href="https://scholar.google.com/scholar?&amp;cluster=6634684154722677465" rel="noopener noreferrer">All Versions</a>]. This book presents, within a conceptually unified theoretical framework, a body of methods that have been developed over the past fifteen years for building and simulating qualitative models of physical systems - bathtubs, tea kettles, automobiles, the physiology of the body, chemical processing plants, control systems, electrical systems - where knowledge of that system is incomplete. The primary tool for this work is the author's QSIM algorithm, which is discussed in detail. Qualitative models are better able than traditional models to express states of incomplete knowledge about continuous mechanisms. Qualitative simulation guarantees to find all possible behaviors consistent with the knowledge in the model. This expressive power and coverage is important in problem solving for diagnosis, design, monitoring, explanation, and other applications of artificial intelligence.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0004370297000507" rel="noopener noreferrer">Qualitative and quantitative simulation: bridging the gap</a> - <em><strong>Artificial Intelligence</strong></em>, 1997. [<a href="https://scholar.google.com/scholar?cluster=9033452473914228535" rel="noopener noreferrer">All Versions</a>]. Shortcomings of qualitative simulation and of quantitative simulation motivate combining them to do simulations exhibiting strengths of both. The resulting class of techniques is called semiquantitative simulation. One approach to semi-quantitative simulation is to use numeric intervals to represent incomplete quantitative information. This research demonstrates semi-quantitative simulation using intervals in an implemented semi-quantitative simulator called Q3. Q3 progressively refines a qualitative simulation, providing increasingly specific quantitative predictions which can converge to a numerical simulation in the limit while retaining important correctness guarantees from qualitative and interval simulation techniques.</p>
</li>
<li><p><a href="https://pubs.acs.org/doi/10.1021/acssynbio.8b00229" rel="noopener noreferrer">A Logic Programming Language for Computational Nucleic Acid Devices</a> - <em><strong>ACS Synthetic Biology</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=3336951672389047784" rel="noopener noreferrer">All Versions</a>]. This paper presents a logic programming language that allows a broad range of computational nucleic acid systems to be designed and analyzed. The language extends standard logic programming with a novel equational theory to express nucleic acid molecular motifs. It automatically identifies matching motifs present in the full system, in order to apply a specified transformation expressed as a logical rule.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41596-021-00675-2" rel="noopener noreferrer">Genetic circuit design automation with Cello 2.0</a> - <em><strong>Nature Protocol</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=7418307542591684967" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.researchgate.net/profile/Samuel-Oliveira-38/publication/358801979_Genetic_circuit_design_automation_with_Cello_20/links/635debf412cbac6a3e0b19e4/Genetic-circuit-design-automation-with-Cello-20.pdf" rel="noopener noreferrer">Preprint</a>]. Cells interact with their environment, communicate among themselves, track time and make decisions through functions controlled by natural regulatory genetic circuits consisting of interacting biological components. Synthetic programmable circuits used in therapeutics and other applications can be automatically designed by computer-aided tools. The Cello software designs the DNA sequences for programmable circuits based on a high-level software description and a library of characterized DNA parts representing Boolean logic gates. This process allows for design specification reuse, modular DNA part library curation and formalized circuit transformations based on experimental data. This protocol describes Cello 2.0, a freely available cross-platform software written in Java. Cello 2.0 enables flexible descriptions of the logic gates’ structure and their mathematical models representing dynamic behavior, new formal rules for describing the placement of gates in a genome, a new graphical user interface, support for Verilog 2005 syntax and a connection to the SynBioHub parts repository software environment. Collectively, these features expand Cello’s capabilities beyond Escherichia coli plasmids to new organisms and broader genetic contexts, including the genome. Designing circuits with Cello 2.0 produces an abstract Boolean network from a Verilog file, assigns biological parts to each node in the Boolean network, constructs a DNA sequence and generates highly structured and annotated sequence representations suitable for downstream processing and fabrication, respectively. The result is a sequence implementing the specified Boolean function in the organism and predictions of circuit performance. Depending on the size of the design space and users’ expertise, jobs may take minutes or hours to complete.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2502.13372" rel="noopener noreferrer">MoVer: Motion Verification for Motion Graphics Animations</a> - <em><strong>ACM Transactions on Graphics</strong></em>, 2025. [<a href="https://scholar.google.com/scholar?cluster=527747131334466686" rel="noopener noreferrer">All Versions</a>]. While large vision-language models can generate motion graphics animations from text prompts, they regularly fail to include all of spatio-temporal properties described in the prompt. This work introduces MoVer, a motion verification DSL based on first-order logic that can check spatio-temporal properties of a motion graphics animation. The authors identify a general set of such properties that people commonly use to describe animations (e.g., the direction and timing of motions, the relative positioning of objects, etc.). The authors implement these properties as predicates in MoVer and provide an execution engine that can apply a MoVer program to any input SVG-based motion graphics animation. The authors then demonstrate how MoVer can be used in an LLM-based synthesis and verification pipeline for iteratively refining motion graphics animations. Given a text prompt, the pipeline synthesizes a motion graphics animation and a corresponding MoVer program. Executing the verification program on the animation yields a report of the predicates that failed and the report can be automatically fed back to LLM to iteratively correct the animation.</p>
</li>
<li><p><a href="https://openreview.net/forum?id=C45YqeBDUM" rel="noopener noreferrer">The KoLMogorov Test: Compression by Code Generation</a> - <em><strong>ICLR'25</strong></em>, 2025. [<a href="https://scholar.google.com/scholar?cluster=16809888292456252135" rel="noopener noreferrer">All Versions</a>]. Compression is at the heart of intelligence. A theoretically optimal way to compress any sequence of data is to find the shortest program that outputs that sequence and then halts. However, such Kolmogorov compression is uncomputable, and code generating LLMs struggle to approximate this theoretical ideal, as it requires reasoning, planning and search capabilities beyond those of current models. This work introduces the KoLMogorov-Test (KT), a compression-as-intelligence intelligence test for code generation LLMs. In KT a model is presented with a sequence of data at inference time, and asked to generate the shortest DSL (designed specifically for the task) program that produces the sequence. The authors identify several benefits of KT for both evaluation and training: an essentially infinite number of problem instances of varying difficulty is readily available, strong baselines already exist, the evaluation metric (compression) cannot be gamed, and pretraining data contamination is highly unlikely. To evaluate current models, the authors use audio, text, and DNA data, as well as sequences produced by random synthetic DSL programs.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41598-022-21801-4" rel="noopener noreferrer">Meta-analysis of the functional neuroimaging literature with probabilistic logic programming</a> - <em><strong>Scientific Reports</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=5952076495542489316" rel="noopener noreferrer">All Versions</a>]. Inferring reliable brain-behavior associations requires synthesizing evidence from thousands of functional neuroimaging studies through meta-analysis. However, existing meta-analysis tools are limited to investigating simple neuroscience concepts and expressing a restricted range of questions. This work expands the scope of neuroimaging meta-analysis by designing NeuroLang: a domain-specific language to express and test hypotheses using probabilistic first-order logic programming. By leveraging formalisms found at the crossroads of artificial intelligence and knowledge representation, NeuroLang provides the expressivity to address a larger repertoire of hypotheses in a meta-analysis, while seamlessly modeling the uncertainty inherent to neuroimaging data. The authors demonstrate the language’s capabilities in conducting comprehensive neuroimaging meta-analysis through use-case examples that address questions of structure-function associations. Specifically, the authors infer the specific functional roles of three canonical brain networks, support the role of the visual word-form area in visuospatial attention, and investigate the heterogeneous organization of the frontoparietal control network.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="dsl-program-synthesis"><a class="anchor" aria-hidden="true" tabindex="-1" href="#dsl-program-synthesis"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>DSL Program Synthesis</h4><ul>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/3220134.3220135" rel="noopener noreferrer">pix2code: Generating Code from a Graphical User Interface Screenshot</a> - <em><strong>ACM SIGCHI Symposium on Engineering Interactive Computing Systems</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8296741513177971931" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/tonybeltramelli/pix2code" rel="noopener noreferrer">Code (⭐12k)</a>]. [<a href="https://uizard.io/research/" rel="noopener noreferrer">Website</a>]. This paper shows that deep learning methods can be leveraged to train a model end-to-end to automatically reverse engineer user interfaces and generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2018/hash/6788076842014c83cedadbe6b0ba0314-Abstract.html" rel="noopener noreferrer">Learning to Infer Graphics Programs from Hand-Drawn Images</a> - <em><strong>NeurIPS'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=14065112485794121024" rel="noopener noreferrer">All Versions</a>]. The method learns a model that uses program synthesis techniques to recover a graphics program from drawing primitives. These programs have constructs like variable bindings, iterative loops, or simple kinds of conditionals. With a graphics program in hand, we can correct errors made by the deep network and extrapolate drawings.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/3571207" rel="noopener noreferrer">babble: Learning Better Abstractions with E-Graphs and Anti-unification</a> - <em><strong>POPL'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=7935064016901049715" rel="noopener noreferrer">All Versions</a>]. This paper proposes library learning modulo theory (LLMT), a new library learning algorithm that additionally takes as input an equational theory for a given problem domain. LLMT uses e-graphs and equality saturation to compactly represent the space of programs equivalent modulo the theory, and uses a novel e-graph anti-unification technique to find common patterns in the corpus more directly and efficiently.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/3571234" rel="noopener noreferrer">Top-Down Synthesis for Library Learning</a> - <em><strong>POPL'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=12324277007659029766" rel="noopener noreferrer">All Versions</a>]. This paper introduces corpus-guided top-down synthesis as a mechanism for synthesizing library functions that capture common functionality from a corpus of programs in a domain specific language (DSL). The algorithm builds abstractions directly from initial DSL primitives, using syntactic pattern matching of intermediate abstractions to intelligently prune the search space and guide the algorithm towards abstractions that maximally capture shared structures in the corpus.</p>
</li>
<li><p><a href="https://royalsocietypublishing.org/doi/full/10.1098/rsta.2022.0050" rel="noopener noreferrer">DreamCoder: growing generalizable, interpretable knowledge with wake–sleep Bayesian program learning</a> - <em><strong>Philosophical Transactions of the Royal Society A</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11356436337624711843" rel="noopener noreferrer">All Versions</a>]. [<a href="https://arxiv.org/abs/2006.08381" rel="noopener noreferrer">Preprint</a>]. This paper presents DreamCoder, a system that learns to solve problems by writing programs. It builds expertise by creating domain-specific programming languages for expressing domain concepts, together with neural networks to guide the search for programs within these languages. A ‘wake–sleep’ learning algorithm alternately extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems. DreamCoder solves both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41467-022-32012-w" rel="noopener noreferrer">Synthesizing theories of human language with Bayesian program induction</a> - <em><strong>Nature Communications</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=8603772394100237159" rel="noopener noreferrer">All Versions</a>]. Automated, data-driven construction and evaluation of scientific models and theories is a long-standing challenge in artificial intelligence. This work presents a framework for algorithmically synthesizing models of a basic part of human language: morpho-phonology, the system that builds word forms from sounds. The authors integrate Bayesian inference with program synthesis and representations inspired by linguistic theory and cognitive models of learning and discovery. Across 70 datasets from 58 diverse languages, the system synthesizes human-interpretable models for core aspects of each language’s morpho-phonology, sometimes approaching models posited by human linguists. Joint inference across all 70 data sets automatically synthesizes a meta-model encoding interpretable cross-language typological tendencies. Finally, the same algorithm captures few-shot learning dynamics, acquiring new morphophonological rules from just one or a few examples. These results suggest routes to more powerful machine-enabled discovery of interpretable models in linguistics and other scientific domains.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/cd40d0d65bfebb894ccc9ea822b47fa8-Abstract-Conference.html" rel="noopener noreferrer">Grammar Prompting for Domain-Specific Language Generation with Large Language Models</a> - <em><strong>NeurIPS'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11694070042468483715" rel="noopener noreferrer">All Versions</a>]. Grammar prompting is a simple approach to enable LLMs to use external knowledge and domain-specific constraints expressed through a grammar in Backus--Naur Form (BNF) during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and SMILES-based molecule generation.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.14100" rel="noopener noreferrer">Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=8063693456660536915" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/ac-rad/xdl-generation" rel="noopener noreferrer">Project (⭐45)</a>]. [<a href="https://ac-rad.github.io/clairify/" rel="noopener noreferrer">Website</a>]. This paper proposes CLAIRIFY, an approach that combines automatic iterative prompting with program verification to ensure programs written in data-scarce domain-specific language are syntactically valid and incorporate environment constraints.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/full/10.1145/3613904.3642319" rel="noopener noreferrer">PhotoScout: Synthesis-Powered Multi-Modal Image Search</a> - <em><strong>ACM SIGCHI'24</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=6522231014055730719" rel="noopener noreferrer">All Versions</a>]. This paper explores a new multi-modal image search approach that allows users to conveniently specify and perform semantic image search tasks. With the tool, PhotoScout, the user interactively provides natural language descriptions, positive and negative examples, and object tags to specify their search tasks. Under the hood, PhotoScout is powered by a program synthesis engine that generates visual queries in a domain-specific language and executes the synthesized program to retrieve the desired images.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/54dd9e0cff6d9214e20d97eb2a3bae49-Abstract-Conference.html" rel="noopener noreferrer">Expert-level protocol translation for self-driving labs</a> - <em><strong>NeurIPS'24</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=13997597682274906943" rel="noopener noreferrer">All Versions</a>]. [<a href="https://autodsl.org/procedure/papers/neurips24shi.html" rel="noopener noreferrer">Project</a>]. Recent development in Artificial Intelligence (AI) models has propelled their application in scientific discovery, but the validation and exploration of these discoveries require subsequent empirical experimentation. The concept of self-driving laboratories promises to automate and thus boost the experimental process following AI-driven discoveries. However, the transition of experimental protocols, originally crafted for human comprehension, into formats interpretable by machines presents significant challenges, which, within the context of specific expert domain, encompass the necessity for structured as opposed to natural language, the imperative for explicit rather than tacit knowledge, and the preservation of causality and consistency throughout protocol steps. Presently, the task of protocol translation predominantly requires the manual and labor-intensive involvement of domain experts and information technology specialists, rendering the process time-intensive. To address these issues, this work proposes a framework that automates the protocol translation process through a three-stage workflow, which incrementally constructs Protocol Dependence Graphs (PDGs) that approach structured on the syntax level, completed on the semantics level, and linked on the execution level. Quantitative and qualitative evaluations have demonstrated its performance at par with that of human experts, underscoring its potential to significantly expedite and democratize the process of scientific discovery by elevating the automation capabilities within self-driving laboratories.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-023-06924-6" rel="noopener noreferrer">Mathematical discoveries from program search with large language models</a> - <em><strong>Nature</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=5653439474813913484" rel="noopener noreferrer">All Versions</a>]. Large language models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations), which can result in them making plausible but incorrect statements1,2. This hinders the use of current large models in scientific discovery. This work introduces FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pretrained LLM with a systematic evaluator. The authors demonstrate the effectiveness of this approach to surpass the best-known results in important problems, pushing the boundary of existing LLM-based approaches3. Applying FunSearch to a central problem in extremal combinatorics—the cap set problem—we discover new constructions of large cap sets going beyond the best-known ones, both in finite dimensional and asymptotic cases. This shows that it is possible to make discoveries for established open problems using LLMs. The authors showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve on widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="cognitive-foundations"><a class="anchor" aria-hidden="true" tabindex="-1" href="#cognitive-foundations"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Cognitive Foundations</h4><ul>
<li><p><a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(20)30174-1" rel="noopener noreferrer">The Child as Hacker</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13128656954836679743" rel="noopener noreferrer">All Versions</a>]. The scope of human learning and development poses a radical challenge for cognitive science. The authors propose that developmental theories can address this challenge by adopting perspectives from computer science. Many of our best models treat learning as analogous to computer programming because symbolic programs provide the most compelling account of sophisticated mental representations. The authors specifically propose that children’s learning is analogous to a particular style of programming called hacking, making code better along many dimensions through an open-ended set of goals and activities. By contrast to existing theories, which depend primarily on local search and simple metrics, this view highlights the many features of good mental representations and the multiple complementary processes children use to create them.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/182aed0379591ebd1d655b2bdc152075-Abstract-Datasets_and_Benchmarks.html" rel="noopener noreferrer">Communicating Natural Programs to Humans and Machines</a> - <em><strong>NeurIPS'22</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=13381039702346039142" rel="noopener noreferrer">All Versions</a>]. While humans readily generate and interpret instructions in a general language, computer systems are shackled to a narrow domain-specific language that they can precisely execute. This makes building intelligent systems that can generalize to novel situations such as ARC difficult. Human-generated instructions are referred as “natural programs”. While they resemble computer programs, they are distinct in two ways: First, they contain a wide range of primitives; Second, they frequently leverage communicative strategies beyond directly executable codes.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41467-024-50966-x" rel="noopener noreferrer">Symbolic metaprogram search improves learning efficiency and explains rule learning in humans</a> - <em><strong>Nature Communications</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=7670274141609367282" rel="noopener noreferrer">All Versions</a>]. Symbolic models based on program learning successfully explain rule-learning in many domains, but performance degrades quickly as program complexity increases. It remains unclear how to scale symbolic rule-learning methods to model human performance in challenging domains. This work shows that symbolic search over the space of metaprograms—programs that revise programs—dramatically improves learning efficiency. On a behavioral benchmark of 100 algorithmically rich rules, this approach fits human learning more accurately than alternative models while also using orders of magnitude less search. The computation required to match median human performance is consistent with conservative estimates of human thinking time. The results suggest that metaprogram-like representations may help human learners to efficiently acquire rules.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41562-025-02227-0" rel="noopener noreferrer">How laypeople evaluate scientific explanations containing jargon</a> - <em><strong>Nature Human Behavior</strong></em>, 2025. [<a href="https://scholar.google.com/scholar?cluster=6467855047925175367" rel="noopener noreferrer">All Versions</a>]. Individuals rely on others’ expertise to achieve a basic understanding of the world. But how can non-experts achieve understanding from explanations that, by definition, they are ill-equipped to assess? Across 9 experiments with 6,698 participants (Study 1A = 737; 1B = 734; 1C = 733; 2A = 1,014; 2B = 509; 2C = 1,012; 3A = 1,026; 3B = 512; 4 = 421), this work addresses this puzzle by focusing on scientific explanations with jargon. The authors identify ‘when’ and ‘why’ the inclusion of jargon makes explanations more satisfying, despite decreasing their comprehensibility. The authors find that jargon increases satisfaction because laypeople assume the jargon fills gaps in explanations that are otherwise incomplete. The authors also identify strategies for debiasing these judgements: when people attempt to generate their own explanations, inflated judgements of poor explanations with jargon are reduced, and people become better calibrated in their assessments of their own ability to explain.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="problem-solving"><a class="anchor" aria-hidden="true" tabindex="-1" href="#problem-solving"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Problem Solving</h3><h4 id="human-level-problem-solving"><a class="anchor" aria-hidden="true" tabindex="-1" href="#human-level-problem-solving"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Human-Level Problem Solving</h4><ul>
<li><p><a href="https://psycnet.apa.org/record/1959-07883-001" rel="noopener noreferrer">Elements of a theory of human problem solving</a> - <em><strong>Psychological Review</strong></em>, 1958. [<a href="https://scholar.google.com/scholar?cluster=6226995019045187501" rel="noopener noreferrer">All Versions</a>]. Herbert Simon's original idea on human problem solving.</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/1973-10478-000" rel="noopener noreferrer">Human Problem Solving</a> - <em><strong>Englewood Cliffs, NJ: Prentice-hall</strong></em>, 1972. [<a href="https://scholar.google.com/scholar?cluster=3996229083126262536&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Herbert Simon's classic idea of human problem solving as search.</p>
</li>
<li><p><a href="http://196.223.158.148/bitstream/handle/123456789/2978/596.pdf?sequence=1&amp;isAllowed=y" rel="noopener noreferrer">Learning to Solve Problems: A Handbook for Designing Problem-Solving Learning Environments</a> - <em><strong>Taylorfrancis</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=13262690779319271809&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.science.org/doi/abs/10.1126/science.185.4157.1124" rel="noopener noreferrer">Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty</a> - <em><strong>Science</strong></em>, 1974. [<a href="https://scholar.google.com/scholar?cluster=17040257859216791312&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Daniel Kahneman's classic idea of prospective theory.</p>
</li>
<li><p><a href="https://www.pnas.org/content/117/47/29381.short" rel="noopener noreferrer">Computational evidence for hierarchically structured reinforcement learning in humans</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5731363475904675608&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A piece of evidence on hierarchical human planning.</p>
</li>
<li><p><a href="https://www.cnbc.cmu.edu/braingroup/papers/sarafyazd_jazayeri_2019.pdf" rel="noopener noreferrer">Hierarchical reasoning by neural circuits in the frontal cortex</a> - <em><strong>Science</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=9875733886908769773&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Neuroscience evidence supporting rule switch.</p>
</li>
<li><p><a href="https://oar.princeton.edu/rt4ds/file/11875/2161" rel="noopener noreferrer">The importance of mixed selectivity in complex cognitive tasks</a> - <em><strong>Nature</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=2707751672275136220&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper introducing mixed selectivity with high-dimensional neural representations.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-022-04743-9" rel="noopener noreferrer">People construct simplified mental representations to plan</a> - <em><strong>Nature</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=12068944400080889789&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A computational account on rational problem representation in human planning.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S1364661322002819" rel="noopener noreferrer">Goals, usefulness and abstraction in value-based choice</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=6256990098976657651&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>]. A review that outlines the computational and biological principles that enable the brain to compute the usefulness of an option or action by creating abstractions that flexibly adapt to changing goals.</p>
</li>
<li><p><a href="https://elifesciences.org/articles/68943" rel="noopener noreferrer">Value signals guide abstraction during learning</a> - <em><strong>eLife</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=10324834842795908439&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/BF00058926" rel="noopener noreferrer">Learning to perceive and act by trial and error</a> - <em><strong>Machine Learning</strong></em>, 1991. [<a href="https://scholar.google.com/scholar?cluster=1987606770603964473&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1801_3" rel="noopener noreferrer">Representations in distributed cognitive tasks</a> - <em><strong>Cognitive Science</strong></em>, 1994. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=14781266698447195483" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0364021399800226" rel="noopener noreferrer">The nature of external representations in problem solving</a> - <em><strong>Cognitive Science</strong></em>, 1997. [<a href="https://scholar.google.com/scholar?cluster=10698887231200401430&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.pnas.org/content/pnas/117/47/29302.full.pdf" rel="noopener noreferrer">Rapid trail-and-error learning with simulation supports flexible tool use and physical reasoning.</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=14400178089019636923" rel="noopener noreferrer">All Versions</a>]. [<a href="https://sites.google.com/view/virtualtoolsgame/home" rel="noopener noreferrer">Project</a>]. [<a href="https://www.pnas.org/content/pnas/suppl/2020/11/20/1912341117.DCSupplemental/pnas.1912341117.sapp.pdf" rel="noopener noreferrer">Appendix</a>]. Many animals, and an increasing number of artificial agents, display sophisticated capabilities to perceive and manipulate objects. But human beings remain distinctive in their capacity for flexible, creative tool use—using objects in new ways to act on the world, achieve a goal, or solve a problem. To study this type of general physical problem solving, this work introduces the Virtual Tools game. In this game, people solve a large range of challenging physical puzzles in just a handful of attempts. This work proposes that the flexibility of human physical problem solving rests on an ability to imagine the effects of hypothesized actions, while the efficiency of human search arises from rich action priors which are updated via observations of the world. The authors instantiate these components in the “sample, simulate, update” (SSUP) model and show that it captures human performance across 30 levels of the Virtual Tools game. More broadly, this model provides a mechanism for explaining how people condense general physical knowledge into actionable, task-specific plans to achieve flexible and efficient physical problem solving.</p>
</li>
<li><p><a href="https://cognitivesciencesociety.org/cogsci20/papers/0765/0765.pdf" rel="noopener noreferrer">Abstract strategy learning underlies flexible transfer in physical problem solving</a> - <em><strong>CogSci'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Abstract+strategy+learning+underlies+flexible+transfer+in+physical+problem+solving.&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openreview.net/forum?id=CXyZrKPz4CU" rel="noopener noreferrer">Physion: Evaluating Physical Prediction from Vision in Humans and Machines</a> - <em><strong>NeurIPS'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=8733318111076645893&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S2352154620301236" rel="noopener noreferrer">Exploration: from machines to humans</a> - <em><strong>Current Opinion in Behavioral Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=8015078432419172621&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S2352154620301467" rel="noopener noreferrer">Balancing exploration and exploitation with information and randomization</a> - <em><strong>Current Opinion in Behavioral Sciences</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=8164388137243077863&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0092867421008369" rel="noopener noreferrer">Hippocampal neurons construct a map of an abstract value space</a> - <em><strong>Cell</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=12658820581876003172&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.pnas.org/content/106/25/10370.short" rel="noopener noreferrer">Insightful problem solving and creative tool modification by captive nontool-using rooks</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=6776471679661065229&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.pnas.org/content/suppl/2009/05/28/0901008106.DCSupplemental" rel="noopener noreferrer">Supplementary Material</a>]. A piece of evidence on creative tool use in intelligent animals.</p>
</li>
<li><p><a href="https://cpilab.org/pubs/Dasgupta2018Learning.pdf" rel="noopener noreferrer">Learning to act by integrating mental simulations and physical experiments</a> - <em><strong>CogSci'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=7342920174595829739&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/ishita-dg/SimulationVSAction" rel="noopener noreferrer">Code (⭐1)</a>].</p>
</li>
<li><p><a href="https://gershmanlab.com/pubs/Momennejad17.pdf" rel="noopener noreferrer">The successor representation in human reinforcement learning</a> - <em><strong>Nature Human Behavior</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=7317529612823134939&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.science.org/doi/abs/10.1126/scirobotics.aav3150" rel="noopener noreferrer">Beyond imitation: Zero-shot task transfer on robots by learning concepts as cognitive programs</a> - <em><strong>Science Robotics</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7451223471302689228" rel="noopener noreferrer">All Versions</a>]. Humans can infer concepts from image pairs and apply those in the physical world in a completely different setting, enabling tasks like IKEA assembly from diagrams. If robots could represent and infer high-level concepts, then it would notably improve their ability to understand our intent and to transfer tasks between different environments. To that end, the authors introduce a computational framework that replicates aspects of human concept learning. Concepts are represented as programs on a computer architecture consisting of a visual perception system, working memory, and action controller. The instruction set of this cognitive computer has commands for parsing a visual scene, directing gaze and attention, imagining new objects, manipulating the contents of a visual working memory, and controlling arm movement. Inferring a concept corresponds to inducing a program that can transform the input to the output. Some concepts require the use of imagination and recursion. Previously learned concepts simplify the learning of subsequent, more elaborate concepts and create a hierarchy of abstractions. The authors demonstrate how a robot can use these abstractions to interpret novel concepts presented to it as schematic images and then apply those concepts in very different situations. By bringing cognitive science ideas on mental imagery, perceptual symbols, embodied cognition, and deictic mechanisms into the realm of machine learning, this work brings researchers closer to the goal of building robots that have interpretable representations and common sense.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="planning"><a class="anchor" aria-hidden="true" tabindex="-1" href="#planning"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Planning</h4><ul>
<li><p><a href="https://jair.org/index.php/jair/article/view/11175" rel="noopener noreferrer">From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning</a> - <em><strong>Journal of Artificial Intelligence Research</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=17962480659445514879" rel="noopener noreferrer">All Versions</a>]. This work considers the problem of constructing abstract representations for planning in high-dimensional, continuous environments. The authors assume an agent equipped with a collection of high-level actions, and construct representations provably capable of evaluating plans composed of sequences of those actions. The authors first consider the deterministic planning case, and show that the relevant computation involves set operations performed over sets of states. The authors then consider probabilistic planning, which they show requires generalizing from sets of states to distributions over states. Finally, the authors apply these techniques to create a physical robot system that autonomously learns its own symbolic representation of a mobile manipulation task directly from sensorimotor data---point clouds, map locations, and joint angles---and then plans using that representation.</p>
</li>
<li><p><a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-control-091420-084139" rel="noopener noreferrer">Integrated Task and Motion Planning</a> - <em><strong>Annual Review of Control, Robotics, and Autonomous Systems</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=478421650694199529" rel="noopener noreferrer">All Versions</a>]. The problem of planning for a robot that operates in environments containing a large number of objects, taking actions to move itself through the world as well as to change the state of the objects, is known as task and motion planning (TAMP). TAMP problems contain elements of discrete task planning, discrete–continuous mathematical programming, and continuous motion planning and thus cannot be effectively addressed by any of these fields directly. In this article, the authors define a class of TAMP problems and survey algorithms for solving them, characterizing the solution methods in terms of their strategies for solving the continuous-space subproblems and their techniques for integrating the discrete and continuous components of the search.</p>
</li>
<li><p><a href="https://dspace.mit.edu/handle/1721.1/126626" rel="noopener noreferrer">Differentiable Physics and Stable Modes for Tool-Use and Manipulation Planning</a> - <em><strong>Robotics: Science and Systems</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=10342169019935480143&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://gershmanlab.com/pubs/Dasgupta18_simulation.pdf" rel="noopener noreferrer">Learning to act by integrating mental simulations and physical experiments</a> - <em><strong>CogSci'21</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=7342920174595829739&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/ftr/10.1111/cogs.12928" rel="noopener noreferrer">What Is the Model in Model-Based Planning?</a> - <em><strong>Cognitive Science</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=10598397017491369972&amp;hl=en&amp;scisbd=1&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2109.11082.pdf" rel="noopener noreferrer">Discovering State and Action Abstractions for Generalized Task and Motion Planning</a> - <em><strong>AAAI'22</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=1054368060554971920" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="intrinsic-motivation"><a class="anchor" aria-hidden="true" tabindex="-1" href="#intrinsic-motivation"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Intrinsic Motivation</h4><ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2004/hash/4be5a36cbaca8ab9d2066debfe4e65c1-Abstract.html" rel="noopener noreferrer">Intrinsically Motivated Reinforcement Learning</a> - <em><strong>NeurIPS'04</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=9736217847061704054&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A comprehensive review on intrinsic reward functions in classic reinforcement learning.</p>
</li>
<li><p><a href="https://www.frontiersin.org/articles/10.3389/neuro.12.006.2007/full" rel="noopener noreferrer">What is intrinsic motivation? A typology of computational approaches</a> - <em><strong>Frontiers in Neurorobotics</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=11901343819872275353&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.jair.org/index.php/jair/article/view/12087" rel="noopener noreferrer">Adapting Behavior via Intrinsic Reward: A Survey and Empirical Study</a> - <em><strong>Journal of Artificial Intelligence Research</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5309595875334344707&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.mlr.press/v70/pathak17a.html" rel="noopener noreferrer">Curiosity-driven Exploration by Self-supervised Prediction</a> - <em><strong>ICML'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=9379743003299559904&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on curiosity as intrinsic motivation.</p>
</li>
<li><p><a href="https://arxiv.org/abs/1706.01502" rel="noopener noreferrer">UCB Exploration via Q-Ensembles</a> - 2017. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=13260404166621290240" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2010.03110" rel="noopener noreferrer">Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning</a> - <em><strong>ICML'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=4880520597219138666&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2015/hash/e00406144c1e7e35240afed70f34166a-Abstract.html" rel="noopener noreferrer">Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning</a> - <em><strong>NeurIPS'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=9262504233068870193&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on empowerment as intrinsic motivation.</p>
</li>
<li><p><a href="https://psyarxiv.com/ybs7g/" rel="noopener noreferrer">Intrinsic Exploration as Empowerment in a Richly Structured Online Game</a> - 2022. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=12321757821600526668" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://gershmanlab.com/pubs/Tomov21.pdf" rel="noopener noreferrer">Multi-task reinforcement learning in humans</a> - <em><strong>Nature Human Behavior</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=14589018692074515644&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/10778628" rel="noopener noreferrer">JARVIS-1: Open-World Multi-Task Agents With Memory-Augmented Multimodal Language Models</a> - <em><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong></em>. [<a href="https://scholar.google.com/scholar?cluster=12845806504666245406" rel="noopener noreferrer">All Versions</a>]. Achieving human-like planning and control with multimodal observations in an open world is a key milestone for more functional generalist agents. Existing approaches can handle certain long-horizon tasks in an open world. However, they still struggle when the number of open-world tasks could potentially be infinite and lack the capability to progressively enhance task completion as game time progresses. This work introduces JARVIS-1, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world Minecraft universe. Specifically, the authors develop JARVIS-1 on top of pre-trained multimodal language models, which map visual observations and textual instructions to plans. The plans will be ultimately dispatched to the goal-conditioned controllers. JARVIS-1 is outfitted with a multimodal memory, which facilitates planning using both pre-trained knowledge and its actual game survival experiences. JARVIS-1 is the existing most general agent in Minecraft, capable of completing over 200 different tasks using control and observation space similar to humans.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="reinforcement-learning"><a class="anchor" aria-hidden="true" tabindex="-1" href="#reinforcement-learning"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Reinforcement Learning</h4><ul>
<li><p><a href="https://www.andrew.cmu.edu/user/rmorina/papers/SuttonBook.pdf" rel="noopener noreferrer">Reinforcement learning: An introduction</a> - <em><strong>MIT Press</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8821915215029978039&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Richard Sutton's comprehensive book on reinforcement learning.</p>
</li>
<li><p><a href="https://www.jair.org/index.php/jair/article/view/10166" rel="noopener noreferrer">Reinforcement learning: A survey</a> - <em><strong>Journal of Artificial Intelligence Research</strong></em>, 1996. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=4983604491168613713" rel="noopener noreferrer">All Versions</a>]. Leslie Kaelbling's review on reinforcement learning.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2011.00583.pdf" rel="noopener noreferrer">An overview of multi-agent reinforcement learning from game theoretical perspective</a> - 2020. [<a href="https://scholar.google.com/scholar?cluster=16197919002723407603&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Yaodong Yang's review on multi-agent reinforcement learning from the perspective of game theory.</p>
</li>
<li><p><a href="https://klab.tch.harvard.edu/academia/classes/Neuro230/ReadingAssignments/MnihEtAlHassibis15NatureControlDeepRL.pdf" rel="noopener noreferrer">Human-level control through deep reinforcement learning</a> - <em><strong>Nature</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=12439121588427761338&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on solving Atari games via Deep Q-Network.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0004370299000521" rel="noopener noreferrer">Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning</a> - <em><strong>Artificial Intelligence</strong></em>, 1999. [<a href="https://scholar.google.com/scholar?cluster=1471968208408231068&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on operation reinforcement learning.</p>
</li>
<li><p><a href="http://oucsace.cs.ohio.edu/~chelberg/classes/680/paperPresentations/NathanPaperToPresent.pdf" rel="noopener noreferrer">On Monte Carlo Tree Search and Reinforcement Learning</a> - <em><strong>Journal of Artificial Intelligence Research</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=5805718077259491860&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/1805.00909" rel="noopener noreferrer">Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review</a> - 2018. [<a href="https://scholar.google.com/scholar?cluster=16437288987337534404&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa18/static/slides/lec-15.pdf" rel="noopener noreferrer">Slides</a>]. Sergey Levine's tutorial on treating reinforcement learning probabilisticly.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2019/hash/4a46fbfca3f1465a27b210f4bdfe6ab3-Abstract.html" rel="noopener noreferrer">A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation</a> - <em><strong>NeurIPS'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7721047641895252765&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openreview.net/forum?id=9SS69KwomAM" rel="noopener noreferrer">Solving Compositional Reinforcement Learning Problems via Task Reduction</a> - <em><strong>ICLR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15628616147808752058&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8460689" rel="noopener noreferrer">Neural Task Programming: Learning to Generalize Across Hierarchical Tasks</a> - <em><strong>ICRA'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=7155333517647976638&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://academic.oup.com/logcom/article-abstract/28/2/337/4695480" rel="noopener noreferrer">Learning to act: qualitative learning of deterministic action models</a> - <em><strong>Journal of Logic and Computation</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=14570482854600886953&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.06076" rel="noopener noreferrer">Learning to Act and Observe in Partially Observable Domains</a> - 2021. [<a href="https://scholar.google.com/scholar?cluster=2258600434630687063&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2107.06277" rel="noopener noreferrer">Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability</a> - <em><strong>NeurIPS'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=9640851185758072663&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A formal treatment on the generalization problem in reinforcement learning.</p>
</li>
<li><p><a href="https://openreview.net/forum?id=r1nTpv9eg" rel="noopener noreferrer">Learning to Perform Physics Experiments via Deep Reinforcement Learning</a> - <em><strong>ICLR'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=13142558595749186250&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9387127" rel="noopener noreferrer">Data-Efficient Learning for Complex and Real-Time Physical Problem Solving Using Augmented Simulation</a> - <em><strong>Robotics and Automation Letters</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=3140653562829320759&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.jmlr.org/papers/volume18/16-634/16-634.pdf" rel="noopener noreferrer">A Survey of Preference-Based Reinforcement Learning Methods</a> - <em><strong>Journal of Machine Learning Research</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=13278778479251450967&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://papers.NeurIPS.cc/paper/2021/file/4079016d940210b4ae9ae7d41c4a2065-Paper.pdf" rel="noopener noreferrer">On the Expressivity of Markov Reward</a> - <em><strong>NeurIPS'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=4524686816939437211&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A formal treatment of tasks and rewards in reinforcement learning modeling.</p>
</li>
<li><p><a href="https://proceedings.mlr.press/v37/schulman15.html" rel="noopener noreferrer">Trust Region Policy Optimization</a> - <em><strong>ICML'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=4215501129336400677&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper introducing TRPO, a method for optimizing control policies, with guaranteed monotonic improvement.</p>
</li>
<li><p><a href="http://proceedings.mlr.press/v70/achiam17a/achiam17a.pdf" rel="noopener noreferrer">Constrained Policy Optimization</a> - <em><strong>ICML'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=6114366704163518185&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on constrained reinforcement learning (safe reinforcement learning).</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper_files/paper/2019/hash/5faf461eff3099671ad63c6f3f094f7f-Abstract.html" rel="noopener noreferrer">When to Trust Your Model: Model-Based Policy Optimization</a> - <em><strong>NeurIPS'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4248859125840907707&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://bair.berkeley.edu/blog/2019/12/12/mbpo/" rel="noopener noreferrer">Post</a>].</p>
</li>
<li><p><a href="http://proceedings.mlr.press/v139/lee21g.html" rel="noopener noreferrer">SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning</a> - <em><strong>ICML'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=8840831494454574191&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/pokaxpoka/sunrise" rel="noopener noreferrer">Code (⭐125)</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2202.13252" rel="noopener noreferrer">The Quest for a Common Model of the Intelligent Decision Maker</a> - <em><strong>Multi-disciplinary Conference on Reinforcement Learning and Decision Making'22</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=7652784232757502910&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Richard Sutton's perspective on the future directions of reinforcement learning research.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.5555/3491440.3492111" rel="noopener noreferrer">Automatic curriculum learning for deep RL: a short survey</a> - <em><strong>IJCAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=10660055557098312214&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://proceedings.mlr.press/v139/romac21a.html" rel="noopener noreferrer">TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL</a> - <em><strong>ICML'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=11016662361926634008&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/flowersteam/TeachMyAgent" rel="noopener noreferrer">Project (⭐73)</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="inverse-reinforcement-learning"><a class="anchor" aria-hidden="true" tabindex="-1" href="#inverse-reinforcement-learning"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Inverse Reinforcement Learning</h4><ul>
<li><p><a href="https://dl.acm.org/doi/pdf/10.1145/1015330.1015430" rel="noopener noreferrer">Apprenticeship Learning via Inverse Reinforcement Learning</a> - <em><strong>ICML'04</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=10260011060619377707&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Pieter Abbeel and Andrew Ng's original paper on inverse reinforcement learning (IRL).</p>
</li>
<li><p><a href="https://www.ijcai.org/Proceedings/07/Papers/416.pdf" rel="noopener noreferrer">Bayesian Inverse Reinforcement Learning</a> - <em><strong>IJCAI'07</strong></em>, 2007. [<a href="https://scholar.google.com/scholar?cluster=4154724070362583557&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A Bayesian account on classic inverse reinforcement learning.</p>
</li>
<li><p><a href="https://arxiv.org/abs/1902.07742" rel="noopener noreferrer">From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following</a> - <em><strong>ICLR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=9128320307925997063&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1904.06317.pdf" rel="noopener noreferrer">Few-shot Bayesian imitation learning with logical program policies.</a> - <em><strong>AAAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5103854692762145813&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://export.arxiv.org/pdf/2011.09854" rel="noopener noreferrer">Generalized Inverse Planning: Learning Lifted non-Markovian Utility for Generalizable Task Representation</a> - 2020. [<a href="https://scholar.google.com/scholar?cluster=18369106870663956780&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.mlr.press/v139/malik21a.html" rel="noopener noreferrer">Inverse Constrained Reinforcement Learning</a> - <em><strong>ICML'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Inverse+Constrained+Reinforcement+Learning+S+Malik&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="system-1--system-2"><a class="anchor" aria-hidden="true" tabindex="-1" href="#system-1--system-2"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>System 1 &amp; System 2</h3><h4 id="dual-coding-theory"><a class="anchor" aria-hidden="true" tabindex="-1" href="#dual-coding-theory"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Dual-Coding Theory</h4><ul>
<li><p><a href="https://zh.pb1lib.org/book/1004349/825277" rel="noopener noreferrer">Mental Representations: A Dual Coding Approach</a> - <em><strong>Oxford University Press</strong></em>, 1990. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0,5&amp;q=mental+representations:+a+dual+coding+approach" rel="noopener noreferrer">All Versions</a>]. The original book on dual coding theory, in the neuroscience account of mental representation.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S1364661321001765" rel="noopener noreferrer">Dual coding of knowledge in the human brain</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=11751507203561842501&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Yanchao Bi's review on neuroscience experiments on dual coding theory.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0896627320302798" rel="noopener noreferrer">Two Forms of Knowledge Representations in the Human Brain</a> - <em><strong>Neuron</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=16941965185680116049" rel="noopener noreferrer">All Versions</a>]. Illustrating language-derived and sensory-derived knowledge.</p>
</li>
<li><p><a href="http://bilab.bnu.edu.cn/paper/2018/Wang_2018_Cerebral_Cortex.pdf" rel="noopener noreferrer">Organizational Principles of Abstract Words in the Human Brain</a> - <em><strong>Cerebral Cortex</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=15272192531353715481" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://bilab.bnu.edu.cn/paper/2022/Fu_2022_CC.pdf" rel="noopener noreferrer">Different computational relations in language are captured by distinct brain systems</a> - <em><strong>Cerebral Cortex</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=720215181903530260&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://europepmc.org/article/med/28190038" rel="noopener noreferrer">The Deese-Roediger-McDermott (DRM) task: A simple cognitive paradigm to investigate false memories in the laboratory</a> - <em><strong>Journal of Visualized Experiments</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=10880194606861797581&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://mri-q.com/uploads/3/4/5/7/34572113/gallant_piis0896627312009348.pdf" rel="noopener noreferrer">A continuous semantic space describes the representation of thousands of object and action categories across the human brain</a> - <em><strong>Neuron</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=10348115268396987731&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41562-021-01259-6" rel="noopener noreferrer">Rational arbitration between statistics and rules in human sequence processing</a> - <em><strong>Nature Human Behavior</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=9856085207409198966&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="neural-symbolic-ai"><a class="anchor" aria-hidden="true" tabindex="-1" href="#neural-symbolic-ai"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Neural-Symbolic AI</h4><ul>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-3-642-59789-3_58" rel="noopener noreferrer">Regression Analysis for Interval-Valued Data</a> - <em><strong>Data Analysis, Classification, and Related Methods</strong></em>, 2000. [<a href="https://scholar.google.com/scholar?cluster=9407097855380377791&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on symbolic regression.</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-3-7908-1709-6_20" rel="noopener noreferrer">Symbolic data analysis: what is it?</a> - <em><strong>Proceedings in Computational Statistics</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=3730437602749399283&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/1805.10872" rel="noopener noreferrer">DeepProbLog: Neural Probabilistic Logic Programming</a> - <em><strong>NeurIPS'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=6079567413300944995&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on neuro-symbolic probabilistic programming.</p>
</li>
<li><p><a href="https://www.jair.org/index.php/jair/article/view/11172" rel="noopener noreferrer">Learning Explanatory Rules from Noisy Data</a> - <em><strong>Journal of Artificial Intelligence Research</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=2553893814364678772&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper for differential Inductive Logic Programming.</p>
</li>
<li><p><a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/aaai17lasin.pdf" rel="noopener noreferrer">Combining Logical Abduction and Statistical Induction: Discovering Written Primitives with Human Knowledge</a> - <em><strong>AAAI'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=14477085725208589393&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1904.10729.pdf" rel="noopener noreferrer">Neural Logic Reinforcement Learning</a> - <em><strong>ICML'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=18074632043038701502&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://papers.NeurIPS.cc/paper/8548-bridging-machine-learning-and-logical-reasoning-by-abductive-learning" rel="noopener noreferrer">Bridging Machine Learning and Logical Reasoning by Abductive Learning.</a> - <em><strong>NeurIPS'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=1518342375288126288&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://daiwz.net/org/slides/ABL-meetup.html#/slide-title" rel="noopener noreferrer">Slides</a>]. [<a href="https://github.com/AbductiveLearning/ABL-HED" rel="noopener noreferrer">Code (⭐99)</a>]. The original paper on Abductive Learning, a derivative-free approach for neuro-symbolic learning.</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s11432-018-9801-4" rel="noopener noreferrer">Abductive learning: towards bridging machine learning and logical reasoning</a> - <em><strong>Science China Information Sciences</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=8541635351775190855&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2010.03514.pdf" rel="noopener noreferrer">Abductive Knowledge Induction From Raw Data</a> - <em><strong>IJCAI'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7027142960863064076&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2021/hash/df7e148cabfd9b608090fa5ee3348bfe-Abstract.html" rel="noopener noreferrer">Fast Abductive Learning by Similarity-based Consistency Optimization</a> - <em><strong>NeurIPS'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=8539963460239876225&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. An approach for accelerating the convergence of Abductive Learning.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2019/file/c20a7ce2a627ba838cfbff082db35197-Paper.pdf" rel="noopener noreferrer">Learning by Abstraction: The Neural State Machine</a> - <em><strong>NeurIPS'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7361406080192630148&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0004370220301855" rel="noopener noreferrer">Making sense of sensory input</a> - <em><strong>Artificial Intelligence</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=11875529139573472578&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2103.14230v1.pdf" rel="noopener noreferrer">Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution</a> - <em><strong>CVPR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=4172146500538799638&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openreview.net/pdf?id=SJlh8CEYDB" rel="noopener noreferrer">Learn to explain efﬁciently via neural logic inductive learning</a> - <em><strong>ICLR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=4550874980727321525&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/gblackout/NLIL" rel="noopener noreferrer">Project (⭐44)</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2006.06649" rel="noopener noreferrer">Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning</a> - <em><strong>ICML'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9257372000778020812&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2003.08978" rel="noopener noreferrer">Generating new concepts with hybrid neuro-symbolic models.</a> - <em><strong>CogSci'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=1912020791698331044" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2006.14448" rel="noopener noreferrer">Learning Task-General Representations with Generative Neuro-Symbolic Modeling</a> - <em><strong>ICLR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=1335404082385789329" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://clgiles.ist.psu.edu/IST597/materials/slides/papers-memory/2016-graves.pdf" rel="noopener noreferrer">Hybrid computing using a neural network with dynamic external memory</a> - <em><strong>Nature</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=8100274942961380405&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.science.org/doi/full/10.1126/sciadv.aay2631" rel="noopener noreferrer">AI Feynman: A physics-inspired method for symbolic regression</a> - <em><strong>Science Advances</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=3655502646441210453" rel="noopener noreferrer">All Versions</a>]. A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, the authors develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. The authors apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, this work improves the state-of-the-art success rate from 15 to 90%.</p>
</li>
<li><p><a href="http://papers.NeurIPS.cc/paper/8546-classification-by-components-probabilistic-modeling-of-reasoning-over-a-set-of-components.pdf" rel="noopener noreferrer">Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components</a> - <em><strong>NeurIPS'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=12691103404451941071&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2006.11524.pdf" rel="noopener noreferrer">Neuro-Symbolic Visual Reasoning: Disentangling “Visual” from “Reasoning”</a> - <em><strong>ICML'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13160160974887139307&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2020/file/0d82627e10660af39ea7eb69c3568955-Paper.pdf" rel="noopener noreferrer">Understanding Deep Architectures with Reasoning Layer</a> - <em><strong>NeurIPS'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=937882599430270789&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1905.10307.pdf" rel="noopener noreferrer">An Explicitly Relational Neural Network Architecture</a> - <em><strong>ICML'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=37732747764322837&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2103.01937.pdf" rel="noopener noreferrer">Neural Production Systems</a> - <em><strong>ICML'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15299280949648915581&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Yoshua Bengio's perspective on slot attention model as a general production system.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2008.06662.pdf" rel="noopener noreferrer">Compositional Generalization via Neural-Symbolic Stack Machines</a> - <em><strong>NeurIPS'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=15612498612943317331&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openreview.net/pdf?id=H1eSS3CcKX" rel="noopener noreferrer">Stochastic Optimization of Sorting Networks via Continuous Relaxations</a> - <em><strong>ICLR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=10619362619006891050&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openreview.net/pdf?id=BkxUvnEYDH" rel="noopener noreferrer">Program Guided Agent</a> - <em><strong>ICLR'20</strong></em>, 2020. [<a href="https://openreview.net/forum?id=BkxUvnEYDH" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2020/hash/7a685d9edd95508471a9d3d6fcace432-Abstract.html" rel="noopener noreferrer">Learning Compositional Rules via Neural Program Synthesis</a> - <em><strong>NeurIPS'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=3160670555314650508&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2006.11287" rel="noopener noreferrer">Discovering Symbolic Models from Deep Learning with Inductive Biases</a> - <em><strong>NeurIPS'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9452091824686227240&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1904.11694.pdf" rel="noopener noreferrer">Neural Logic Machines</a> - <em><strong>ICLR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4525183211642569463&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1904.12584.pdf" rel="noopener noreferrer">The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision</a> - <em><strong>ICLR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=8837128214653317831&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://papers.NeurIPS.cc/paper/2019/file/98d8a23fd60826a2a474c5b4f5811707-Paper.pdf" rel="noopener noreferrer">Visual Concept-Metaconcept Learning</a> - <em><strong>NeurIPS'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=1888051343232298875&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.16564" rel="noopener noreferrer">Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning</a> - <em><strong>ICLR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=16735976343684307244&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://jiajunwu.com/papers/toqnet_ijcai.pdf" rel="noopener noreferrer">Temporal and Object Quantification Networks</a> - <em><strong>IJCAI'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=17251222943638414124&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2009.01719.pdf" rel="noopener noreferrer">Grounded Language Learning Fast and Slow</a> - <em><strong>ICLR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=17735027444431750346&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/deepmind/dm_fast_mapping?s=05" rel="noopener noreferrer">Project (⭐54)</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s10994-022-06142-7" rel="noopener noreferrer">Detect, Understand, Act: A Neuro-symbolic Hierarchical Reinforcement Learning Framework</a> - <em><strong>Machine Learning</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=10321228117236432485&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A neuro-symbolic framework that integrates meta-policy learning in inductive logic programming.</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.html" rel="noopener noreferrer">Visual Programming: Compositional Visual Reasoning Without Training</a> - <em><strong>CVPR'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=16156060658942400125&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. VISPROG, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions, using the in-context learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9338352" rel="noopener noreferrer">Semi-Supervised Abductive Learning and Its Application to Theft Judicial Sentencing</a> - <em><strong>ICDM'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16646246740380524224" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.lamda.nju.edu.cn/huangyx/src/ICDM20-SSABL.pdf" rel="noopener noreferrer">Preprint</a>]. In many practical tasks, there are usually two kinds of common information: cheap unlabeled data and domain knowledge in the form of symbols. There are some attempts using one single information source, such as semi-supervised learning and abductive learning. However, there is little work to use these two kinds of information sources at the same time, because it is very difficult to combine symbolic logical representation and numerical model optimization effectively. The learning becomes even more challenging when the domain knowledge is insufficient. This paper presents an attempt-Semi-Supervised ABductive Learning (SS-ABL) framework. In this framework, semi-supervised learning is trained via pseudo labels of unlabeled data generated by abductive learning, and the background knowledge is refined via the label distribution predicted by semi-supervised learning. The above framework can be optimized iteratively and can be naturally interpretable. The effectiveness of the framework has been fully verified in the theft judicial sentencing of real legal documents. In the case of missing sentencing elements and mixed legal rules, the framework is apparently superior to many existing baseline practices, and provides explanatory assistance to judicial sentencing.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="explainability"><a class="anchor" aria-hidden="true" tabindex="-1" href="#explainability"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Explainability</h3><h4 id="trustworthy-ai"><a class="anchor" aria-hidden="true" tabindex="-1" href="#trustworthy-ai"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Trustworthy AI</h4><ul>
<li><p><a href="https://www.pnas.org/doi/full/10.1073/pnas.2111547119" rel="noopener noreferrer">Bayesian modeling of human–AI complementarity</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=15735143859968841009&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A Bayesian framework for combining the predictions and different types of confidence scores from humans and machines.</p>
</li>
<li><p><a href="https://www.science.org/doi/10.1126/scirobotics.aay4663" rel="noopener noreferrer">A tale of two explanations: Enhancing human trust by explaining robot behavior</a> - <em><strong>Science Robotics</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=3985046411399524590" rel="noopener noreferrer">All Versions</a>]. [<a href="https://yzhu.io/publication/openbottle2019scirob/paper.pdf" rel="noopener noreferrer">Preprint</a>]. The ability to provide comprehensive explanations of chosen actions is a hallmark of intelligence. Lack of this ability impedes the general acceptance of AI and robot systems in critical tasks. This paper examines what forms of explanations best foster human trust in machines and proposes a framework in which explanations are generated from both functional and mechanistic perspectives. The robot system learns from human demonstrations to open medicine bottles using (i) an embodied haptic prediction model to extract knowledge from sensory feedback, (ii) a stochastic grammar model induced to capture the compositional structure of a multistep task, and (iii) an improved Earley parsing algorithm to jointly leverage both the haptic and grammar models. The robot system not only shows the ability to learn from human demonstrators but also succeeds in opening new, unseen bottles. Using different forms of explanations generated by the robot system, we conducted a psychological experiment to examine what forms of explanations best foster human trust in the robot. The authors found that comprehensive and real-time visualizations of the robot’s internal decisions were more effective in promoting human trust than explanations based on summary text descriptions. In addition, forms of explanation that are best suited to foster trust do not necessarily correspond to the model components contributing to the best task performance. This divergence shows a need for the robotics community to integrate model components to enhance both task execution and human trust in machines.</p>
</li>
<li><p><a href="https://arxiv.org/abs/1909.06907" rel="noopener noreferrer">X-ToM: Explaining with Theory-of-Mind for Gaining Justified Human Trust</a> - <em><strong>CVPR XAI Workshop'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7751326666821697923" rel="noopener noreferrer">All Versions</a>]. This work presents a new explainable AI (XAI) framework aimed at increasing justified human trust and reliance in the AI machine through explanations. The authors pose explanation as an iterative communication process, i.e. dialog, between the machine and human user. More concretely, the machine generates sequence of explanations in a dialog which takes into account three important aspects at each dialog turn: (a) human's intention (or curiosity); (b) human's understanding of the machine; and (c) machine's understanding of the human user. To do this, the authors use Theory of Mind (ToM) which helps us in explicitly modeling human's intention, machine's mind as inferred by the human as well as human's mind as inferred by the machine. In other words, these explicit mental representations in ToM are incorporated to learn an optimal explanation policy that takes into account human's perception and beliefs. Furthermore, the authors also show that ToM facilitates in quantitatively measuring justified human trust in the machine by comparing all the three mental representations.
We applied our framework to three visual recognition tasks, namely, image classification, action recognition, and human body pose estimation. The authors argue that our ToM based explanations are practical and more natural for both expert and non-expert users to understand the internal workings of complex machine learning models. This is the first work to derive explanations using ToM. Extensive human study experiments verify our hypotheses, showing that the proposed explanations significantly outperform the state-of-the-art XAI methods in terms of all the standard quantitative and qualitative XAI evaluation metrics including human trust, reliance, and explanation satisfaction.</p>
</li>
<li><p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/5643" rel="noopener noreferrer">CoCoX: Generating Conceptual and Counterfactual Explanations via Fault-Lines</a> - <em><strong>AAAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=17443137068166403183&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S2589004221015510" rel="noopener noreferrer">CX-ToM: Counterfactual explanations with theory-of-mind for enhancing human trust in image recognition models</a> - <em><strong>iScience</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=17526041764295337444" rel="noopener noreferrer">All Versions</a>]. This work proposes CX-ToM, short for counterfactual explanations with theory-of-mind, a new explainable AI (XAI) framework for explaining decisions made by a deep convolutional neural network (CNN). In contrast to the current methods in XAI that generate explanations as a single shot response, the authors pose explanation as an iterative communication process, i.e., dialogue between the machine and human user. More concretely, this CX-ToM framework generates a sequence of explanations in a dialogue by mediating the differences between the minds of the machine and human user. To do this, the authors use Theory of Mind (ToM) which helps us in explicitly modeling the human’s intention, the machine’s mind as inferred by the human, as well as human's mind as inferred by the machine. Moreover, most state-of-the-art XAI frameworks provide attention (or heat map) based explanations. In this work, the authors show that these attention-based explanations are not sufficient for increasing human trust in the underlying CNN model. In CX-ToM, the authors instead use counterfactual explanations called fault-lines which are defined as follows: given an input image I for which a CNN classification model M predicts class cpred, a fault-line identifies the minimal semantic-level features (e.g., stripes on zebra), referred to as explainable concepts, that need to be added to or deleted from I to alter the classification category of I by M to another specified class calt. Extensive experiments verify the hypotheses, demonstrating that CX-ToM significantly outperforms the state-of-the-art XAI models.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s42256-023-00692-8" rel="noopener noreferrer">Explaining machine learning models with interactive natural language conversations using TalkToModel</a> - <em><strong>Nature Machine Intelligence</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=7044008493489695982" rel="noopener noreferrer">All Versions</a>]. Practitioners increasingly use machine learning (ML) models, yet models have become more complex and harder to understand. To understand complex models, researchers have proposed techniques to explain model predictions. However, practitioners struggle to use explainability methods because they do not know which explanation to choose and how to interpret the explanation. This work addresses the challenge of using explainability methods by proposing TalkToModel: an interactive dialogue system that explains ML models through natural language conversations. TalkToModel consists of three components: an adaptive dialogue engine that interprets natural language and generates meaningful responses; an execution component that constructs the explanations used in the conversation; and a conversational interface. In real-world evaluations, 73% of healthcare workers agreed they would use TalkToModel over existing systems for understanding a disease prediction model, and 85% of ML professionals agreed TalkToModel was easier to use, demonstrating that TalkToModel is highly effective for model explainability.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="strong-machine-learning"><a class="anchor" aria-hidden="true" tabindex="-1" href="#strong-machine-learning"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Strong Machine Learning</h4><ul>
<li><p><a href="https://link.springer.com/article/10.1007/s10994-018-5707-3" rel="noopener noreferrer">Ultra-Strong Machine Learning: comprehensibility of programs learned with ILP</a> - <em><strong>Machine Learning</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=17551060457946144913" rel="noopener noreferrer">All Versions</a>]. During the 1980s Michie defined Machine Learning in terms of two orthogonal axes of performance: predictive accuracy and comprehensibility of generated hypotheses. Since predictive accuracy was readily measurable and comprehensibility not so, later definitions in the 1990s, such as Mitchell’s, tended to use a one-dimensional approach to Machine Learning based solely on predictive accuracy, ultimately favouring statistical over symbolic Machine Learning approaches. In this paper the authors provide a definition of comprehensibility of hypotheses which can be estimated using human participant trials. The authors present two sets of experiments testing human comprehensibility of logic programs. In the first experiment we test human comprehensibility with and without predicate invention. Results indicate comprehensibility is affected not only by the complexity of the presented program but also by the existence of anonymous predicate symbols. In the second experiment the authors directly test whether any state-of-the-art ILP systems are ultra-strong learners in Michie’s sense, and select the Metagol system for use in humans trials. Results show participants were not able to learn the relational concept on their own from a set of examples but they were able to apply the relational definition provided by the ILP system correctly. This implies the existence of a class of relational concepts which are hard to acquire for humans, though easy to understand given an abstract explanation. The authors believe improved understanding of this class could have potential relevance to contexts involving human learning, teaching and verbal interaction.</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007%2Fs10994-020-05941-0" rel="noopener noreferrer">Beneficial and harmful explanatory machine learning</a> - <em><strong>Machine Learning</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=16983722694047294963" rel="noopener noreferrer">All Versions</a>]. Given the recent successes of Deep Learning in AI there has been increased interest in the role and need for explanations in machine learned theories. A distinct notion in this context is that of Michie’s definition of ultra-strong machine learning (USML). USML is demonstrated by a measurable increase in human performance of a task following provision to the human of a symbolic machine learned theory for task performance. A recent paper demonstrates the beneficial effect of a machine learned logic theory for a classification task, yet no existing work has examined the potential harmfulness of machine’s involvement for human comprehension during learning. This paper investigates the explanatory effects of a machine learned theory in the context of simple two person games and proposes a framework for identifying the harmfulness of machine explanations based on the Cognitive Science literature. The approach involves a cognitive window consisting of two quantifiable bounds and it is supported by empirical evidence collected from human trials. The quantitative and qualitative results indicate that human learning aided by a symbolic machine learned theory which satisfies a cognitive window has achieved significantly higher performance than human self learning. Results also demonstrate that human learning aided by a symbolic machine learned theory that fails to satisfy this window leads to significantly worse performance than unaided human learning.</p>
</li>
<li><p><a href="https://www.ijcai.org/Proceedings/2017/497" rel="noopener noreferrer">Deep Forest: Towards An Alternative to Deep Neural Networks</a> - <em><strong>IJCAI'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=7391596872731517007" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/LAMDA-NJU/Deep-Forest" rel="noopener noreferrer">Project (⭐950)</a>]. This paper proposes gcForest, a decision tree ensemble approach with performance highly competitive to deep neural networks in a broad range of tasks. In contrast to deep neural networks which require great effort in hyper-parameter tuning, gcForest is much easier to train; even when it is applied to different data across different domains in the experiments, excellent performance can be achieved by almost same settings of hyper-parameters. The training process of gcForest is efficient, and users can control training cost according to computational resource available. The efficiency may be further enhanced because gcForest is naturally apt to parallel implementation. Furthermore, in contrast to deep neural networks which require large-scale training data, gcForest can work well even when there are only small-scale training data.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2004.00221" rel="noopener noreferrer">NBDT: Neural-Backed Decision Trees</a> - <em><strong>NeurIPS'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1902399007162005819&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/alvinwan/neural-backed-decision-trees" rel="noopener noreferrer">Code (⭐622)</a>]. Expliciting the decision process of a decision tree through neural networks.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="explainable-deep-learning"><a class="anchor" aria-hidden="true" tabindex="-1" href="#explainable-deep-learning"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Explainable Deep Learning</h4><ul>
<li><p><a href="https://github.com/jacobgil/pytorch-grad-cam" rel="noopener noreferrer">pytorch-grad-cam (⭐12k)</a> - 2021. Class Activation Map methods implemented in Pytorch, with many elegant features.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/8099837" rel="noopener noreferrer">Network dissection: Quantifying interpretability of deep visual representations</a> - <em><strong>CVPR'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=18069685615852396783&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="http://netdissect.csail.mit.edu/" rel="noopener noreferrer">Project</a>]. [<a href="http://places2.csail.mit.edu/index.html" rel="noopener noreferrer">Dataset: Places365</a>]. The original paper on visualizing the class activation maps to explain convolutional neural networks.</p>
</li>
<li><p><a href="https://www.pnas.org/content/pnas/early/2020/08/31/1907375117.full.pdf" rel="noopener noreferrer">Understanding the role of Individual Units in a Deep Neural Network</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=11996680970579301810&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. David Bau's review on network dissection for discriminative and generative models.</p>
</li>
<li><p><a href="https://distill.pub/2020/circuits/zoom-in/" rel="noopener noreferrer">Zoom In: An Introduction to Circuits</a> - <em><strong>Distill</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9053581372570691569&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A perspective on treating neural networks as circuits.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2020/hash/c74956ffb38ba48ed6ce977af6727275-Abstract.html" rel="noopener noreferrer">Compositional Explanations of Neurons</a> - <em><strong>NeurIPS'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=15725346730266402738&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/jayelm/compexp" rel="noopener noreferrer">Project (⭐25)</a>]. A concept-composition version of network dissection.</p>
</li>
<li><p><a href="http://papers.NeurIPS.cc/paper/9095-this-looks-like-that-deep-learning-for-interpretable-image-recognition.pdf" rel="noopener noreferrer">This Looks Like That: Deep Learning for Interpretable Image Recognition</a> - <em><strong>NeurIPS'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=9461838581952136719&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.pnas.org/content/pnas/116/16/7723.full.pdf" rel="noopener noreferrer">Unsupervised learning by competing hidden units</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=1228003598355915526&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2006.09994.pdf" rel="noopener noreferrer">Noise or Signal: The Role of Backgrounds in Image Classification</a> - <em><strong>ICLR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=14729938011425134088&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/MadryLab/backgrounds_challenge" rel="noopener noreferrer">Code &amp; Data (⭐141)</a>]. [<a href="https://gradientscience.org/background/" rel="noopener noreferrer">Project</a>]. A perspective on image background provides strong clue for foreground classification.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2018/hash/5fc34ed307aac159a30d81181c99847e-Abstract.html" rel="noopener noreferrer">Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation</a> - <em><strong>NeurIPS'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=401428033641216502&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Maching the learned pattern of neurons in different neural networks.</p>
</li>
<li><p><a href="https://kriegeskortelab.zuckermaninstitute.columbia.edu/sites/default/files/content/MehrerKietzmann_2020_NatureComms.pdf" rel="noopener noreferrer">Individual differences among deep neural network models</a> - <em><strong>Nature Communications</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=8259893575188417318&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="embodied-intelligence"><a class="anchor" aria-hidden="true" tabindex="-1" href="#embodied-intelligence"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Embodied Intelligence</h3><ul>
<li><p><a href="https://plato.stanford.edu/entries/embodied-cognition/" rel="noopener noreferrer">Embodied Cognition</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Embodied Cognition, which emphasizes the significance of an agent's physical body in cognitive abilities.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/content-externalism/" rel="noopener noreferrer">Externalism About the Mind</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on mind externalism, a long-term debate about the boundary of embodied intelligence.</p>
</li>
<li><p><a href="https://www.researchgate.net/profile/David-Woods-19/publication/242545872_Cognitive_Engineering_Human_Problem_Solving_with_Tools/links/542becf70cf29bbc126ac097/Cognitive-Engineering-Human-Problem-Solving-with-Tools.pdf" rel="noopener noreferrer">Cognitive engineering: Human problem solving with tools</a> - <em><strong>Human Factors</strong></em>, 1988. [<a href="https://scholar.google.com/scholar?cluster=14194840995416222723&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original idea of investigating huamn tool use in problem solving.</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/1993-97340-000" rel="noopener noreferrer">Tools, language and cognition in human evolution</a> - <em><strong>Cambridge University Press</strong></em>, 1993. [<a href="https://scholar.google.com/scholar?cluster=6046350461147957220&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A classic perspective correlating human tool use with the evolution of civilization.</p>
</li>
<li><p><a href="https://icds.uoregon.edu/wp-content/uploads/2014/06/Clark-and-Chalmers-The-Extended-Mind.pdf" rel="noopener noreferrer">The Extended Mind</a> - <em><strong>Analysis</strong></em>, 1998. [<a href="https://scholar.google.com/scholar?cluster=9546561188261943866&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on the debate of mind externalism.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S1364661303003231" rel="noopener noreferrer">The neural bases of complex tool use in humans</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=3612212926196611828&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A neuroscience account of human tool use.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0960982207017708" rel="noopener noreferrer">Spontaneous Metatool Use by New Caledonian Crows</a> - <em><strong>Current Biology</strong></em>, 2007. [<a href="https://scholar.google.com/scholar?cluster=9263531730425342443&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A piece of evidence that intelligent animals can take advantage of matatools to make tools for problem solving.</p>
</li>
<li><p><a href="https://journals.sagepub.com/doi/abs/10.1177/0956797610371962" rel="noopener noreferrer">Rapid Assimilation of External Objects Into the Body Schema</a> - <em><strong>Psychological Science</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=854636910326733489&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.eva.mpg.de/documents/Cambridge/Tennie_Cultural_BehBrainSci_2012_1566208.pdf" rel="noopener noreferrer">The cognitive bases of human tool use</a> - <em><strong>Behavioral and Brain Sciences</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=4648150119820414671&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00214/full" rel="noopener noreferrer">The embodied mind extended: using words as social tools</a> - <em><strong>Frontiers in Psychology</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=14719988081062606352&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://royalsocietypublishing.org/doi/10.1098/rstb.2012.0408" rel="noopener noreferrer">Tool use as adaptation</a> - <em><strong>Philosophical Transactions of the Royal Society B: Biological Sciences</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=8060841461200774807&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0028393214000232" rel="noopener noreferrer">Intensive tool-practice and skillfulness facilitate the extension of body representations in humans</a> - <em><strong>Neuropsychologia</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=10578024091098127929&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2Frev0000027" rel="noopener noreferrer">Tool use and affordance: Manipulation-based versus reasoning-based approaches</a> - <em><strong>Psychological Review</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=3284942486402374505&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A classic review on human tool use and affordance.</p>
</li>
<li><p><a href="https://escholarship.org/uc/item/5gf0m7x3" rel="noopener noreferrer">Meta-strategy learning in physical problem-solving: the effect of embodied experience</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=9713842177532954702" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://yzhu.io/publication/tool2015cvpr/paper.pdf" rel="noopener noreferrer">Understanding Tools: Task-Oriented Object Modeling, Learning and Recognition</a> - <em><strong>CVPR'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=4609926671953500969&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://yzhu.io/publication/tool2015cvpr/" rel="noopener noreferrer">Project</a>]. The original paper introducing affordance and physically-grounded tool use into computer vision.</p>
</li>
<li><p><a href="https://robotics.sciencemag.org/content/6/54/eabd7935.abstract" rel="noopener noreferrer">Robotic hand augmentation drives changes in neural body representation</a> - <em><strong>Science Robotics</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=1622125726197763917&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.jneurosci.org/content/jneuro/41/13/2980.full.pdf" rel="noopener noreferrer">Expert Tool Users Show Increased Differentiation between Visual Representations of Hands and Tools</a> - <em><strong>Journal of Neuroscience</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=13454164767827515188&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2106.05654.pdf" rel="noopener noreferrer">Visual scoping operations for physical assembly</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7238090583833839&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.cc.gatech.edu/ai/robot-lab/online-publications/StoytchevICRA2005.pdf" rel="noopener noreferrer">Behavior-grounded representation of tool affordances</a> - <em><strong>ICRA'05</strong></em>, 2005. [<a href="https://scholar.google.com/scholar?cluster=6115815663915603675&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-3-642-38812-5_1" rel="noopener noreferrer">A Relational Approach to Tool-Use Learning in Robots</a> - <em><strong>ILP'12</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=18374178227592386332&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s10514-017-9637-x" rel="noopener noreferrer">Relational affordances for multiple-object manipulation</a> - <em><strong>Autonomous Robots</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=6357646940615855682&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://m.roboticsproceedings.org/rss15/p01.pdf" rel="noopener noreferrer">Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight</a> - <em><strong>RSS'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4316276917607326251&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://escholarship.org/uc/item/5gf0m7x3" rel="noopener noreferrer">Meta-strategy learning in physical problem-solving: the effect of embodied experience</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=9713842177532954702" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/SHI-Yu-Zhe/awesome-agi-cocosci/blob/master/README.md/" rel="noopener noreferrer">Preprint</a>]. This paper focuses on how natural embodied experience affects what kinds of abstract physical problem-solving strategies people use in a virtual task. The findings suggest that differences in embodied experience drive the acquisition of different meta-strategies for balancing acting with thinking, deciding what kinds of actions to try, and deciding how persistent to be with a current action plan.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2002.06289" rel="noopener noreferrer">3D dynamic scene graphs: Actionable spatial perception with places, objects, and humans</a> - <em><strong>RSS'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=4428742298455436054&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A system for modeling 3D dynamic scene graphs on multiple levels (metric-semantic mesh, objects and agents, places and structures, rooms, and buildings).</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="evolutionary-intelligence"><a class="anchor" aria-hidden="true" tabindex="-1" href="#evolutionary-intelligence"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Evolutionary Intelligence</h3><ul>
<li><p><a href="http://websites.umich.edu/~zhanglab/clubPaper/06_08_2012.pdf" rel="noopener noreferrer">Evolutionary trade-offs, Pareto optimality, and the geometry of phenotype space</a> - <em><strong>Science</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=16162252507845975080&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A classic paper correlating biological trade-offs with the evolution of pareto optimality.</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/BF01442131" rel="noopener noreferrer">Pareto optimality in multiobjective problems</a> - <em><strong>Applied Mathematics and Optimization</strong></em>, 1977. [<a href="https://scholar.google.com/scholar?cluster=11305142600366783354&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on the pareto optimality in multiobjective problems.</p>
</li>
<li><p><a href="http://www.soft-computing.de/SMC0805.pdf" rel="noopener noreferrer">Pareto-Based Multiobjective Machine Learning: An Overview and Case Studies</a> - <em><strong>IEEE Transactions on Systems, Man, and Cybernetics</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=11308312498510305429&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A comprehensive review on the application of pareto optimality to multiobjective machine learning.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-019-1153-z" rel="noopener noreferrer">Phylogenetic evidence for Sino-Tibetan origin in northern China in the Late Neolithic</a> - <em><strong>Nature</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=13913123623752818925&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A Bayesian phylogenetic analysis on two competing hypotheses of the origin of the Sino-Tibetan language family suggests that the initial expansion of Sino-Tibetan languages occurred approximately 4,000–6,000 years before present (BP; taken as AD 1950) in the Yellow River basin of northern China, and that this expansion is associated with the development of the Yangshao and/or Majiayao Neolithic cultures.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-021-04108-8" rel="noopener noreferrer">Triangulation supports agricultural spread of the Transeurasian languages</a> - <em><strong>Nature</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=1183005894965630508&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.nature.com/articles/d41586-021-03037-w" rel="noopener noreferrer">Nature News</a>]. A triangulation of linguistic, archaeological and genetic data suggests that the Transeurasian language family originated in a population of grain farmers in China around 9,000 years ago, and that agriculture underpinned its spread.</p>
</li>
<li><p><a href="https://www.science.org/doi/abs/10.1126/science.ade7981" rel="noopener noreferrer">From language development to language evolution: A unified view of human lexical creativity</a> - <em><strong>Science</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=15871163761816546924&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://brochhagen.github.io/content/ms/accepted-lexical-creativity.pdf" rel="noopener noreferrer">Preprint</a>]. This work supports a unified foundation for human lexical creativity underlying both the fleeting products of individual ontogeny and the evolutionary products of phylogeny across languages.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="methodologies-for-experiments"><a class="anchor" aria-hidden="true" tabindex="-1" href="#methodologies-for-experiments"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Methodologies for Experiments</h3><h4 id="quantitative-analysis"><a class="anchor" aria-hidden="true" tabindex="-1" href="#quantitative-analysis"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Quantitative Analysis</h4><ul>
<li><p><a href="http://www.jakebowers.org/ITVExperiments/angristimbensrubin96.pdf" rel="noopener noreferrer">Identification of Causal Effects Using Instrumental Variables</a> - <em><strong>Journal of the American Statistical Association</strong></em>, 1996. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=17166265099721941605" rel="noopener noreferrer">All Versions</a>]. The original paper on Instrumental Variables for natural sociology studies.</p>
</li>
<li><p><a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-psych-122414-033702" rel="noopener noreferrer">Experiments with More Than One Random Factor: Designs, Analytic Models, and Statistical Power</a> - <em><strong>Annual Review of Psychology</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=6652444619934494760&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A comprehensive review of the quantitative analysis techniques for behavioral studies.</p>
</li>
<li><p><a href="https://mpra.ub.uni-muenchen.de/4823/1/MPRA_paper_4823.pdf" rel="noopener noreferrer">With or Without U? The Appropriate Test for a U-Shaped Relationship</a> - <em><strong>Oxford Bulletin of Economics and Statistics</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=1574723532506536904&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original method for testing U-shape relation from the data, which is distinctive from the quadratic regression test.</p>
</li>
<li><p><a href="https://journals.sagepub.com/doi/pdf/10.1177/2515245918805755" rel="noopener noreferrer">Two lines: A valid alternative to the invalid testing of U-shaped relationships with quadratic regressions</a> - <em><strong>Advances in Methods and Practices in Psychological Science</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=12010185803500406162&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. An alternative method to test the statistical significance of U-shaped relationships.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="scaling-up-behavioral-studies"><a class="anchor" aria-hidden="true" tabindex="-1" href="#scaling-up-behavioral-studies"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Scaling Up Behavioral Studies</h4><ul>
<li><p><a href="https://osf.io/wksv8" rel="noopener noreferrer">Scaling up experimental social, behavioral, and economic science</a> - <em><strong>Open Science Foundation Preprints</strong></em>. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Scaling+up+experimental+social%2C+behavioral%2C+and+economic+science&amp;btnG=" rel="noopener noreferrer">All Versions</a>]. A white paper on scaling up social, behavioral, and econimic experiments.</p>
</li>
<li><p><a href="https://scholar.harvard.edu/files/henrich/files/henrich_heine_norenzayan_2010-2.pdf" rel="noopener noreferrer">The weirdest people in the world?</a> - <em><strong>Brain and Behavioral Sciences</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=3129419557801277936&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on rethinking and tackling the sample bias in behaivoral studies, where most subjects are drawn from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies.</p>
</li>
<li><p><a href="https://www.pnas.org/doi/10.1073/pnas.1915841117" rel="noopener noreferrer">Scaling up psychology via Scientific Regret Minimization</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=8011895688226766944&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The statistical and ecological basis for scaling up behavioral studies.</p>
</li>
<li><p><a href="https://cpb-us-w2.wpmucdn.com/web.sas.upenn.edu/dist/a/511/files/2021/06/Bhatia-He-Science.pdf" rel="noopener noreferrer">Machine-generated theories of human decision-making</a> - <em><strong>Science</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7065547001880027350&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://cocosci.princeton.edu/jpeterson/papers/peterson2021-science.pdf" rel="noopener noreferrer">Using large-scale experiments and machine learning to discover theories of human decision-making</a> - <em><strong>Science</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7456250222852859810&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A piece of evidence for the merits brought by large-scale behavioral studies in social science.</p>
</li>
<li><p><a href="http://jakehofman.com/pdfs/integrating-prediction-and-explanation.pdf" rel="noopener noreferrer">Integrating explanation and prediction in computational social science</a> - <em><strong>Nature</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=288245575125750925&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://cocosci.princeton.edu/josh/papers/griffiths-largeimagedatabases-topics2016.pdf" rel="noopener noreferrer">Exploring human cognition using large image databases</a> - <em><strong>Topics in Cognitive Sciences</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=3629906005701226294&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://web.archive.org/web/20170809024454id_/http://www.kevinjing.com/visual_search_at_pinterest.pdf" rel="noopener noreferrer">Visual Search at Pinterest</a> - <em><strong>KDD'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=2051024301293529405&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Large scale user study in the development of the recommendations system by Pinterest.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="decision-making"><a class="anchor" aria-hidden="true" tabindex="-1" href="#decision-making"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Decision Making</h4><ul>
<li><a href="https://link.springer.com/article/10.3758/s13428-022-01789-5" rel="noopener noreferrer">A computational process-tracing method for measuring people’s planning strategies and how they change over time</a> - <em><strong>Behavior Research Methods</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=10405935000926098041" rel="noopener noreferrer">All Versions</a>]. Model-based strategy identification.</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="question-answering"><a class="anchor" aria-hidden="true" tabindex="-1" href="#question-answering"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Question Answering</h4><ul>
<li><p><a href="https://cogsci.mindmodeling.org/2016/papers/0122/paper0122.pdf" rel="noopener noreferrer">Searching large hypothesis spaces by asking questions</a> - <em><strong>CogSci'16</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=3398849603439166012&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A behavioral study for the 20 questions game.</p>
</li>
<li><p><a href="https://gureckislab.org/papers/RotheLakeGureckis-2016cogsci.pdf" rel="noopener noreferrer">Asking and evaluating natural language questions</a> - <em><strong>CogSci'16</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=34641833161282231&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A behavioral study for the battleship game.</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s42113-018-0005-5" rel="noopener noreferrer">Do People Ask Good Questions?</a> - <em><strong>Computational Brain &amp; Behavior</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=14595996621617337270&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://nyuccl.org/papers/Rothe-Lake-Gureckis-2019-Cogsci.pdf" rel="noopener noreferrer">Asking goal-oriented questions and learning from answers</a> - <em><strong>CogSci'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=14185546187726917682&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="human-machine-comparison"><a class="anchor" aria-hidden="true" tabindex="-1" href="#human-machine-comparison"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Human-Machine Comparison</h4><ul>
<li><p><a href="https://psycnet.apa.org/record/1973-00249-001" rel="noopener noreferrer">Elimination by aspects: A theory of choice</a> - <em><strong>Psychological Review</strong></em>, 1972. [<a href="https://scholar.google.com/scholar?cluster=1633792484482810297&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Herbert Simon's early experiments on computer aided behavioral studies.</p>
</li>
<li><p><a href="https://stacks.stanford.edu/file/druid:qv796fc9687/qv796fc9687.pdf" rel="noopener noreferrer">Problem Solving and Rule Induction: A Unified View</a> - <em><strong>Knowledge and cognition</strong></em>, 1974. [<a href="https://scholar.google.com/scholar?cluster=12943734683291006234&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.pnas.org/content/112/37/11708.short" rel="noopener noreferrer">Evidence integration in model-based tree search</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=11085043350027609187&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/content/pdf/10.1007/s42113-019-00053-y.pdf" rel="noopener noreferrer">People Infer Recursive Visual Concepts from Just a Few Examples</a> - <em><strong>Computational Brain &amp; Behavior</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=3871396883970734141&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://escholarship.org/content/qt3xf2n3vc/qt3xf2n3vc.pdf" rel="noopener noreferrer">One-shot learning of generative speech concepts</a> - <em><strong>CogSci'14</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=15482292457660075957&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/1901.04587" rel="noopener noreferrer">Human few-shot learning of compositional instructions</a> - <em><strong>CogSci'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=12841163907815018136&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2103.05823.pdf" rel="noopener noreferrer">Fast and flexible: Human program induction in abstract reasoning tasks</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=5294483826040237516&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://proceedings.mlr.press/v80/dubey18a.html" rel="noopener noreferrer">Investigating Human Priors for Playing Video Games</a> - <em><strong>ICML'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=2202192690517876762&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S2352154619300622" rel="noopener noreferrer">Tasks for aligning human and machine planning</a> - <em><strong>Current Opinion in Behavioral Sciences</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=8308872468787875598&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://perception.jhu.edu/files/PDFs/19_Adversarial_Deciphering/ZhouFirestone-AdversarialDeciphering.pdf" rel="noopener noreferrer">Humans can decipher adversarial images</a> - <em><strong>Nature Communications</strong></em>. 2019. [<a href="https://scholar.google.com/scholar?cluster=4423950118844131054&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41593-022-01026-4.pdf" rel="noopener noreferrer">Shared computational principles for language processing in humans and deep language models</a> - <em><strong>Nature Neuroscience</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=16078004657063602593&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="association-test"><a class="anchor" aria-hidden="true" tabindex="-1" href="#association-test"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Association Test</h4><ul>
<li><p><a href="https://en.wikipedia.org/wiki/Implicit-association_test" rel="noopener noreferrer">Implicit Association Test</a> - <em><strong>Wikipedia</strong></em>. Wikipedia on the Implicit Association Test, a controversial assessment intended to detect subconscious associations between mental representations of objects (concepts) in memory.</p>
</li>
<li><p><a href="http://faculty.fortlewis.edu/burke_b/Senior/BLINK%20replication/IAT.pdf" rel="noopener noreferrer">Measuring Individual Differences in Implicit Cognition: The Implicit Association Test</a> - <em><strong>Journal of Personality and Social Psychology</strong></em>, 1998. [<a href="https://scholar.google.com/scholar?cluster=302378224541015580&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper introducing the Implicit Association Test.</p>
</li>
<li><p><a href="http://faculty.washington.edu/agg/pdf/Gwald_Nosek_ZEITSCHR_2001.OCR.pdf" rel="noopener noreferrer">Health of the Implicit Association Test at age 3</a> - <em><strong>Zeitschrift für Experimentelle Psychologie</strong></em>, 2001. [<a href="https://scholar.google.com/scholar?cluster=10868478693422595588&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The 3rd year review for the IAT.</p>
</li>
<li><p><a href="https://faculty.washington.edu/agg/pdf/Nosek%20&amp;%20al.IATatage7.2007.pdf" rel="noopener noreferrer">The Implicit Association Test at Age 7: A Methodological and Conceptual Review</a> - <em><strong>Social psychology and the unconscious: The automaticity of higher mental processes (pp. 265–292), Psychology Press</strong></em>, 2007. [<a href="https://scholar.google.com/scholar?cluster=16189750920013376566&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The 7th year review for the IAT.</p>
</li>
<li><p><a href="http://faculty.washington.edu/agg/IATmaterials/PDFs/Hofmann%20&amp;%20al%20(PSPB,2005).pdf" rel="noopener noreferrer">A Meta-Analysis on the Correlation Between the Implicit Association Test and Explicit Self-Report Measures</a> - <em><strong>Personality and Social Psychology Bulletin</strong></em>, 2005. [<a href="https://scholar.google.com/scholar?cluster=4888328728717829047&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="virtual-reality"><a class="anchor" aria-hidden="true" tabindex="-1" href="#virtual-reality"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Virtual Reality</h4><ul>
<li><p><a href="https://www.nature.com/articles/nn948" rel="noopener noreferrer">Virtual reality in behavioral neuroscience and beyond</a> - <em><strong>Nature Neuroscience</strong></em>, 2002. [<a href="https://scholar.google.com/scholar?cluster=12168354203281280346&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A classic review on the early applications of Virtual Reality to behavioral studies.</p>
</li>
<li><p><a href="https://stanfordvr.com/mm/2009/fox-jmp-vr-survival.pdf" rel="noopener noreferrer">Virtual reality: A survival guide for the social scientist</a> - <em><strong>Journal of Media Psychology</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=17318470193315023264&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/2022-60836-006" rel="noopener noreferrer">The psychology of virtual reality</a> - <em><strong>The psychology of technology: Social science research in the age of Big Data (pp. 155–193), American Psychological Association</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=11535480055596209683&amp;hl=en&amp;as_sdt=0,5&amp;as_ylo=2021" rel="noopener noreferrer">All Versions</a>]. Jeremy Bailenson's review on the applications of Virtual Reality to behavioral studies.</p>
</li>
<li><p><a href="https://stanfordvr.com/mm/2015/cummings-mp-how-immersive.pdf" rel="noopener noreferrer">How Immersive Is Enough? A Meta-Analysis of the Effect of Immersive Technology on User Presence</a> - <em><strong>Media Psychology</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=9218122072360464558&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A meta-analysis on the extent to which technologies need to be immersive in order to generate a sense of presence.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/10108427" rel="noopener noreferrer">Towards an Understanding of Distributed Asymmetric Collaborative Visualization on Problem-solving</a> - <em><strong>VR'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11228377215337222005&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/3139131.3139152" rel="noopener noreferrer">Agent: automatic generation of experimental protocol runtime</a> - <em><strong>VRST'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=3511549412244980073" rel="noopener noreferrer">All Versions</a>]. This paper proposes the use of Domain-Specific Languages (DSLs) to ease the description and generation of VR experiments, thus letting experiment designers focus on their core tasks: designing, conducting, and reporting experiments.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/3654777.3676358" rel="noopener noreferrer">What's the Game, then? Opportunities and Challenges for Runtime Behavior Generation</a> - <em><strong>UIST'24</strong></em>, 2024. [<a href="https://github.com/SHI-Yu-Zhe/awesome-agi-cocosci/blob/master/README.md/" rel="noopener noreferrer">All Versions</a>]. Procedural content generation (PCG), the process of algorithmically creating game components instead of manually, has been a common tool of game development for decades. Recent advances in large language models (LLMs) enable the generation of game behaviors based on player input at runtime. Such code generation brings with it the possibility of entirely new gameplay interactions that may be difficult to integrate with typical game development workflows. This work explores these implications through GROMIT, a novel LLM-based runtime behavior generation system for Unity. When triggered by a player action, GROMIT generates a relevant behavior which is compiled without developer intervention and incorporated into the game.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="meta-level-considerations"><a class="anchor" aria-hidden="true" tabindex="-1" href="#meta-level-considerations"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Meta-Level Considerations</h3><h4 id="meta-learning"><a class="anchor" aria-hidden="true" tabindex="-1" href="#meta-learning"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Meta Learning</h4><ul>
<li><p><a href="https://arxiv.org/pdf/2201.03916.pdf" rel="noopener noreferrer">Automated Reinforcement Learning (AutoRL): A Survey and Open Problems</a> - 2022. [<a href="https://scholar.google.com/scholar?cluster=9025378857688824887&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A comprehensive review on AutoRL.</p>
</li>
<li><p><a href="https://proceedings.mlr.press/v70/finn17a/finn17a.pdf" rel="noopener noreferrer">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a> - <em><strong>ICML'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=17278604844873996878" rel="noopener noreferrer">All Versions</a>]. [<a href="https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/" rel="noopener noreferrer">Post</a>]. Chelsea Finn's original paper on Model-Agnostic Meta-Learning (MAML).</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2018/hash/e1021d43911ca2c1845910d84f40aeae-Abstract.html" rel="noopener noreferrer">Bayesian Model-Agnostic Meta-Learning</a> - <em><strong>NeurIPS'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=7370333111335795917&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A Bayesian account on MAML.</p>
</li>
<li><p><a href="https://openreview.net/forum?id=SJeD3CEFPH" rel="noopener noreferrer">Meta-Q-Learning</a> - <em><strong>ICLR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=2865388954464396222&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The milestone paper on context Meta-RL.</p>
</li>
<li><p><a href="http://proceedings.mlr.press/v97/rakelly19a.html" rel="noopener noreferrer">Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables</a> - <em><strong>ICML'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15379570585451726919&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openreview.net/forum?id=TQt98Ya7UMP" rel="noopener noreferrer">Balancing Constraints and Rewards with Meta-Gradient D4PG</a> - <em><strong>ICLR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=2805226315118298313&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openreview.net/forum?id=Bk8BvDqex" rel="noopener noreferrer">Metacontrol for Adaptive Imagination-Based Optimization</a> - <em><strong>ICLR'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=16728474512617398730&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2021/hash/1e4d36177d71bbb3558e43af9577d70e-Abstract.html" rel="noopener noreferrer">On Effective Scheduling of Model-based Reinforcement Learning</a> - <em><strong>NeurIPS'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=11128521607771619105&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="marrs-levels-of-analysis"><a class="anchor" aria-hidden="true" tabindex="-1" href="#marrs-levels-of-analysis"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Marr's Levels of Analysis</h4><ul>
<li><p><a href="https://usa1lib.org/book/1223444/8e5ca8" rel="noopener noreferrer">Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</a> - <em><strong>MIT Press</strong></em>, 1982. [<a href="https://scholar.google.com/scholar?cluster=14386368570811483142&amp;hl=en&amp;as_sdt=0,44" rel="noopener noreferrer">All Versions</a>]. David Marr's original book on the levels of analysis.</p>
</li>
<li><p><a href="https://dspace.mit.edu/bitstream/handle/1721.1/5782/AIM-357.pdf?sequence=2" rel="noopener noreferrer">From understanding computation to understanding neural circuitry</a> - <em><strong>Neuroscience Research Program Bulletin</strong></em>, 1979. [<a href="https://scholar.google.com/scholar?start=0&amp;hl=en&amp;as_sdt=0,5&amp;cluster=11150567121969913334" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://cocosci.princeton.edu/tom/papers/LabPublications/BridgingLevelsAnalysis.pdf" rel="noopener noreferrer">Bridging Levels of Analysis for Probabilistic Models of Cognition</a> - <em><strong>Current Directions in Psychological Science</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=5063382112136991296&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A Marr's paradigm account on probabilistic models.</p>
</li>
<li><p><a href="https://people.csail.mit.edu/pkrafft/papers/krafft-griffiths-levels-css.pdf" rel="noopener noreferrer">Levels of Analysis in Computational Social Science</a> - <em><strong>CogSci'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=10178929388985626844&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A Marr's paradigm account on computational social science.</p>
</li>
<li><p><a href="https://baicsworkshop.github.io/pdf/BAICS_6.pdf" rel="noopener noreferrer">Levels of Analysis for Machine Learning</a> - <em><strong>ICLR'20 Bridging AI and Cognitive Science Workshop</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13819038971626384115&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A Marr's paradigm account on machine learning.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="gestalt"><a class="anchor" aria-hidden="true" tabindex="-1" href="#gestalt"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Gestalt</h4><ul>
<li><p><a href="https://psycnet.apa.org/record/2007-10344-001" rel="noopener noreferrer">Gestalt theory</a> - <em><strong>A source book of Gestalt psychology</strong></em>, 1938. [<a href="https://scholar.google.com/scholar?cluster=18133275659218646817&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original book on Gestalt psychology.</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/BF00422382" rel="noopener noreferrer">Gestalt Psychology</a> - <em><strong>Psychologische Forschung</strong></em>, 1967. [<a href="https://scholar.google.com/scholar?cluster=16023098380090751616&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Wolfgang Köhler's review on Gestalt psychology.</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9450.1984.tb01001.x" rel="noopener noreferrer">Restructuring revisited I. Summary and critique of the Gestalt theory of problem solving</a> - <em><strong>Scandinavian Journal of Psychology</strong></em>, 1984. [<a href="https://scholar.google.com/scholar?cluster=1540079499182933565&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9450.1984.tb01005.x" rel="noopener noreferrer">Restructuring revisited II. An information processing theory of restructuring and insight</a> - <em><strong>Scandinavian Journal of Psychology</strong></em>, 1984. [<a href="https://scholar.google.com/scholar?cluster=1821980539002417470&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/1993-36184-001" rel="noopener noreferrer">Thoughts beyond words: When language overshadows insight</a> - <em><strong>Journal of Experimental Psychology</strong></em>, 1993. [<a href="https://scholar.google.com/scholar?cluster=13773440938721955384&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://hk1lib.org/book/1244721/20ddc5" rel="noopener noreferrer">Deep Learning: How the Mind Overrides Experience</a> - <em><strong>Cambridge University Press</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=231021877034210140" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="the-aha-moment"><a class="anchor" aria-hidden="true" tabindex="-1" href="#the-aha-moment"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The Aha! Moment</h4><ul>
<li><p><a href="https://en.wikipedia.org/wiki/Eureka_effect" rel="noopener noreferrer">Eureka Effect</a> - <em><strong>Wikipedia</strong></em>. Wikipedia on Eureka effect (a.k.a. Aha! moment, insight, and epiphany), the common human experience of suddenly understanding a previously incomprehensible problem or concept.</p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/Insight" rel="noopener noreferrer">Insight</a> - <em><strong>Wikipedia</strong></em>. Wikipedia on insight.</p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/Epiphany_(feeling)" rel="noopener noreferrer">Epiphany</a> - <em><strong>Wikipedia</strong></em>. Wikipedia on epiphany, the "feeling" when the Aha! moment comes.</p>
</li>
<li><p><a href="https://escholarship.org/uc/item/54x8v354" rel="noopener noreferrer">A computational model of scientific insight</a> - <em><strong>The nature of creativity: Contemporary psychological perspectives</strong></em>, 1988. [<a href="https://scholar.google.com/scholar?cluster=13633357571064621019&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A computational account on insights for scientific discovery.</p>
</li>
<li><p><a href="https://www.researchgate.net/profile/Thomas-Ormerod/publication/8909475_What_Makes_an_Insight_Problem_The_Roles_of_Heuristics_Goal_Conception_and_Solution_Recoding_in_Knowledge-Lean_Problems/links/00b7d5159f3c057eb5000000/What-Makes-an-Insight-Problem-The-Roles-of-Heuristics-Goal-Conception-and-Solution-Recoding-in-Knowledge-Lean-Problems.pdf" rel="noopener noreferrer">What Makes an Insight Problem? The Roles of Heuristics, Goal Conception, and Solution Recoding in Knowledge-Lean Problems</a> - <em><strong>Journal of Experimental Psychology</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=17529631069707671285&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://psycnet.apa.org/record/2003-10949-002" rel="noopener noreferrer">APA</a>].</p>
</li>
<li><p><a href="https://www.hf.uni-koeln.de/data/fgpsych/File/Haider/Knoblich_etal_1999.pdf" rel="noopener noreferrer">Constraint relaxation and chunk decomposition in insight problem solving</a> - <em><strong>Journal of Experimental Psychology</strong></em>, 1999. [<a href="https://scholar.google.com/scholar?cluster=8057214169831054227&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://psycnet.apa.org/record/1999-01477-011" rel="noopener noreferrer">APA</a>].</p>
</li>
<li><p><a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=818fec7c896ea3716eeb637da095293e9e6d1806" rel="noopener noreferrer">Dynamics and constraints in insight problem solving</a> - <em><strong>Journal of Experimental Psychology</strong></em>, 2002. [<a href="https://scholar.google.com/scholar?cluster=12067671710370549516&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://psycnet.apa.org/record/2002-01361-014" rel="noopener noreferrer">APA</a>].</p>
</li>
<li><p><a href="https://bpb-us-e1.wpmucdn.com/sites.northwestern.edu/dist/a/699/files/2015/11/Salvi_etal_Insight-is-right_TR2016-2n3ns9l.pdf" rel="noopener noreferrer">Insight solutions are correct more often than analytic solutions</a> - <em><strong>Thinking &amp; Reasoning</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=883561570778414219&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=1094&amp;context=jps" rel="noopener noreferrer">Human Performance on Insight Problem Solving: A Review</a> - <em><strong>The Journal of Problem Solving</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?cluster=15913242870565808883&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01424/full" rel="noopener noreferrer">Insight Is Not in the Problem: Investigating Insight in Problem Solving across Task Types</a> - <em><strong>Frontiers in Psychology</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=4564128114316001308&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.researchgate.net/profile/Trina-Kershaw/publication/8909474_Multiple_Causes_of_Difficulty_in_Insight_The_Case_of_the_Nine-Dot_Problem/links/55dca27e08aeb38e8a8d23b6/Multiple-Causes-of-Difficulty-in-Insight-The-Case-of-the-Nine-Dot-Problem.pdf" rel="noopener noreferrer">Multiple Causes of Difficulty in Insight: The Case of the Nine-Dot Problem</a> - <em><strong>Journal of Experimental Psychology</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=15600199808825346018&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://psycnet.apa.org/record/2003-10949-001" rel="noopener noreferrer">APA</a>].</p>
</li>
<li><p><a href="https://www.researchgate.net/profile/Gary-Jones-14/publication/23152585_Investigating_the_Effect_of_Mental_Set_on_Insight_Problem_Solving/links/0fcfd50abb767b1102000000/Investigating-the-Effect-of-Mental-Set-on-Insight-Problem-Solving.pdf" rel="noopener noreferrer">Investigating the effect of Mental Set on Insight Problem Solving</a> - <em><strong>Experimental Psychology</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=11054712671934144981&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="rationality"><a class="anchor" aria-hidden="true" tabindex="-1" href="#rationality"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Rationality</h4><ul>
<li><p><a href="https://plato.stanford.edu/entries/bounded-rationality/" rel="noopener noreferrer">Bounded Rationality</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Bounded Rationality, an elementary hypothesis of human intelligence in psychology and ecology.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/rationality-instrumental/" rel="noopener noreferrer">Instrumental Rationality</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Instrumental Rationality, a dabate on whether an agent's decision is made intentionally or out of rational coherence.</p>
</li>
<li><p><a href="https://www.taylorfrancis.com/books/mono/10.4324/9781315083223/study-thinking-jerome-bruner-jacqueline-goodnow-george-austin" rel="noopener noreferrer">A Study of Thinking</a> - <em><strong>Routledge</strong></em>, 1956. [<a href="https://scholar.google.com/scholar?cluster=17466297915128086930" rel="noopener noreferrer">All Versions</a>]. This book is a pioneering account of how human beings achieve a measure of rationality in spite of the constraints imposed by time and ignorance.</p>
</li>
<li><p><a href="http://act-r.psy.cmu.edu/wordpress/wp-content/uploads/2012/12/89AdaptiveNature.pdf" rel="noopener noreferrer">The Adaptive Nature of Human Categorization Behavior</a> - <em><strong>Psychological Review</strong></em>, 1991. [<a href="https://scholar.google.com/scholar?cluster=7349048316173616836&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper that relates cognitive resource limitation with Bayesian rational analysis, in the case of categorization behavior.</p>
</li>
<li><p><a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(03)00028-7?large_figure=true&amp;mobileUi=0" rel="noopener noreferrer">Task switching</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2003. [<a href="https://scholar.google.com/scholar?cluster=676255515965300942&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="http://psychfiles.net/experimental/Monsell_2003.pdf" rel="noopener noreferrer">Preprint</a>]. The original paper on ``switch cost'', where subjects' responses are substantially slower and, usually, more error-prone immediately after a task switch.</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/tops.12086" rel="noopener noreferrer">Computational Rationality: Linking Mechanism and Behavior Through Bounded Utility Maximization</a> - <em><strong>Topics in Cognitive Science</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=15813211310327194798&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Introducing the computational rationality framework for including information-processing bounds in rational analyses, which emphasizes the incorporation of computational mechanism into the definition of rational action.</p>
</li>
<li><p><a href="https://gershmanlab.com/pubs/GershmanHorvitzTenenbaum15.pdf" rel="noopener noreferrer">Computational rationality: A converging paradigm for intelligence in brains, minds, and machines</a> - <em><strong>Science</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=7744057022238735461&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A comprehensive review on the rationality of Bayesian computational models.</p>
</li>
<li><p><a href="https://cocosci.princeton.edu/papers/lieder_resource.pdf" rel="noopener noreferrer">Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources</a> - <em><strong>Behavioral and Brain Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1642626865293965288&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A resource-rational account on interpreting human intelligence.</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/tops.12142" rel="noopener noreferrer">Rational Use of Cognitive Resources: Levels of Analysis Between the Computational and the Algorithmic</a> - <em><strong>Topics in Cognitive Science</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=16305499937147933368&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. An earlier version of the paper above.</p>
</li>
<li><p><a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(20)30215-1" rel="noopener noreferrer">Understanding Human Intelligence through Human Limitations</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=6469796133334580403" rel="noopener noreferrer">All Versions</a>]. [<a href="https://cocosci.princeton.edu/papers/griffiths_understanding.pdf" rel="noopener noreferrer">Preprint</a>]. Recent progress in artificial intelligence provides the opportunity to ask the question of what is unique about human intelligence, but with a new comparison class. The author argues that we can understand human intelligence, and the ways in which it may differ from artificial intelligence, by considering the characteristics of the kind of computational problems that human minds have to solve. The author claims that these problems acquire their structure from three fundamental limitations that apply to human beings: limited time, limited computation, and limited communication. From these limitations we can derive many of the properties we associate with human intelligence, such as rapid learning, the ability to break down problems into parts, and the capacity for cumulative cultural evolution.</p>
</li>
<li><p><a href="https://eccl.mit.edu/s/Pelz_Foundations-of-intuitive-power-analyses-in-children-and-adults.pdf" rel="noopener noreferrer">Foundations of intuitive power analyses in children and adults</a> - <em><strong>Nature Human Behavior</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=4370839893505978405&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Evidences support that people have some of the foundations for 'intuitive power analyses', which help people use intuitive statistical reasoning and metacognitive strategies to estimate how much information they might need to solve different discrimination problems.</p>
</li>
<li><p><a href="https://cocosci.princeton.edu/papers/ho2022cognitive.pdf" rel="noopener noreferrer">Cognitive Science as a Source of Forward and Inverse Models of Human Decisions for Robotics and Control</a> - <em><strong>Annual Review of Control, Robotics, and Autonomous Systems</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=14055765901243029337" rel="noopener noreferrer">All Versions</a>]. The review focuses on how cognitive science can provide forward models of human decision-making and inverse models of how humans think about others’ decision-making. The authors highlight relevant recent developments, including approaches that synthesize black box and theory-driven modeling, accounts that recast heuristics and biases as forms of bounded optimality, and models that characterize human theory of mind and communication in decision-theoretic terms.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="cognitive-architecture"><a class="anchor" aria-hidden="true" tabindex="-1" href="#cognitive-architecture"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Cognitive Architecture</h4><ul>
<li><p><a href="https://plato.stanford.edu/entries/epistemology/" rel="noopener noreferrer">Epistemology</a> - <em><strong>Plato Stanford</strong></em>.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S1364661321001285" rel="noopener noreferrer">The secret life of predictive brains: what's spontaneous activity for?</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=719229834892860829&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A neuroscience account on brain as a generative model.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/0004370287900506" rel="noopener noreferrer">SOAR: An architecture for general intelligence</a> - <em><strong>Artificial Intelligence</strong></em>, 1987. [<a href="https://scholar.google.com/scholar?cluster=10873259207109132615&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://act-r.psy.cmu.edu/wordpress/wp-content/uploads/2013/09/Anderson91.pdf" rel="noopener noreferrer">Is human cognition adaptive?</a> - <em><strong>Behavioral and Brain Sciences</strong></em>, 1991. [<a href="https://scholar.google.com/scholar?cluster=3639936076538071052&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper introducing the adaptation perspective of human intelligence, the theoretical basis of the ACT cognitive architecture.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0004370205001530" rel="noopener noreferrer">Metacognition in computation: A selected research review</a> - <em><strong>Artificial Intelligence</strong></em>, 2005. [<a href="https://scholar.google.com/scholar?cluster=4240334051245008914&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0010027718301604" rel="noopener noreferrer">Basic functional trade-offs in cognition: An integrative framework</a> - <em><strong>Cognition</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=11475742130443069967&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://doi.org/10.1126/SCIENCE.AAN8871" rel="noopener noreferrer">What is consciousness, and could machines have it?</a> - <em><strong>Science</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=6932714857132107942&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A perspective on the two levels of consciousness in machine intelligence.</p>
</li>
<li><p><a href="https://www.worldscientific.com/doi/abs/10.1142/S2705078521500028" rel="noopener noreferrer">A Theoretical Computer Science Perspective on Consciousness</a> - <em><strong>Journal of Artificial Intelligence and Consciousness</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16430561748075101972&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="science-logology"><a class="anchor" aria-hidden="true" tabindex="-1" href="#science-logology"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Science Logology</h3><h4 id="philosophy-of-science"><a class="anchor" aria-hidden="true" tabindex="-1" href="#philosophy-of-science"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Philosophy of Science</h4><ul>
<li><p><a href="https://www-inst.eecs.berkeley.edu/~cs298-7/fa20/readings/kuhn.pdf" rel="noopener noreferrer">The structure of scientific revolutions</a> - <em><strong>University of Chicago Press: Chicago</strong></em>, 1970. [<a href="https://scholar.google.com/scholar?cluster=8909475038284903063&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Thomas Kuhn's original book on the emergence and the shift of scientific paradigms.</p>
</li>
<li><p><a href="https://jamacoartney.net/Abend%20(2008).pdf" rel="noopener noreferrer">The Meaning of "Theory"</a> - <em><strong>Sociological Theory</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=4876642889050563131&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A philosophical account on the definition of "theory" in social science (also can be generalized to natural science).</p>
</li>
<li><p><a href="https://journals.sagepub.com/doi/pdf/10.4256/mio.2013.015" rel="noopener noreferrer">The blind men and the elephant: A metaphor to illuminate the role of researchers and reviewers in social science</a> - <em><strong>Methodological Innovations Online</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=1654629562068006152&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/3576896" rel="noopener noreferrer">A Computational Inflection for Scientific Discovery</a> - <em><strong>Communications of the ACM</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=1756108647531090189&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="science-of-science"><a class="anchor" aria-hidden="true" tabindex="-1" href="#science-of-science"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Science of Science</h4><ul>
<li><p><a href="https://en.wikipedia.org/wiki/Metascience" rel="noopener noreferrer">Metascience</a> - <em><strong>Wikipedia</strong></em>.</p>
</li>
<li><p><a href="http://ctbergstrom.com/publications/pdfs/2018Science.pdf" rel="noopener noreferrer">Science of Science</a> - <em><strong>Science</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=6471468823556848055&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A comprehensive large-scale review on the science of science.</p>
</li>
<li><p><a href="https://www.pnas.org/doi/abs/10.1073/pnas.0307752101" rel="noopener noreferrer">Finding Scientific Topics</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=17382767110929995134&amp;hl=zh-CN&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Thomas L. Griffiths's analysis of scientific topics using Bayesian model.</p>
</li>
<li><p><a href="https://www.pnas.org/doi/10.1073/pnas.1618569114" rel="noopener noreferrer">Meta-assessment of Bias in Science</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=14575889060982308028&amp;hl=zh-CN&amp;as_sdt=0,5" rel="noopener noreferrer">All Verisions</a>]. An analysis of bias patterns and risk factors in science.</p>
</li>
<li><p><a href="https://www.pnas.org/doi/10.1073/pnas.2021636118" rel="noopener noreferrer">Slowed Canonical Progress in Large Fields of Science</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=7541922918797308487&amp;hl=zh-CN&amp;as_sdt=0,5" rel="noopener noreferrer">All Verisions</a>]. An analysis of why too many papers published each year in a field can lead to stagnation rather than advance.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/10.1145/2858036.2858283" rel="noopener noreferrer">HCI Research as Problem-Solving</a> - <em><strong>ACM SIGCHI'16</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=3206201064123443333&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. This essay contributes a meta-scientific account of human-computer interaction (HCI) research as problem-solving. We build on the philosophy of Larry Laudan, who develops problem and solution as the foundational concepts of science. We argue that most HCI research is about three main types of problem: empirical, conceptual, and constructive.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="literature-mining"><a class="anchor" aria-hidden="true" tabindex="-1" href="#literature-mining"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Literature Mining</h4><ul>
<li><p><a href="https://www.nature.com/articles/s41467-024-45563-x" rel="noopener noreferrer">Structured information extraction from scientific text with large language models</a> - <em><strong>Nature Communications</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=13694008040033857249" rel="noopener noreferrer">All Versions</a>]. This paper presents a simple approach to joint named entity recognition and relation extraction and demonstrate how pretrained large language models can be fine-tuned to extract useful records of complex scientific knowledge. The authors test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41467-020-17266-6" rel="noopener noreferrer">Automated extraction of chemical synthesis actions from experimental procedures</a> - <em><strong>Nature Communications</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1626689948540815082" rel="noopener noreferrer">All Versions</a>]. This paper presents a method to convert unstructured experimental procedures written in English to structured synthetic steps (action sequences) reflecting all the operations needed to successfully conduct the corresponding chemical reactions.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41467-021-22951-1" rel="noopener noreferrer">Inferring experimental procedures from text-based representations of chemical reactions</a> - <em><strong>Nature Communications</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15772647675166217556" rel="noopener noreferrer">All Versions</a>]. This paper presents data-driven models for predicting the entire sequence of synthesis steps starting from a textual representation of a chemical equation, for application in batch organic chemistry.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41467-023-43836-5" rel="noopener noreferrer">Language models and protocol standardization guidelines for accelerating synthesis planning in heterogeneous catalysis</a> - <em><strong>Nature Communications</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=8186755371438552520" rel="noopener noreferrer">All Versions</a>]. This paper introduces a transformer model for automated synthesis protocol analysis in catalyst discovery, exemplified using single-atom heterogeneous catalysts (SACs), a rapidly expanding catalyst family. The model adeptly converts SAC protocols into action sequences, and this output is used to facilitate statistical inference of their synthesis trends and applications, potentially expediting literature review and analysis.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41598-025-02643-2" rel="noopener noreferrer">An intelligent guided troubleshooting method for aircraft based on HybirdRAG</a> - <em><strong>Scientific Reports</strong></em>, 2025. [<a href="https://scholar.google.com/scholar?cluster=4924119792997395046" rel="noopener noreferrer">All Versions</a>]. To enhance aircraft fault diagnosis efficiency, this paper proposes HybridRAG, an intelligent-guided troubleshooting framework that integrates knowledge graphs and large language models (LLMs). Unlike conventional retrieval-augmented generation (RAG) methods that rely on single-modal retrieval, HybridRAG adopts a multi-dimensional retrieval strategy, combining graph-based reasoning with both vector-based and BM25-based text retrieval techniques. This hybrid approach ensures comprehensive extraction of relevant information from both unstructured text and structured fault graphs, enhancing diagnostic precision, relevance, and robustness. Experimental results demonstrate that HybridRAG achieves an F1 score improvement of at least 4% and reduces hallucination rates by over 7% compared to mainstream RAG baselines. These advancements, combined with its unique integration of multi-modal retrieval, position HybridRAG as a novel framework for addressing complex aircraft maintenance challenges. Additionally, the paper presents an agent-based intelligent troubleshooting assistant that supports more interactive, adaptive, and flexible diagnostic Q&amp;A, providing maintenance personnel with a significant advanced intelligent, context-aware diagnostic tool.</p>
</li>
<li><p><a href="https://galactica.org/static/paper.pdf" rel="noopener noreferrer">Galactica: A Large Language Model for Science</a> - <em><strong>Meta AI</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=15782429788006956926&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A large language model trained on large-scale scientific corpus.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.03512" rel="noopener noreferrer">CORWA: A Citation-Oriented Related Work Annotation Dataset</a> - <em><strong>NAACL'22</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=14605899782190710454&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://aclanthology.org/2021.acl-demo.14/" rel="noopener noreferrer">ESRA: Explainable Scientific Research Assistant</a> - <em><strong>ACL'21 Demo Track</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=4387915912582172679&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A tool for constructing and visualizing the knowledge graph of a query keyword in literature retrieving.</p>
</li>
<li><p><a href="https://matthewberger.github.io/papers/cite2vec.pdf" rel="noopener noreferrer">cite2vec: Citation-Driven Document Exploration via Word Embeddings</a> - <em><strong>IEEE Transactions on Visualization and Computer Graphics</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=6949650208780085923&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://cic.tju.edu.cn/faculty/zhangjiawan/Jiawan_Zhang_files/paper/zeyuli2020.pdf" rel="noopener noreferrer">Galex: Exploring the evolution and intersection of disciplines</a> - <em><strong>IEEE Transactions on Visualization and Computer Graphics</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=13313104491218225635&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="scientific-writing"><a class="anchor" aria-hidden="true" tabindex="-1" href="#scientific-writing"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Scientific Writing</h4><ul>
<li><p><a href="http://library.lol/main/8036CBB1CCC448CA7E036774D810EBC0" rel="noopener noreferrer">The uses of argument</a> - <em><strong>Cambridge University Press</strong></em>, 1958. [<a href="https://scholar.google.com/scholar?cluster=12052408655432810103&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Stephen Toulmin's introduction to the Toulmin argument pattern, which is generally consist of a claim, a justification, and a rebuttal.</p>
</li>
<li><p><a href="https://www.jstor.org/stable/355200" rel="noopener noreferrer">A tagmemic approach to paragraph analysis</a> - <em><strong>College Composition and Communication</strong></em>, 1965. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=A+Tagmemic+Approach+to+Paragraph+Analysis+AL+Becker&amp;btnG=" rel="noopener noreferrer">All Versions</a>]. The original paper on analyzing the structure of expository paragraphs, with the two patterns---the Topic-Restriction-Illustration pattern and the Problem-Solution pattern.</p>
</li>
<li><p><a href="https://journals.sagepub.com/doi/abs/10.1177/0741088398015002004" rel="noopener noreferrer">The uses and complexity of argument structures in expert and student persuasive writing</a> - <em><strong>Written Communication</strong></em>, 1998. [<a href="https://scholar.google.com/scholar?cluster=3218190258774062869&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A behaviorial study revealing the argument structures exploited by people in argumentative writing.</p>
</li>
<li><p><a href="https://pure.mpg.de/rest/items/item_3020351/component/file_3045811/content" rel="noopener noreferrer">Towards an argument interchange format</a> - <em><strong>The Knowledge Engineering Review</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=11123720528835823517&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper introducing the Argument Interchange Format (AIF) framework for argumentation analysis.</p>
</li>
<li><p><a href="https://www.aaai.org/ocs/index.php/WS/AAAIW11/paper/viewFile/3940/4244" rel="noopener noreferrer">Speech Acts of Argumentation: Inference Anchors and Peripheral Cues in Dialogue</a> - <em><strong>AAAI'12</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=9761955212933152906&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper introducing the Information Anchoring Theory (IAT) as an alternate for AIF.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="science-education"><a class="anchor" aria-hidden="true" tabindex="-1" href="#science-education"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Science Education</h4><ul>
<li><p><a href="https://www.harvardlds.org/wp-content/uploads/2018/05/Carey-Cognitive-science-and-science-education.-American-Psychologist.pdf" rel="noopener noreferrer">Cognitive Science and Science Education</a> - <em><strong>American Psychologist</strong></em>, 1986. [<a href="https://scholar.google.com/scholar?cluster=6627805813997387166&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Susan Carey's review on cognitive-science-based methodologies for science education research.</p>
</li>
<li><p><a href="https://aclanthology.org/2023.acl-demo.2/" rel="noopener noreferrer">PersLEARN: Research Training through the Lens of Perspective Cultivation</a> - <em><strong>ACL'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=6242389165210232890" rel="noopener noreferrer">All Versions</a>]. Scientific research is inherently shaped by its authors’ perspectives, influenced by various factors such as their personality, community, or society. Junior researchers often face challenges in identifying the perspectives reflected in the existing literature and struggle to develop their own viewpoints. To address the problem, this paper introduces PersLEARN, a tool designed to facilitate the cultivation of scientific perspectives, starting from a basic seed idea and progressing to a well-articulated framework.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="democratization-of-science"><a class="anchor" aria-hidden="true" tabindex="-1" href="#democratization-of-science"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Democratization of Science</h4><ul>
<li><p><a href="https://www.science.org/doi/full/10.1126/science.1250475" rel="noopener noreferrer">Reproducibility</a> - <em><strong>Science</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=676974831306442279&amp;hl=en&amp;as_sdt=0,10" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41557-024-01470-8" rel="noopener noreferrer">Bridging the information gap in organic chemical reactions</a> - <em><strong>Nature Chemistry</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=5365091261196953334" rel="noopener noreferrer">All Versions</a>]. This perspective article formulates eight principles to improve data management in scientific publications relating to data standardization, reproducibility and evaluation, and encourage scientists to go beyond current publication standards.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41562-016-0021" rel="noopener noreferrer">A manifesto for reproducible science</a> - <em><strong>Nature Human Behavior</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=9515807942859203900&amp;hl=en&amp;as_sdt=0,10" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.nature.com/articles/533452a" rel="noopener noreferrer">1,500 scientists lift the lid on reproducibility</a> - <em><strong>Nature</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=11479406257389837824&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4204808/" rel="noopener noreferrer">How to Make More Published Research True</a> - <em><strong>PLoS Medicine</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=10945341175996677908" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.nature.com/articles/d42473-019-00004-y" rel="noopener noreferrer">Six factors affecting reproducibility in life science research and how to handle them</a> - <em><strong>Nature Advertisement</strong></em>.</p>
</li>
<li><p><a href="https://www.nature.com/articles/d41586-021-02428-3" rel="noopener noreferrer">Five keys to writing a reproducible lab protocol</a> - <em><strong>Nature</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=13259206850261301938" rel="noopener noreferrer">All Versions</a>]. This interviewing paper introduces five ways to increase the reproducibility of experimental protocols: (i) documenting protocols as the experiment goes; (ii) providing video illustrations in addition to written protocols; (iii) using electronic lab notebooks (ELNs) for managing experimental resources digitally; (iv) depositing and documenting reagents with understanding the rationale behind every step; and (v) exploiting online platforms to share tips, extensions, methods, and data among researchers.</p>
</li>
<li><p><a href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2003779" rel="noopener noreferrer">The Experimental Design Assistant</a> - <em><strong>PLoS Biology</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=12481490526120919925" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.nature.com/articles/nmeth.4462" rel="noopener noreferrer">Nature Methods Correspondence</a>]. [<a href="https://eda.nc3rs.org.uk/" rel="noopener noreferrer">EDA Website</a>]. The EDA is a web-based tool that guides the in vivo researcher through the experimental design and analysis process, providing automated feedback on the proposed design and generating a graphical summary that aids communication with colleagues, funders, regulatory authorities, and the wider scientific community.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="laboratory-automation"><a class="anchor" aria-hidden="true" tabindex="-1" href="#laboratory-automation"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Laboratory Automation</h4><ul>
<li><p><a href="https://www.science.org/doi/full/10.1126/science.aat0650" rel="noopener noreferrer">Reconfigurable system for automated optimization of diverse chemical reactions</a> - <em><strong>Science</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=3076614068291119943" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.science.org/doi/pdf/10.1126/science.aat0650" rel="noopener noreferrer">Preprint</a>]. This paper describes a plug-and-play, continuous-flow chemical synthesis system that mitigates this challenge with an integrated combination of hardware, software, and analytics. The system software controls the user-selected reagents and unit operations (reactors and separators), processes reaction analytics (high-performance liquid chromatography, mass spectrometry, vibrational spectroscopy), and conducts automated optimizations.</p>
</li>
<li><p><a href="https://www.science.org/doi/full/10.1126/science.abc2986" rel="noopener noreferrer">A universal system for digitization and automatic execution of the chemical synthesis literature</a> - <em><strong>Science</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13909991218383718512" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.chem.gla.ac.uk/cronin/images/pubs/Mehr-ScienceOct2020.pdf" rel="noopener noreferrer">Preprint</a>]. [<a href="https://croningroup.gitlab.io/chemputer/xdl/index.html" rel="noopener noreferrer">XDL Documentation</a>]. [<a href="https://zenodo.org/records/3955107" rel="noopener noreferrer">XDL Schema Database</a>]. This paper reports a software platform that uses natural language processing to translate the organic chemistry literature directly into editable code, which in turn can be compiled to drive automated synthesis of the compound in the laboratory.</p>
</li>
<li><p><a href="https://www.science.org/doi/full/10.1126/science.abo0058" rel="noopener noreferrer">Digitization and validation of a chemical synthesis literature database in the ChemPU</a> - <em><strong>Science</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=17368503277308594977" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.researchgate.net/profile/Aamir-Khan/publication/361857872_Digitization_and_validation_of_a_chemical_synthesis_literature_database_in_the_ChemPU/links/62cd356d00d0b451104cbfe9/Digitization-and-validation-of-a-chemical-synthesis-literature-database-in-the-ChemPU.pdf" rel="noopener noreferrer">Preprint</a>]. This paper presents an automatically executable chemical reaction database of 100 molecules representative of the range of reactions found in contemporary organic synthesis. The chemical reaction codes or χDLs for the reactions have been stored in a database for version control, validation, collaboration, and data mining. Of these syntheses, more than 50 entries from the database have been downloaded and robotically run in seven modular chemputers with yields and purities comparable to those achieved by an expert chemist.</p>
</li>
<li><p><a href="https://pubs.acs.org/doi/full/10.1021/jacsau.1c00303" rel="noopener noreferrer">Chemputation and the Standardization of Chemical Informatics</a> - <em><strong>Journal of the American Chemical Society (Au)</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=3884902150148113559" rel="noopener noreferrer">All Versions</a>]. This paper describes a standard hardware (the chemical processing programming architecture --- the ChemPU) to encompass all chemical synthesis, an approach which unifies all chemistry automation strategies, from solid-phase peptide synthesis, to HTE flow chemistry platforms, while at the same time establishing a publication standard so that researchers can exchange chemical code (χDL) to ensure reproducibility and interoperability.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41557-020-00596-9" rel="noopener noreferrer">Convergence of multiple synthetic paradigms in a universally programmable chemical synthesis machine</a> - <em><strong>Nature Chemistry</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=18024303106901939347" rel="noopener noreferrer">All Versions</a>]. [<a href="https://eprints.gla.ac.uk/231947/" rel="noopener noreferrer">Preprint</a>]. This paper shows how the Chemputer synthesis robot can be programmed to perform many different reactions, including solid-phase peptide synthesis, iterative cross-coupling and accessing reactive, unstable diazirines in a single, unified system with high yields and purity.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41557-022-01016-w" rel="noopener noreferrer">An autonomous portable platform for universal chemical synthesis</a> - <em><strong>Nature Chemistry</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=4484997534431409967" rel="noopener noreferrer">All Versions</a>]. [<a href="https://eprints.gla.ac.uk/275574/" rel="noopener noreferrer">Preprint</a>]. This paper presents a portable suitcase-sized chemical synthesis platform containing all the modules required for synthesis and purification. The system uses a chemical programming language coupled to a digital reactor generator to produce reactors and executable protocols based on text-based literature syntheses. Simultaneously, the platform generates a reaction pressure fingerprint, used to monitor processes within the reactors and remotely perform a protocol quality control.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41467-024-45444-3" rel="noopener noreferrer">An integrated self-optimizing programmable chemical synthesis and reaction engine</a> - <em><strong>Nature Communications</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=9157508627971047184" rel="noopener noreferrer">All Versions</a>]. This paper presents a dynamically programmable system capable of making, optimizing, and discovering new molecules which utilizes seven sensors that continuously monitor the reaction. By developing a dynamic programming language, the work demonstrates the 10-fold scale-up of a highly exothermic oxidation reaction, end point detection, as well as detecting critical hardware failures.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-020-2442-2" rel="noopener noreferrer">A mobile robotic chemist</a> - <em><strong>Nature</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13216902493789027324&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://strathprints.strath.ac.uk/74759/1/Burger_etal_Nature_2020_A_mobile_robotic.pdf" rel="noopener noreferrer">Preprint</a>]. This work uses a mobile robot to search for improved photocatalysts for hydrogen production from water. The robot operated autonomously over eight days, performing 688 experiments within a ten-variable experimental space, driven by a batched Bayesian search algorithm. This autonomous search identified photocatalyst mixtures that were six times more active than the initial formulations, selecting beneficial components and deselecting negative ones.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-023-06734-w" rel="noopener noreferrer">An autonomous laboratory for the accelerated synthesis of novel materials</a> - <em><strong>Nature</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=17944003281308189532" rel="noopener noreferrer">All Versions</a>]. This paper introduces the A-Lab, an autonomous laboratory for the solid-state synthesis of inorganic powders. This platform uses computations, historical data from the literature, machine learning (ML) and active learning to plan and interpret the outcomes of experiments performed using robotics. Over 17 days of continuous operation, the A-Lab realized 41 novel compounds from a set of 58 targets including a variety of oxides and phosphates that were identified using large-scale ab initio phase-stability data from the Materials Project and Google DeepMind.</p>
</li>
<li><p><a href="https://www.nature.com/articles/542125a" rel="noopener noreferrer">The Internet of Things comes to the lab</a> - <em><strong>Nature</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=7747117198956166976&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The emergence of connected instruments and equipment promises to untether researchers from the laboratory --- letting them fine-tune experiments and analyse data remotely.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41467-023-44599-9" rel="noopener noreferrer">A dynamic knowledge graph approach to distributed self-driving laboratories</a> - <em><strong>Nature Communications</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=7070798385652764751" rel="noopener noreferrer">All Versions</a>]. This work employs ontologies to capture data and material flows in design-make-test-analyse cycles, utilising autonomous agents as executable knowledge components to carry out the experimentation workflow. Data provenance is recorded to ensure its findability, accessibility, interoperability, and reusability. The architecture is built upon the World Avatar project, which seeks to create an all-encompassing digital twin based on a dynamic knowledge graph.</p>
</li>
<li><p><a href="https://pubs.rsc.org/en/content/articlehtml/2021/sc/d1sc04588a" rel="noopener noreferrer">Automation isn't automatic</a> - <em><strong>Chemical Science</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=14176714971050097971" rel="noopener noreferrer">All Versions</a>]. This perspective provides an overview of the current state of automation of synthetic chemistry at the benchtop scale with a particular emphasis on core considerations and the ensuing challenges of deploying a system. The authors aim to reframe automation as decidedly not automatic but rather an iterative process that involves a series of careful decisions (both human and computational) and constant adjustment.</p>
</li>
<li><p><a href="https://www.cell.com/trends/chemistry/fulltext/S2589-5974(23)00249-6" rel="noopener noreferrer">Balancing act: when to flex and when to stay fixed</a> - <em><strong>Trends in Chemistry</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=14208571639305934551" rel="noopener noreferrer">All Versions</a>]. This perspective article provides essential insights into the decision-making process for choosing automation platforms, highlighting the suitability of fixed automation for standardized tasks and the strategic use of flexible automation in dynamic research settings.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S2590238522006385" rel="noopener noreferrer">What is a minimal working example for a self-driving laboratory?</a> - <em><strong>Matter</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=1612804023616680548" rel="noopener noreferrer">All Versions</a>]. This paper proposes SDL-Demo: a low-cost “Hello, World!” for self-driving laboratories that combines “Hello, World!” tasks from electronics, physics-based simulations, and optimization. SDL-Demo is modular and extensible, making it an ideal candidate for low-cost teaching and prototyping of self-driving laboratory concepts.</p>
</li>
<li><p><a href="https://elifesciences.org/articles/77007" rel="noopener noreferrer">Robotic search for optimal cell culture in regenerative medicine</a> - <em><strong>eLife</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=1330075145723138159" rel="noopener noreferrer">All Versions</a>]. This paper develops a robotic AI system with a batch Bayesian optimization algorithm that autonomously induces the differentiation of induced pluripotent stem cell-derived retinal pigment epithelial (iPSC-RPE) cells. From 200 million possible parameter combinations, the system performed cell culture in 143 different conditions in 111 days, resulting in 88% better iPSC-RPE production than that obtained by the pre-optimized culture in terms of the pigmentation scores.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="ai-assisted-research"><a class="anchor" aria-hidden="true" tabindex="-1" href="#ai-assisted-research"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>AI Assisted Research</h4><ul>
<li><p><a href="https://www.nature.com/articles/s41586-023-06221-2" rel="noopener noreferrer">Scientific discovery in the age of artificial intelligence</a> - <em><strong>Nature</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11962817646389491592" rel="noopener noreferrer">All Versions</a>]. A review article that examines breakthroughs over the past decade that include self-supervised learning, which allows models to be trained on vast amounts of unlabelled data, and geometric deep learning, which leverages knowledge about the structure of scientific data to enhance model accuracy and efficiency.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2311.07361" rel="noopener noreferrer">The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4</a> - <em><strong>Microsoft Research AI4Science</strong></em>, 2023. [<a href="https://github.com/SHI-Yu-Zhe/awesome-agi-cocosci/blob/master/README.md/" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/microsoft/LLM4ScientificDiscovery" rel="noopener noreferrer">Project (⭐73)</a>]. A survey on the performance of LLMs within the context of scientific discovery, focusing on GPT-4.</p>
</li>
<li><p><a href="https://www.science.org/doi/full/10.1126/sciadv.aay4275" rel="noopener noreferrer">Machine learning-assisted molecular design and efficiency prediction for high-performance organic photovoltaic materials</a> - <em><strong>Science Advances</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=12392230644945701722" rel="noopener noreferrer">All Versions</a>]. In the process of finding high-performance materials for organic photovoltaics (OPVs), it is meaningful if one can establish the relationship between chemical structures and photovoltaic properties even before synthesizing them. This work first establishes a database containing over 1700 donor materials reported in the literature. Through supervised learning, our machine learning (ML) models can build up the structure-property relationship and, thus, implement fast screening of OPV materials. The authors explore several expressions for molecule structures, i.e., images, ASCII strings, descriptors, and fingerprints, as inputs for various ML algorithms. It is found that fingerprints with length over 1000 bits can obtain high prediction accuracy. The reliability of the approach is further verified by screening 10 newly designed donor materials. Good consistency between model predictions and experimental outcomes is obtained. The result indicates that ML is a powerful tool to prescreen new OPV materials, thus accelerating the development of the OPV field.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41598-018-34533-1" rel="noopener noreferrer">Design of metalloproteins and novel protein folds using variational autoencoders</a> - <em><strong>Scientific Reports</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=18126187509308242959" rel="noopener noreferrer">All Versions</a>]. The design of novel proteins has many applications but remains an attritional process with success in isolated cases. Meanwhile, deep learning technologies have exploded in popularity in recent years and are increasingly applicable to biology due to the rise in available data. This work attempts to link protein design and deep learning by using variational autoencoders to generate protein sequences conditioned on desired properties. Potential copper and calcium binding sites are added to non-metal binding proteins without human intervention and compared to a hidden Markov model. In another use case, a grammar of protein structures is developed and used to produce sequences for a novel protein topology. One candidate structure is found to be stable by molecular dynamics simulation. The ability of the model to confine the vast search space of protein sequences and to scale easily has the potential to assist in a variety of protein design tasks.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-021-03819-2" rel="noopener noreferrer">Highly accurate protein structure prediction with AlphaFold</a> - <em><strong>Nature</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=6286436358625670901" rel="noopener noreferrer">All Versions</a>]. This paper provides the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. This approach is a canonical application of observation- and explanation- based method for protein structure prediction instead of first-principle-based methods.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-023-05773-7" rel="noopener noreferrer">Human–machine collaboration for improving semiconductor process development</a> - <em><strong>Nature</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=10295771969614897767" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.nature.com/articles/d41586-023-01353-x" rel="noopener noreferrer">Nature News</a>]. This work studies Bayesian optimization algorithms to investigate how artificial intelligence (AI) might decrease the cost of developing complex semiconductor chip processes. In particular, this work create a controlled virtual process game to systematically benchmark the performance of humans and computers for the design of a semiconductor fabrication process. The authors find that human engineers excel in the early stages of development, whereas the algorithms are far more cost-efficient near the tight tolerances of the target.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-023-06555-x" rel="noopener noreferrer">A foundation model for generalizable disease detection from retinal images</a> - <em><strong>Nature</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=3139988207343394501" rel="noopener noreferrer">All Versions</a>]. This paper presents RETFound, a foundation model for retinal images that learns generalizable representations from unlabelled retinal images and provides a basis for label-efficient model adaptation in several applications. Specifically, RETFound is trained on 1.6 million unlabelled retinal images by means of self-supervised learning and then adapted to disease detection tasks with explicit labels.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-023-06185-3" rel="noopener noreferrer">Accurate medium-range global weather forecasting with 3D neural networks</a> - <em><strong>Nature</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=7198604620204619820" rel="noopener noreferrer">All Versions</a>]. This paer introduces an artificial-intelligence-based method for accurate, medium-range global weather forecasting. It shows that three-dimensional deep networks equipped with Earth-specific priors are effective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39 years of global data, the program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the world’s best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts.</p>
</li>
<li><p><a href="https://www.science.org/doi/10.1126/science.adi2336" rel="noopener noreferrer">Learning skillful medium-range global weather forecasting</a> - <em><strong>Science</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=269756601245477923&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-023-06184-4" rel="noopener noreferrer">Skilful nowcasting of extreme precipitation with NowcastNet</a> - <em><strong>Nature</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=17837864391812838009&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-023-06792-0" rel="noopener noreferrer">Autonomous chemical research with large language models</a> - <em><strong>Nature</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=8097577445064259203" rel="noopener noreferrer">All Versions</a>]. An artificial intelligence system driven by GPT-4 that autonomously designs, plans and performs complex experiments by incorporating large language models empowered by tools such as internet and documentation search, code execution and experimental automation.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s42256-024-00832-8" rel="noopener noreferrer">Augmenting large language models with chemistry tools</a> - <em><strong>Nature Machine Intelligence</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=9291969834799338362" rel="noopener noreferrer">All Versions</a>]. [<a href="https://arxiv.org/abs/2304.05376" rel="noopener noreferrer">Preprint</a>]. This paper introduces ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery and materials design. By integrating 18 expert-designed tools and using GPT-4 as the LLM, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. The agent autonomously planned and executed the syntheses of an insect repellent and three organocatalysts and guided the discovery of a novel chromophore.</p>
</li>
<li><p><a href="https://aclanthology.org/2023.emnlp-main.162/" rel="noopener noreferrer">BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology</a> - <em><strong>EMNLP'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=1222312709622462659" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/bioplanner/bioplanner" rel="noopener noreferrer">Project (⭐25)</a>]. This paper presents an automatic evaluation framework for the task of planning experimental protocols, and introduces BioProt: a dataset of biology protocols with corresponding pseudocode representations.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41467-024-47997-9" rel="noopener noreferrer">A human-machine interface for automatic exploration of chemical reaction networks</a> - <em><strong>Nature Communications</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=13306522324804014261" rel="noopener noreferrer">All Versions</a>]. Autonomous reaction network exploration algorithms offer a systematic approach to explore mechanisms of complex chemical processes. However, the resulting reaction networks are so vast that an exploration of all potentially accessible intermediates is computationally too demanding. This paper introduces a STEERING WHEEL to guide an otherwise unbiased automated exploration. The STEERING WHEEL algorithm is intuitive, generally applicable, and enables one to focus on specific regions of an emerging network. It also allows for guiding automated data generation in the context of mechanism exploration, catalyst design, and other chemical optimization challenges.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41467-024-50779-y" rel="noopener noreferrer">PatCID: an open-access dataset of chemical structures in patent documents</a> - <em><strong>Nature Communications</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=329287456191953845" rel="noopener noreferrer">All Versions</a>]. The automatic analysis of patent publications has potential to accelerate research across various domains, including drug discovery and material science. Within patent documents, crucial information often resides in visual depictions of molecule structures. PatCID (Patent-extracted Chemical-structure Images database for Discovery) allows to access such information at scale. It enables users to search which molecules are displayed in which documents. PatCID contains 81M chemical-structure images and 14M unique chemical structures. This work compares PatCID with state-of-the-art chemical patent-databases. On a random set, PatCID retrieves 56.0% of molecules, which is higher than automatically-created databases, Google Patents (41.5%) and SureChEMBL (23.5%), as well as manually-created databases, Reaxys (53.5%) and SciFinder (49.5%). Leveraging state-of-the-art methods of document understanding, PatCID high-quality data outperforms currently available automatically-generated patent-databases. PatCID even competes with proprietary manually-created patent-databases. This enables promising applications for automatic literature review and learning-based molecular generation methods.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2311.00176" rel="noopener noreferrer">ChipNeMo: Domain-Adapted LLMs for Chip Design</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=5962372610489019326" rel="noopener noreferrer">All Versions</a>]. ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, the authors instead adopt the following domain adaptation techniques: domain-adaptive tokenization, domain-adaptive continued pretraining, model alignment with domain-specific instructions, and domain-adapted retrieval models. The authors evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Evaluations demonstrate that domain-adaptive pretraining of language models, can lead to superior performance in domain related downstream tasks compared to their base LLaMA2 counterparts, without degradations in generic capabilities. In particular, the largest model, ChipNeMo-70B, outperforms the highly capable GPT-4 on two of the use cases, namely engineering assistant chatbot and EDA scripts generation, while exhibiting competitive performance on bug summarization and analysis. These results underscore the potential of domain-specific customization for enhancing the effectiveness of large language models in specialized applications.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41467-021-22048-9" rel="noopener noreferrer">Single-atom alloy catalysts designed by first-principles calculations and artificial intelligence</a> - <em><strong>Nature Communications</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=6593978922251447907" rel="noopener noreferrer">All Versions</a>]. This paper addresses the problem of new Single-atom-alloy catalysts (SAACs) discovery by applying a compressed-sensing data-analytics approach parameterized with density-functional inputs.</p>
</li>
<li><p><a href="https://www.pnas.org/doi/abs/10.1073/pnas.2016239118" rel="noopener noreferrer">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15181490380139888639&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s00449-016-1659-9" rel="noopener noreferrer">Comparability of automated human induced pluripotent stem cell culture: a pilot study</a> - <em><strong>Bioprocess and Biosystems Engineering</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=14666375402220991095&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://pubs.acs.org/doi/full/10.1021/jacs.4c00338" rel="noopener noreferrer">Artificial Intelligence for Retrosynthetic Planning Needs Both Data and Expert Knowledge</a> - <em><strong>Journal of the American Chemical Society</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=10595951443492961310" rel="noopener noreferrer">All Versions</a>]. The development of AI synthesis planners trained solely on reaction-example-data has stagnated and is not on par with the performance of “hybrid” algorithms combining AI with expert knowledge. This Perspective examines possible causes of these shortcomings, extending beyond the established reasoning of insufficient quantities of reaction data. Drawing attention to the intricacies and data biases that are specific to the domain of synthetic chemistry, the authors advocate augmenting the unique capabilities of AI with the knowledge base and the reasoning strategies of domain experts. By actively involving synthetic chemists, who are the end users of any synthesis planning software, into the development process, the authors envision to bridge the gap between computer algorithms and the intricate nature of chemical synthesis.</p>
</li>
<li><p><a href="https://www.cell.com/cell-reports-medicine/fulltext/S2666-3791(21)00197-X" rel="noopener noreferrer">Virtual and augmented reality for biomedical applications</a> - <em><strong>Cell Reports Medicine</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=14732259085495422063" rel="noopener noreferrer">All Versions</a>]. 3D visualization technologies such as virtual reality (VR), augmented reality (AR), and mixed reality (MR) have gained popularity in the recent decade. Digital extended reality (XR) technologies have been adopted in various domains ranging from entertainment to education because of their accessibility and affordability. XR modalities create an immersive experience, enabling 3D visualization of the content without a conventional 2D display constraint. This paper provides a perspective on XR in current biomedical applications and demonstrate case studies using cell biology concepts, multiplexed proteomics images, surgical data for heart operations, and cardiac 3D models. Emerging challenges associated with XR technologies in the context of adverse health effects and a cost comparison of distinct platforms are discussed. The presented XR platforms will be useful for biomedical education, medical training, surgical guidance, and molecular data visualization to enhance trainees’ and students’ learning, medical operation accuracy, and the comprehensibility of complex biological systems.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41591-019-0539-7" rel="noopener noreferrer">An augmented reality microscope with real-time artificial intelligence integration for cancer diagnosis</a> - <em><strong>Nature Medicine</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=3280260879383275625" rel="noopener noreferrer">All Versions</a>]. The microscopic assessment of tissue samples is instrumental for the diagnosis and staging of cancer, and thus guides therapy. However, these assessments demonstrate considerable variability and many regions of the world lack access to trained pathologists. Though artificial intelligence (AI) promises to improve the access and quality of healthcare, the costs of image digitization in pathology and difficulties in deploying AI solutions remain as barriers to real-world use. This work proposes a cost-effective solution: the augmented reality microscope (ARM). The ARM overlays AI-based information onto the current view of the sample in real time, enabling seamless integration of AI into routine workflows.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/10059206" rel="noopener noreferrer">Optimizing Spaced Repetition Schedule by Capturing the Dynamics of Memory</a> - <em><strong>IEEE Transactions on Knowledge and Data Engineering</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=949715967083833369&amp;hl=en&amp;as_sdt=0,10" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://aclanthology.org/2020.findings-emnlp.261/" rel="noopener noreferrer">LEGAL-BERT: The Muppets straight out of Law School</a> - <em><strong>EMNLP'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=11254432523766039890&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Generating answers to legal questions, analyze contracts, and summarizing legal documents, making legal knowledge more accessible to non-experts.</p>
</li>
<li><p><a href="https://academic.oup.com/bioinformatics/article/36/4/1234/5566506" rel="noopener noreferrer">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</a> - <em><strong>Bioinformatics</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=2783127196632783403&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Answering medical questions, identifying relevant clinical trials, and diagnosing diseases based on symptoms, making medical information more accessible to the general public.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.5555/3491440.3492062" rel="noopener noreferrer">Finbert: A pre-trained financial language representation model for financial text mining</a> - <em><strong>IJCAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=17844713837232165872&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Predicting stock market trends, analyzing financial documents, and generating summaries of economic news articles, helping to disseminate financial knowledge.</p>
</li>
<li><p><a href="https://aclanthology.org/D19-1371/" rel="noopener noreferrer">SciBERT: A Pretrained Language Model for Scientific Text</a> - <em><strong>EMNLP'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7377999893003631695&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Searching and synthesizing scientific literature, aiding researchers in hypothesis generation, and assisting with experimental design, making scientific knowledge more accessible.</p>
</li>
<li><p><a href="https://aclanthology.org/2020.findings-emnlp.139/" rel="noopener noreferrer">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a> - <em><strong>EMNLP'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9055786889913621082&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Completing code, generating programming documentation, and providing technical support, making programming knowledge more accessible to non-experts.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="theory-of-mind"><a class="anchor" aria-hidden="true" tabindex="-1" href="#theory-of-mind"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Theory of Mind</h3><ul>
<li><p><a href="https://en.wikipedia.org/wiki/Theory_of_mind" rel="noopener noreferrer">Theory of Mind</a> - <em><strong>Wikipedia</strong></em>. Wikipedia on Theory of Mind (ToM), a cognitive capability that estimating others' goal, belief, and desire.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/intentionality/" rel="noopener noreferrer">Intentionality</a> - <em><strong>Plato Stanford</strong></em>.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/mental-imagery/" rel="noopener noreferrer">Mental Imagery</a> - <em><strong>Plato Stanford</strong></em>.</p>
</li>
</ul>




<ul>
<li><p><a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(16)30053-5" rel="noopener noreferrer">The naïve utility calculus: Computational principles underlying commonsense psychology</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=6894095575934067763" rel="noopener noreferrer">All Versions</a>]. [<a href="http://sll.stanford.edu/docs/2016_JaraEttinger_Gweon_Schulz_Tenenbaum_TiCS.pdf" rel="noopener noreferrer">Preprint</a>]. This review article proposes that human social cognition is structured around a basic understanding of ourselves and others as intuitive utility maximizers: from a young age, humans implicitly assume that agents choose goals and actions to maximize the rewards they expect to obtain relative to the costs they expect to incur. This ‘naïve utility calculus’ allows both children and adults observe the behavior of others and infer their beliefs and desires, their longer-term knowledge and preferences, and even their character: who is knowledgeable or competent, who is praiseworthy or blameworthy, who is friendly, indifferent, or an enemy.</p>
</li>
<li><p><a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(22)00185-1" rel="noopener noreferrer">Planning with theory of mind</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=8461125353366208047" rel="noopener noreferrer">All Versions</a>]. [<a href="https://saxelab.mit.edu/sites/default/files/publications/HoSaxeCushman2022.pdf" rel="noopener noreferrer">Preprint</a>]. A perspective on understanding Theory of Mind through planning that consists of abstract structured causal representations and supports efficient search and selection from innumerable possible actions. Planning requires that Theory of Mind consists of abstract structured causal representations and supports efficient search and selection from innumerable possible actions. Theory of Mind contrasts with less cognitively demanding alternatives: statistical predictive models of other people’s actions, or model-free reinforcement of actions by their effects on other people. Theory of Mind is likely used to plan novel interventions and predict their effects, for example, in pedagogy, emotion regulation, and impression management.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0010027709001607" rel="noopener noreferrer">Action Understanding as Inverse Planning</a> - <em><strong>Cognition</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=11478704181983566675" rel="noopener noreferrer">All Versions</a>]. [<a href="https://ars.els-cdn.com/content/image/1-s2.0-S0010027709001607-mmc1.pdf" rel="noopener noreferrer">Appendix</a>]. The original paper on Inverse Planning, a computational implementation of Theory of Mind. Humans are adept at inferring the mental states underlying other agents’ actions, such as goals, beliefs, desires, emotions and other thoughts. This paper proposes a computational framework based on Bayesian inverse planning for modeling human action understanding. The framework represents an intuitive theory of intentional agents’ behavior based on the principle of rationality: the expectation that agents will plan approximately rationally to achieve their goals, given their beliefs about the world. The mental states that caused an agent's behavior are inferred by inverting this model of rational planning using Bayesian inference, integrating the likelihood of the observed actions with the prior over mental states.</p>
</li>
<li><p><a href="https://escholarship.org/content/qt5rk7z59q/qt5rk7z59q.pdf" rel="noopener noreferrer">Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution</a> - <em><strong>CogSci'11</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?cluster=7454981153033683025" rel="noopener noreferrer">All Versions</a>]. [<a href="http://web.mit.edu/9.s915/www/classes/theoryOfMind.pdf" rel="noopener noreferrer">Preprint</a>]. This paper presents a computational framework for understanding Theory of Mind (ToM): the human capacity for reasoning about agents’ mental states such as beliefs and desires. The proposed Bayesian model of ToM (or BToM) expresses the predictive model of belief- and desire-dependent action at the heart of ToM as a partially observable Markov decision process (POMDP), and reconstructs an agent’s joint belief state and reward function using Bayesian inference, conditioned on observations of the agent’s behavior in some environmental context.</p>
</li>
<li><p><a href="https://psyarxiv.com/f692k/" rel="noopener noreferrer">The Signature of All Things: Children Infer Knowledge States from Static Images</a> - <em><strong>CogSci'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=12380982112592086477&amp;hl=en&amp;as_sdt=0,5&amp;as_ylo=2017" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S1364661316301565?via%3Dihub" rel="noopener noreferrer">Bayesian Brains without Probabilities</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=13076510377612067772&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A perspective on human probabilistic modeling without explicit probabilistic computation.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41562-017-0064" rel="noopener noreferrer">Rational quantitative attribution of beliefs, desires and percepts in human mentalizing</a> - <em><strong>Nature Human Behavior</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=9377509910551057835" rel="noopener noreferrer">All Versions</a>]. [<a href="https://cbmm.mit.edu/sites/default/files/publications/article.pdf" rel="noopener noreferrer">Preprint</a>]. This paper presents a model of core mentalizing computations: inferring jointly an actor’s beliefs, desires and percepts from how they move in the local spatial environment. The proposed Bayesian theory of mind (BToM) model is based on probabilistically inverting artificial-intelligence approaches to rational planning and state estimation, which extend classical expected-utility agent models to sequential actions in complex, partially observable domains.</p>
</li>
<li><p><a href="http://proceedings.mlr.press/v80/rabinowitz18a.html" rel="noopener noreferrer">Machine theory of mind</a> - <em><strong>ICML'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=6267278380616425333" rel="noopener noreferrer">All Versions</a>]. Theory of mind (ToM) broadly refers to humans’ ability to represent the mental states of others, including their desires, beliefs, and intentions. This work proposes a Theory of Mind neural network --- a ToMnet --- which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents’ future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents’ characteristics and mental states.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S2352154618302055?via%3Dihub" rel="noopener noreferrer">Theory of mind as inverse reinforcement learning</a> - <em><strong>Current Opinion in Behavioral Sciences</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=14959443239271810913" rel="noopener noreferrer">All Versions</a>]. This paper reviews the idea that Theory of Mind --- humans' ability to reason about other people's mental states --- can be formalized as inverse reinforcement learning. Under this framework, expectations about how mental states produce behavior are captured in a reinforcement learning (RL) model. Predicting other people’s actions is achieved by simulating a RL model with the hypothesized beliefs and desires, while mental-state inference is achieved by inverting this model. Although many advances in inverse reinforcement learning (IRL) did not have human Theory of Mind in mind, this paper focuses on what they reveal when conceptualized as cognitive theories.</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/tops.12371" rel="noopener noreferrer">Computational Models of Emotion Inference in Theory of Mind: A Review and Roadmap</a> - <em><strong>Topics in Cognitive Science</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15919410726494658168" rel="noopener noreferrer">All Versions</a>]. This paper proposes an intuitive theory framework to studying affective cognition—how humans reason about emotions—and derive a taxonomy of inferences within affective cognition. Using this taxonomy, the authors review formal computational modeling work on such inferences, including causal reasoning about how others react to events, reasoning about unseen causes of emotions, reasoning with multiple cues, as well as reasoning from emotions to other mental states. This framework proposes unifying these various types of reasoning as Bayesian inference within a common “intuitive Theory of Emotion.”</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0010028520300633" rel="noopener noreferrer">The Naïve Utility Calculus as a unified, quantitative framework for action understanding</a> - <em><strong>Cognitive Psychology</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=10366690800692546587" rel="noopener noreferrer">All Versions</a>]. [<a href="http://www.github.com/julianje/bishop" rel="noopener noreferrer">Project</a>]. This paper presents a formal theory of the Naïve Utility Calculus as a probabilistic generative model, which highlights the role of cost and reward tradeoffs in a Bayesian framework for action-understanding. The model predicts with quantitative accuracy how people infer agents’ subjective costs and rewards based on their observable actions. By distinguishing between desires, goals, and intentions, the model extends to complex action scenarios unfolding over space and time in scenes with multiple objects and multiple action episodes.</p>
</li>
<li><p><a href="http://proceedings.mlr.press/v139/shu21a.html" rel="noopener noreferrer">AGENT: A Benchmark for Core Psychological Reasoning</a> - <em><strong>ICML'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=9729067071974484204" rel="noopener noreferrer">All Versions</a>]. Inspired by cognitive development studies on intuitive psychology, this paper presents a benchmark consisting of a large dataset of procedurally generated 3D animations, AGENT (Action, Goal, Efficiency, coNstraint, uTility), structured around four scenarios (goal preferences, action efficiency, unobserved constraints, and cost-reward trade-offs) that probe key concepts of core intuitive psychology. The results suggest that to pass the designed tests of core intuitive psychology at human levels, a model must acquire or have built-in representations of how agents plan, combining utility computations and core knowledge of objects and physics.</p>
</li>
<li><p><a href="https://www.annualreviews.org/doi/pdf/10.1146/annurev-psych-081420-110718" rel="noopener noreferrer">Experimental Games and Social Decision Making</a> - <em><strong>Annual Review of Psychology</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=4713510112126264116" rel="noopener noreferrer">All Versions</a>]. Experimental games model situations in which the future outcomes of individuals and groups depend on their own choices and on those of other (groups of) individuals. Games are a powerful tool to identify the neural and psychological mechanisms underlying interpersonal and group cooperation and coordination. This review article discusses recent developments in how experimental games are used and adapted, with an increased focus on repeated interactions, partner control through sanctioning, and partner (de)selection for future interactions.</p>
</li>
<li><p><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4574" rel="noopener noreferrer">Theory of Minds: Understanding Behavior in Groups through Inverse Planning</a> - <em><strong>AAAI'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=6755247312077985817" rel="noopener noreferrer">All Versions</a>]. Towards the goal of building machine-learning algorithms with human-like social intelligence, this paper develops a generative model of multiagent action understanding based on a novel representation for these latent relationships called Composable Team Hierarchies (CTH). This representation is grounded in the formalism of stochastic games and multi-agent reinforcement learning. This work uses CTH as a target for Bayesian inference yielding a new algorithm for understanding behavior in groups that can both infer hidden relationships as well as predict future actions for multiple agents interacting together.</p>
</li>
<li><p><a href="https://psycnet.apa.org/fulltext/2019-58384-001.pdf?auth_token=0859666184839448b848053cd7bdceb2bdf2745a" rel="noopener noreferrer">Leveraging Facial Expressions and Contextual Information to Investigate Opaque Representations of Emotion</a> - <em><strong>Emotion</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=9634378462684744548&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://linkinghub.elsevier.com/retrieve/pii/S0010027712002235" rel="noopener noreferrer">Waiting and weighting: Information sampling is a balance between efficiency and error-reduction</a> - <em><strong>Cognition</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=12787722822882067638&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0896627313005503?via%3Dihub" rel="noopener noreferrer">Natural scene statistics account for the representation of scene categories in human visual cortex</a> - <em><strong>Neuron</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=14030885492052338412&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41598-018-23618-6" rel="noopener noreferrer">Using human brain activity to guide machine learning</a> - <em><strong>Scientific Report</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=12987955253653036948&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/2019-27729-001" rel="noopener noreferrer">Unit of visual working memory: A Boolean map provides a better account than an object does</a> - <em><strong>Journal of Experimental Psychology</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=14909735035752892020&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.pnas.org/content/117/42/26158.short" rel="noopener noreferrer">The logic of universalization guides moral judgment</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13482051983012049752&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Fan_Learning_Triadic_Belief_Dynamics_in_Nonverbal_Communication_From_Videos_CVPR_2021_paper.html" rel="noopener noreferrer">Learning Triadic Belief Dynamics in Nonverbal Communication from Videos</a> - <em><strong>CVPR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15365483338824697316" rel="noopener noreferrer">All Versions</a>]. [<a href="https://arxiv.org/abs/2104.02841" rel="noopener noreferrer">Preprint</a>]. This paper incorporates different nonverbal communication cues (e.g., gaze, human poses, and gestures) to represent, model, learn, and infer agents' mental states from pure visual inputs. Crucially, such a mental representation takes the agent's belief into account so that it represents what the true world state is and infers the beliefs in each agent's mental state, which may differ from the true world states. By aggregating different beliefs and true world states, the model essentially forms "five minds" during the interactions between two agents. This "five minds" model differs from prior works that infer beliefs in an infinite recursion; instead, agents' beliefs are converged into a "common mind". Based on this representation, this work further devises a hierarchical energy-based model that jointly tracks and predicts all five minds. From this new perspective, a social event is interpreted by a series of nonverbal communication and belief dynamics, which transcends the classic keyframe video summary.</p>
</li>
<li><p><a href="https://dspace.mit.edu/bitstream/handle/1721.1/112291/ivc_full_preprint.pdf?sequence=1&amp;isAllowed=y" rel="noopener noreferrer">Ten-month-old infants infer the value of goals from the costs of actions</a> - <em><strong>Science</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=11862940312128630925&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A piece of evidence for children's capability on ToM.</p>
</li>
<li><p><a href="https://www.pnas.org/content/116/36/17747" rel="noopener noreferrer">Origins of the concepts cause, cost, and goal in prereaching infants</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15973074852436355789&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://static1.squarespace.com/static/595a9f155016e1f7ead6edf1/t/61eeb3e7bbc41a23cd288f8a/1643033708945/Gandhi_etal_2021.pdf" rel="noopener noreferrer">Baby Intuitions Benchmark (BIB): Discerning the goals, preferences, and actions of others</a> - <em><strong>NeurIPS'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=16514364601966350574" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2011.05558.pdf" rel="noopener noreferrer">Intentonomy: a Dataset and Study towards Human Intent Understanding</a> - <em><strong>CVPR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=5268870345003195142&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A large-scale database on human intentionally-posted images on social media.</p>
</li>
<li><p><a href="https://www.tshu.io/HeiderSimmel/CogSci20/Flatland_CogSci20.pdf" rel="noopener noreferrer">Adventures in Flatland: Perceiving Social Interactions Under Physical Dynamics</a> - <em><strong>CogSci'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1928005249823745390&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16167" rel="noopener noreferrer">PHASE: PHysically-grounded Abstract Social Events for Machine Social Perception</a> - <em><strong>AAAI'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15536873427310696150&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://tshu.io/PHASE/" rel="noopener noreferrer">Project</a>].</p>
</li>
<li><p><a href="https://openreview.net/forum?id=w_7JMpGZRh0" rel="noopener noreferrer">Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration</a> - <em><strong>ICLR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=16340001407726295133" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://escholarship.org/uc/item/2j53v5nv" rel="noopener noreferrer">Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities</a> - <em><strong>CogSci'24</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=902767361177896884" rel="noopener noreferrer">All Versions</a>]. This work eveloped a comprehensive theoretical framework for social dynamics and introduced two evaluation tasks: Inverse Reasoning (IR) and Inverse Inverse Planning (IIP). The approach also encompassed a computational model based on recursive Bayesian inference, adept at elucidating diverse human behavioral patterns. Extensive experiments and detailed analyses revealed that humans surpassed the latest GPT models in overall performance, zero-shot learning, one-shot generalization, and adaptability to multi-modalities.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="analogy"><a class="anchor" aria-hidden="true" tabindex="-1" href="#analogy"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Analogy</h3><ul>
<li><p><a href="https://plato.stanford.edu/entries/metaphor/" rel="noopener noreferrer">Metaphor</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Metaphor, a poetically or rhetorically ambitious use of words, a figurative as opposed to literal use.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/reasoning-analogy/" rel="noopener noreferrer">Analogy and Analogical Reasoning</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Analogy, a comparison between two objects, or systems of objects, that highlights respects in which they are thought to be similar.</p>
</li>
<li><p><a href="https://1lib.net/book/1165963/e9aa3d" rel="noopener noreferrer">A Cognitive Theory of Metaphor</a> - <em><strong>MIT Press</strong></em>, 1985. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=a+cognitive+theory+of+metaphor&amp;btnG=" rel="noopener noreferrer">All Versions</a>]. A cognitive account on Metaphor.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/0004370289900775" rel="noopener noreferrer">The structure-mapping engine: Algorithm and examples</a> - <em><strong>Artificial Intelligence</strong></em>, 1989. [<a href="https://scholar.google.com/scholar?cluster=16104901325436513899&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A computational implementation of analogy.</p>
</li>
<li><p><a href="https://cogsci.ucsd.edu/~coulson/203/gentner-markman-97.pdf" rel="noopener noreferrer">Structure mapping in analogy and similarity</a> - <em><strong>American Psychologist</strong></em>, 1997. [<a href="https://scholar.google.com/scholar?cluster=3497411606978611830&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A perspective unifying analogy and similarity judgement.</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/2022-26663-001" rel="noopener noreferrer">A theory of relation learning and cross-domain generalization</a> - <em><strong>Psychological Review</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=8559821723107269122&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A comprehensive review on the perspective of treating analogy as cross-domain generalization.</p>
</li>
<li><p><a href="https://www.pnas.org/content/pnas/116/10/4176.full.pdf" rel="noopener noreferrer">Emergence of analogy from relation learning</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4877125748339538047&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Analogy feature in language models.</p>
</li>
<li><p><a href="https://proceedings.mlr.press/v97/allen19a.html" rel="noopener noreferrer">Analogies Explained: Towards Understanding Word Embeddings</a> - <em><strong>ICML'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15445529659618849253&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Explaining the analogy capability in word embeddings.</p>
</li>
<li><p><a href="https://aclanthology.org/P17-1007/" rel="noopener noreferrer">Skip-Gram − Zipf + Uniform = Vector Additivity</a> - <em><strong>ACL'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=11732363456979525246&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.iiia.csic.es/~enric/papers/generalize_and_blend.pdf" rel="noopener noreferrer">Generalize and Blend: Concept Blending Based on Generalization, Analogy, and Amalgams</a> - <em><strong>ICCC'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=11073359237116879862&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://proceedings.mlr.press/v28/juhwang13.pdf" rel="noopener noreferrer">Analogy-preserving Semantic Embedding for Visual Object Categorization</a> - <em><strong>ICML'13</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=9332855910734484101&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The first application of analogy to machine learning.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2015/file/45f31d16b1058d586fc3be7207b58053-Paper.pdf" rel="noopener noreferrer">VISALOGY: Answering Visual Analogy Questions</a> - <em><strong>NeurIPS'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=7665427758655324654&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/9010418" rel="noopener noreferrer">Detecting Unseen Visual Relations Using Analogies</a> - <em><strong>CVPR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=16686853801653819556&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0004370218301863" rel="noopener noreferrer">Analogy between concepts</a> - <em><strong>Artificial Intelligence</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=1397905953174123757&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A mathematical account on analogy.</p>
</li>
<li><p><a href="https://arxiv.org/abs/1902.00120" rel="noopener noreferrer">Learning to Make Analogies by Contrasting Abstract Relational Structure</a> - <em><strong>ICLR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15521573039503233138&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://aclanthology.org/2020.figlang-1.pdf#page=140" rel="noopener noreferrer">Sky + Fire = Sunset. Exploring Parallels between Visually Grounded Metaphors and Image Classifiers</a> - <em><strong>ACL'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5747285277687442001&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2006.04156.pdf" rel="noopener noreferrer">Analogy as Nonparametric Bayesian Inference over Relational Systems</a> - <em><strong>CogSci'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1798148167130120057&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.cs.jhu.edu/~alanlab/Pubs21/ichien2021visual.pdf" rel="noopener noreferrer">Visual Analogy: Deep Learning Versus Compositional Models</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=1187822306970312749&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A human-deep-learning comparison on similarity judgement.</p>
</li>
<li><p><a href="https://escholarship.org/content/qt3j2576vv/qt3j2576vv.pdf" rel="noopener noreferrer">Preschoolers and adults make inferences from novel metaphors</a> - <em><strong>CogSci'22</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=16038983545360341739&amp;hl=en&amp;as_sdt=0,44" rel="noopener noreferrer">All Versions</a>]. A piece of evidence that understanding metaphors is capable for different cognitive development phases.</p>
</li>
<li><p><a href="https://pcl.sitehost.iu.edu/rgoldsto/pdfs/simdiff.pdf" rel="noopener noreferrer">Similarity involving attributes and relations: Judgments of similarity and difference are not inverses</a> - <em><strong>Psychological Science</strong></em>, 1990. [<a href="https://scholar.google.com/scholar?cluster=13205938250772079784&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="causality"><a class="anchor" aria-hidden="true" tabindex="-1" href="#causality"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Causality</h3><ul>
<li><p><a href="https://en.wikipedia.org/wiki/Causality" rel="noopener noreferrer">Causality</a> - <em><strong>Wikipedia</strong></em>. Wikipedia on causality, which is influence by which one event, process, state, or object (a cause) contributes to the production of another event, process, state, or object (an effect) where the cause is partly responsible for the effect, and the effect is partly dependent on the cause.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/causal-models/" rel="noopener noreferrer">Causal Models</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Causal models, which are mathematical models representing causal relationships within an individual system or population.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/content-causal/" rel="noopener noreferrer">Causal Theories of Mental Content</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on causal theories of mental content, which attempts to explain how thoughts can be about things.</p>
</li>
<li><p><a href="http://www.jakebowers.org/ITVExperiments/angristimbensrubin96.pdf" rel="noopener noreferrer">Identification of Causal Effects Using Instrumental Variables</a> - <em><strong>Journal of the American Statistical Association</strong></em>, 1996. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=17166265099721941605" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.psych.uni-goettingen.de/de/cognition/publikationen-dateien-waldmann/1992_predictive_vs_diagnostic.pdf" rel="noopener noreferrer">Predictive and Diagnostic Learning Within Causal Models: Asymmetries in Cue Competition</a> - <em><strong>Journal of Experimental Psychology</strong></em>, 1992. [<a href="https://scholar.google.com/scholar?cluster=9614241045842043939&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Experimental evidences for distincting causality and association.</p>
</li>
<li><p><a href="https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780195376746.001.0001/oxfordhb-9780195376746-e-46" rel="noopener noreferrer">Causal Reasoning</a> - <em><strong>The Oxford Handbook of Cognitive Psychology</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=11361740093816709089&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ftp.cs.ucla.edu/pub/stat_ser/R265.pdf" rel="noopener noreferrer">Reasoning with cause and effect</a> - 1998. Judea Pearl's tutorials on causal reasoning with operations on Bayesian networks.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/pdf/10.1145/3241036" rel="noopener noreferrer">The Seven Tools of Causal Inference, with Reflections on Machine Learning</a> - <em><strong>Communications of the ACM</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=13296019510897277617&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Judea Pearl's review on causal inference in probabilistic graph models.</p>
</li>
<li><p><a href="https://cardiacmr.hms.harvard.edu/files/cardiacmr/files/toward_causal_representation_learning.pdf" rel="noopener noreferrer">Toward Causal Representation Learning</a> - <em><strong>Proceedings of the IEEE</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=15629454810797806102&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Yoshua Bengio's review on the perspective of treating causal inference as a representation learning problem.</p>
</li>
<li><p><a href="https://cocosci.princeton.edu/tom/papers/tbci.pdf" rel="noopener noreferrer">Theory-Based Causal Induction</a> - <em><strong>Psychological Review</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=13980129728092173387&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Thomas Griffiths' review on causal Bayesian theory induction.</p>
</li>
<li><p><a href="https://ojs.aaai.org//index.php/AAAI/article/view/5483" rel="noopener noreferrer">Theory-Based Causal Transfer: Integrating Instance-Level Induction and Abstract-Level Structure Learning</a> - <em><strong>AAAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9411622427165139667&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A computatinoal account on causal transfer.</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2703_6" rel="noopener noreferrer">Inferring causal networks from observations and interventions</a> - <em><strong>Cognitive Science</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=12050301037347772984&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://cogsci.mindmodeling.org/2015/papers/0418/paper0418.pdf" rel="noopener noreferrer">Constraints on Hypothesis Selection in Causal Learning</a> - <em><strong>CogSci'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=2005&amp;sciodt=0%2C5&amp;cites=16920774374067505248&amp;scipsc=&amp;q=Constraints+on+hypothesis+selection+in+causal+learning&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://cocolab.stanford.edu/papers/GerstenbergEtAl17_PsychScience.pdf" rel="noopener noreferrer">Eye-tracking causality</a> - <em><strong>Psychological Science</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=17518200401109470519" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=d0TfP8EAAAAJ&amp;sortby=pubdate&amp;citation_for_view=d0TfP8EAAAAJ:S16KYo8Pm5AC" rel="noopener noreferrer">What happened? Reconstructing the past through vision and sound</a> - 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=12975579257004398798" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s42113-021-00124-z" rel="noopener noreferrer">How do people generalize causal relations over objects? A non-parametric Bayesian account</a> - <em><strong>Computational Brain &amp; Behavior</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=3364672295201228487" rel="noopener noreferrer">All Versions</a>]. [<a href="https://psyarxiv.com/x57hf/" rel="noopener noreferrer">Preprint</a>]. How do people decide how general a causal relationship is, in terms of the entities or situations it applies to? What features do people use to decide whether a new situation is governed by a new causal law or an old one? How can people make these difficult judgments in a fast, efficient way? This paper addresses these questions in two experiments that ask participants to generalize from one (Experiment 1) or several (Experiment 2) causal interactions between pairs of objects. In each case, participants see an agent object act on a recipient object, causing some changes to the recipient.</p>
</li>
<li><p><a href="https://www.psych.uni-goettingen.de/de/cognition/publikationen-dateien-waldmann/2006_science.pdf" rel="noopener noreferrer">Causal Reasoning in Rats</a> - <em><strong>Science</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=17987039255457850949&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A piece of evidence for the capability of causal reasoning in intelligent animals.</p>
</li>
<li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.183.4674&amp;rep=rep1&amp;type=pdf" rel="noopener noreferrer">Do New Caledonian crows solve physical problems through causal reasoning?</a> - <em><strong>Proceedings of the Royal Society B: Biological Sciences</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=18374985546068164189&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A piece of evidence for the capability of causal reasoning in intelligent animals.</p>
</li>
<li><p><a href="http://fitelson.org/woodward/leslie.pdf" rel="noopener noreferrer">Do six-month-old infants perceive causality?</a> - <em><strong>Cognition</strong></em>, 1987. [<a href="https://scholar.google.com/scholar?cluster=14270905342434182186&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="commonsense"><a class="anchor" aria-hidden="true" tabindex="-1" href="#commonsense"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Commonsense</h3><h4 id="intuitive-physics"><a class="anchor" aria-hidden="true" tabindex="-1" href="#intuitive-physics"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Intuitive Physics</h4><ul>
<li><p><a href="https://github.com/lishiqianhugh/Intuitive_Physics_Reading_List" rel="noopener noreferrer">Intuitive Physics Reading List (⭐15)</a> - <em><strong>GitHub</strong></em>. A reading list on intuitive physics, maintained actively by Shiqian Li.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S1364661317301262" rel="noopener noreferrer">Intuitive Physics: Current Research and Controversies</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?start=0&amp;hl=en&amp;as_sdt=0,5&amp;cluster=12085981794958916203" rel="noopener noreferrer">All Versions</a>]. Hongjing Lu's review on intuitive physics.</p>
</li>
<li><p><a href="https://www.pnas.org/content/pnas/110/45/18327.full.pdf" rel="noopener noreferrer">Simulation as an engine of physical scene understanding</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=5892822406285231676&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://www.pnas.org/content/pnas/suppl/2013/10/18/1306572110.DCSupplemental/pnas.201306572SI.pdf?targetid=nameddest%3DSTXT" rel="noopener noreferrer">Appendix</a>]. The first attempt to computationally simulate intuitive physics.</p>
</li>
<li><p><a href="https://www.pnas.org/doi/pdf/10.1073/pnas.1610344113" rel="noopener noreferrer">Functional neuroanatomy of intuitive physical inference</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=1792195093536891402&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A piece of evidence for the functional part of intuitive physics in human brain.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S1364661317301134" rel="noopener noreferrer">Mind Games: Game Engines as an Architecture for Intuitive Physics</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=14527964477161848029&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>]. Tomer Ullman's review on simulation-based intuitive physics.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0010028517301822" rel="noopener noreferrer">Learning physical parameters from dynamic scenes</a> - <em><strong>Cognitive Psychology</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=5103729321433959736&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0010028521000190" rel="noopener noreferrer">Limits on Simulation Approaches in Intuitive Physics</a> - <em><strong>Cognitive Psychology</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=6329029167380621767" rel="noopener noreferrer">All Versions</a>]. Ernest Davis's perspective against intuitive physics, that physcial reasoning is logical reasoning instead of intuition.</p>
</li>
<li><p><a href="https://psyarxiv.com/y4a8x/download?format=pdf" rel="noopener noreferrer">Partial Mental Simulation Explains Fallacies in Physical Reasoning</a> - <em><strong>Cognitive Neuropsychology</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=15541954459060383152&amp;hl=en&amp;as_sdt=2005" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41562-022-01394-8" rel="noopener noreferrer">Intuitive physics learning in a deep-learning model inspired by developmental psychology</a> - <em><strong>Nature Human Behavior</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=13803979681049451699&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A machine-learning dataset designed to evaluate conceptual understanding of intuitive physics, adopting the violation-of-expectation (VoE) paradigm from developmental psychology; a deep-learning system that learns intuitive physics directly from visual data, inspired by studies of visual cognition in children.</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2019/hash/4191ef5f6c1576762869ac49281130c9-Abstract.html" rel="noopener noreferrer">PHYRE: A New Benchmark for Physical Reasoning</a> - <em><strong>NeurIPS'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=9555658528231205655&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A benchmark for AI physical reasoning.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s42256-022-00583-4" rel="noopener noreferrer">Phy-Q as a measure for physical reasoning intelligence</a> - <em><strong>Nature Machine Intelligence</strong></em>, 2023. [<a href="https://www.nature.com/articles/s42256-019-0072-x" rel="noopener noreferrer">NMI Challenge</a>]. An interactive benchmark for AI physical reasoning.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="ai-commonsense-reasoning"><a class="anchor" aria-hidden="true" tabindex="-1" href="#ai-commonsense-reasoning"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>AI Commonsense Reasoning</h4><ul>
<li><p><a href="https://www.sciencedirect.com/book/9781483207704/representations-of-commonsense-knowledge" rel="noopener noreferrer">Representations of Commonsense Knowledge</a> - <em><strong>Morgan Kaufmann</strong></em>, 1990. [<a href="https://scholar.google.com/scholar?cluster=8861902735724600978" rel="noopener noreferrer">All Versions</a>]. A classic book on commonsense knowledge.</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007%2F3-540-53487-3_59" rel="noopener noreferrer">Towards a theory of commonsense visual reasoning</a> - <em><strong>FSTTCS</strong></em>, 1990. [<a href="https://scholar.google.com/scholar?cluster=13178231862265713961&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on visual commonsense.</p>
</li>
<li><p><a href="http://cs.wellesley.edu/~cs125/reading/commonsenseAI.pdf" rel="noopener noreferrer">Commonsense reasoning and commonsense knowledge in artificial intelligence</a> - <em><strong>Communications of the ACM</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=13786590180441485203&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Gary Marcus's review on commonsense knowledge in AI.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8953217" rel="noopener noreferrer">From Recognition to Cognition: Visual Commonsense Reasoning</a> - <em><strong>CVPR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=15467433880059136365&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="http://visualcommonsense.com/" rel="noopener noreferrer">Project</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1911.11641.pdf" rel="noopener noreferrer">PIQA: Reasoning about Physical Commonsense in Natural Language</a> - <em><strong>AAAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=10110424163152713144&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9156347" rel="noopener noreferrer">Visual Commonsense R-CNN</a> - <em><strong>CVPR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=6886229776034162585&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openreview.net/pdf?id=Byg1v1HKDB" rel="noopener noreferrer">Abductive Commonsense Reasoning</a> - <em><strong>ICLR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16544200144479839958&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Abductive commonsense reasoning on large language models.</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007%2F978-3-030-58558-7_30" rel="noopener noreferrer">VisualCOMET: Reasoning About the Dynamic Context of a Still Image</a> - <em><strong>ECCV'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7681600847940772451&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-3-031-20059-5_32" rel="noopener noreferrer">The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning</a> - <em><strong>ECCV'22</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=18355807581692234364" rel="noopener noreferrer">All Versions</a>]. [<a href="https://arxiv.org/abs/2202.04800" rel="noopener noreferrer">Preprint</a>]. This paper presents Sherlock, an annotated corpus of 103K images for testing machine capacity for abductive reasoning beyond literal image contents. The corpus construction process adopts a free-viewing paradigm: participants first observe and identify salient clues within images (e.g., objects, actions) and then provide a plausible inference about the scene, given the clue.</p>
</li>
<li><p><a href="https://aclanthology.org/2024.naacl-long.469/" rel="noopener noreferrer">UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations</a> - <em><strong>NAACL'24</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=470445696014235795" rel="noopener noreferrer">All Versions</a>]. This paper explores the task of uncommonsense abductive reasoning. Given a piece of context with an unexpected outcome, this task requires reasoning abductively to generate an explanation that makes the unexpected outcome more likely in the context.</p>
</li>
<li><p><a href="https://aclanthology.org/2020.emnlp-main.703.pdf" rel="noopener noreferrer">Experience Grounds Language</a> - <em><strong>EMNLP'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=3734668471751920487&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A perspective on the furture of computational linguistics research---commonsense-driven and embodied language.</p>
</li>
<li><p><a href="https://aclanthology.org/2021.emnlp-main.162/" rel="noopener noreferrer">Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning</a> - <em><strong>EMNLP'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=12305856131717604775&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://www.charleskemp.com/papers/hanrpk_humanlikepropertyinductionisachallengeforlargelanguagemodels.pdf" rel="noopener noreferrer">Human-like property induction is a challenge for large language models</a> - <em><strong>CogSci'22</strong></em>, 2022.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.17390" rel="noopener noreferrer">SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks</a> - <em><strong>NeurIPS'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=3844178012869500706&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://swiftsage.github.io/" rel="noopener noreferrer">Project</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h4 id="commonsense-knowledgebase"><a class="anchor" aria-hidden="true" tabindex="-1" href="#commonsense-knowledgebase"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Commonsense Knowledgebase</h4><ul>
<li><p><a href="https://www.wikihow.com/Main-Page" rel="noopener noreferrer">wikiHow</a> - <em><strong>wikiHow.com</strong></em>. wikiHow is on website hosting step-by-step "How-to" procedural instructions across various domains and topics.</p>
</li>
<li><p><a href="https://theworldavatar.io/" rel="noopener noreferrer">The World Avatar</a> - <em><strong>The World Avatar™</strong></em>. A large-scale dynamic knowledge graph connecting concepts with relations to digitalize molecules, buildings, cities, and countries.</p>
</li>
<li><p><a href="https://faculty.cc.gatech.edu/~isbell/classes/reading/papers/lenat95cyc.pdf" rel="noopener noreferrer">CYC: A Large-Scale Investment in Knowledge Infrastructure</a> - <em><strong>Communications of the ACM</strong></em>, 1995. [<a href="https://scholar.google.com/scholar?cluster=6505009388871605141&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The first attempt to build large-scale commonse knoweldgebase from human knowledge.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1612.03975.pdf" rel="noopener noreferrer">ConceptNet 5.5: An Open Multilingual Graph of General Knowledge</a> - <em><strong>AAAI'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=7089916805257737701&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Latest version of ConceptNet.</p>
</li>
<li><p><a href="https://www.aaai.org/Library/Symposia/Spring/2002/ss02-09-011.php" rel="noopener noreferrer">The Public Acquisition of Commonsense Knowledge</a> - <em><strong>Proceedings of AAAI Spring Symposium on Acquiring (and Using) Linguistic (and World) Knowledge for Information Access</strong></em>, 2002. [<a href="https://scholar.google.com/scholar?cluster=12533779219524472080&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The first attempt for acquring commonsense knowlege from humans' activities on the internet.</p>
</li>
<li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.472.914&amp;rep=rep1&amp;type=pdf" rel="noopener noreferrer">Open Mind Common Sense: Knowledge Acquisition from the General Public</a> - <em><strong>OTM Confederated International Conferences'02</strong></em>, 2002. [<a href="https://scholar.google.com/scholar?cluster=11431785236825227404&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]..</p>
</li>
<li><p><a href="http://www.aladdin.cs.cmu.edu/papers/pdfs/y2006/verbosity.pdf" rel="noopener noreferrer">Verbosity: A Game for Collecting Common-Sense Facts</a> - <em><strong>CHI'06</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=7793704394155465847&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://dl.acm.org/doi/fullHtml/10.1145/1378704.1378719" rel="noopener noreferrer">Designing games with a purpose</a> - <em><strong>Communications of the ACM</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=18332117920150730595&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://people.mpi-inf.mpg.de/~ntandon/papers/aaai-2014-tandon.pdf" rel="noopener noreferrer">Acquiring Comparative Commonsense Knowledge from the Web</a> - <em><strong>AAAI'14</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=16641273554706459553&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9904017" rel="noopener noreferrer">Visual Concept Programming: A Visual Analytics Approach to Injecting Human Intelligence at Scale</a> - <em><strong>IEEE Transactions on Visualization and Computer Graphics</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=10724509334112758172&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. This paper presents Visual Concept Programming, a first-of-its-kind visual analytics approach of using visual concepts to program image data at scale while requiring a few human efforts.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="inductive-logic--program-synthesis"><a class="anchor" aria-hidden="true" tabindex="-1" href="#inductive-logic--program-synthesis"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Inductive Logic &amp; Program Synthesis</h3><ul>
<li><p><a href="https://plato.stanford.edu/entries/logic-inductive/" rel="noopener noreferrer">Inductive Logic</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Inductive Logic, which is a logic of evidential support.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/modeltheory-fo/" rel="noopener noreferrer">First-order Model Theory</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on First-order Model Theory, which is a branch of mathematics that deals with the relationships between descriptions in first-order languages and the structures that satisfy these descriptions.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/logic-paraconsistent/" rel="noopener noreferrer">Paraconsistent Logic</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Paraconsistent Logic, where any logic is paraconsistent as long as it is not explosive.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/logical-consequence/" rel="noopener noreferrer">Logical Consequence</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Logical Consequence, which is about the relation between premises and conclusions in valid arguments.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/logical-pluralism/" rel="noopener noreferrer">Logic Pluralism</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Logic Pluralism, which is the view that there is more than one correct logic.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/logic-firstorder-emergence/" rel="noopener noreferrer">The Emergence of First-Order Logic</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on the emergence of first-order logic, mainly about first-order logic is natural retrospect.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/logic-higher-order/" rel="noopener noreferrer">Second-order and Higher-order Logic</a> - <em><strong>Plato Stanford</strong></em>.</p>
</li>
<li><p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/10/program_synthesis_now.pdf" rel="noopener noreferrer">Program Synthesis</a> - <em><strong>Foundations and Trends in Programming Languages</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=5442933587668978421&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Sumit Gulwani's comprehensive review on program synthesis.</p>
</li>
<li><p><a href="https://www.ijcai.org/Proceedings/83-1/Papers/109.pdf" rel="noopener noreferrer">The Discovery of the Equator or Concept Driven Learning</a> - <em><strong>IJCAI'83</strong></em>, 1983. [<a href="https://scholar.google.com/scholar?cluster=15712225225140903169&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on second-order metarules.</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007%2F3-540-44797-0_10" rel="noopener noreferrer">Towards combining inductive logic programming with Bayesian networks</a> - <em><strong>ILP'01</strong></em>, 2001. [<a href="https://scholar.google.com/scholar?cluster=2904180673047700407&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://www.doc.ic.ac.uk/~shm/Papers/metagol_gram.pdf" rel="noopener noreferrer">Meta-interpretive learning: application to grammatical inference</a> - <em><strong>Machine Learning</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=17075313112718885592&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Stephen Muggleton's original paper on Meta-Interpretive Learning (MIL).</p>
</li>
<li><p><a href="http://andrewcropper.com/pubs/ijcai15-metagolo.pdf" rel="noopener noreferrer">Learning Efficient Logical Robot Strategies Involving Composable Objects</a> - <em><strong>IJCAI'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=5109851972354087162&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://andrewcropper.com/pubs/ijcai16-metafunc.pdf" rel="noopener noreferrer">Learning Higher-Order Logic Programs through Abstraction and Invention</a> - <em><strong>IJCAI'16</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=10945054943203858325&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007%2F978-3-319-99960-9_3" rel="noopener noreferrer">How Much Can Experimental Cost Be Reduced in Active Learning of Agent Strategies?</a> - <em><strong>ILP'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8152380236842970357&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/s10994-018-5710-8" rel="noopener noreferrer">Meta-Interpretive Learning from noisy images</a> - <em><strong>Machine Learning</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=5719375383968868329&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://andrewcropper.com/pubs/mlj18-metaopt.pdf" rel="noopener noreferrer">Learning efficient logic programs</a> - <em><strong>Machine Learning</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=17955696870252443734&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://andrewcropper.com/pubs/mlj19-metaho.pdf" rel="noopener noreferrer">Learning higher-order logic programs</a> - <em><strong>Machine Learning</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=6723896359456002413&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://andrewcropper.com/pubs/mlj19-reduce.pdf" rel="noopener noreferrer">Logical reduction of metarules</a> - <em><strong>Machine Learning</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4577603126537024540&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://andrewcropper.com/pubs/ijcai19-playgol.pdf" rel="noopener noreferrer">Playgol: Learning Programs Through Play</a> - <em><strong>IJCAI'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=556522464212000763&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007%2Fs00354-019-00054-2" rel="noopener noreferrer">Machine Discovery of Comprehensible Strategies for Simple Games Using Meta-interpretive Learning</a> - <em><strong>New Generation Computing</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=11019349634035542991&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://andrewcropper.com/pubs/aaai20-forgetgol.pdf" rel="noopener noreferrer">Forgetting to Learn Logic Programs</a> - <em><strong>AAAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=13676986733133377042&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.ijcai.org/proceedings/2020/673" rel="noopener noreferrer">Turning 30: New Ideas in Inductive Logic Programming</a> - <em><strong>IJCAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=17980870844719684257&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2008.07912" rel="noopener noreferrer">Inductive logic programming at 30: a new introduction</a> - <em><strong>Journal of Artificial Intelligence Research</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=317114056670544302&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A 30-year comprehensive review on Inductive Logic Programming.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2005.02259.pdf" rel="noopener noreferrer">Learning programs by learning from failures</a> - <em><strong>Machine Learning</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=6797200487935462023&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.ijcai.org/proceedings/2020/320" rel="noopener noreferrer">Complete Bottom-Up Predicate Invention in Meta-Interpretive Learning</a> - <em><strong>IJCAI'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=6085183078630665234&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2106.07464.pdf" rel="noopener noreferrer">Meta-Interpretive Learning as Metarule Specialisation</a> - <em><strong>Machine Learning</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=14684315775211086859&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0004370204000591" rel="noopener noreferrer">Qualitative choice logic</a> - <em><strong>Artificial Intelligence</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=1586187056162326386&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.ijcai.org/Proceedings/16/Papers/278.pdf" rel="noopener noreferrer">Derivative-free optimization of high-dimensional non-convex functions by sequential random embeddings</a> - <em><strong>IJCAI'16</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=15955040483290586781&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/S0024610704006106" rel="noopener noreferrer">Finitely Generated Groups and First-Order Logic</a> - <em><strong>Journal of The London Mathematical Society-second Series</strong></em>, 2005. [<a href="https://scholar.google.com/scholar?cluster=3457158221419711506&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://vigilworkshop.github.io/static/papers-2021/25.pdf" rel="noopener noreferrer">Leveraging Language for Abstraction and Program Search</a> - <em><strong>ICML'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Leveraging+Language+for+Abstraction+and+Program+Search&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2021/hash/f7e2b2b75b04175610e5a00c1e221ebb-Abstract.html" rel="noopener noreferrer">Program Synthesis Guided Reinforcement Learning</a> - <em><strong>NeurIPS'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=17353674428642875269&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://cogtoolslab.github.io/pdf/wang_cogsci_2021a.pdf" rel="noopener noreferrer">Learning Part-Based Abstractions for Visual Object Concepts</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?lookup=0&amp;q=Learning+Part-Based+Abstractions+for+Visual+Object+Concepts&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.07732" rel="noopener noreferrer">Program Synthesis with Large Language Models</a> - 2021. [<a href="https://scholar.google.com/scholar?cluster=15213050540818392833" rel="noopener noreferrer">All Versions</a>]. This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/3571249" rel="noopener noreferrer">Combining Functional and Automata Synthesis to Discover Causal Reactive Programs</a> - <em><strong>POPL'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=10470162446663474225&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A new algorithm that synthesizes functional reactive programs from observation data, which iterates between a functional synthesis step, which attempts to generate a transition function over observed states, and an automata synthesis step, which adds any additional latent state necessary to fully account for the observations.</p>
</li>
<li><p><a href="http://cap.csail.mit.edu/sites/default/files/research-pdfs/Synthesizing%20theories%20of%20human%20language%20with%20Bayesian%20program%20induction.pdf" rel="noopener noreferrer">Synthesizing theories of human language with Bayesian program induction</a> - <em><strong>Nature Communications</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?cluster=8603772394100237159&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.12672" rel="noopener noreferrer">From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=13778788929096574993" rel="noopener noreferrer">All Versions</a>]. Rational meaning construction, a computational framework for language-informed thinking that combines neural language models with probabilistic models for rational inference. Linguistic meaning is framed as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling.</p>
</li>
<li><p><a href="https://proceedings.mlr.press/v139/hong21a.html" rel="noopener noreferrer">Latent Programmer: Discrete Latent Codes for Program Synthesis</a> - <em><strong>ICML'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=9789877360194738968" rel="noopener noreferrer">All Versions</a>]. Paper introducing the Latent Programmer, a two-level program synthesis method that first predicts a discrete latent code from input/output examples, and then generates the program in the target language.</p>
</li>
<li><p><a href="https://proceedings.mlr.press/v202/gao23f" rel="noopener noreferrer">PAL: Program-aided Language Models</a> - <em><strong>ICML'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=14898051625978777315&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Paper presenting an approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter.</p>
</li>
<li><p><a href="https://aclanthology.org/2023.acl-long.411/" rel="noopener noreferrer">Large Language Models Meet NL2Code: A Survey</a> - <em><strong>ACL'23</strong></em>, 2023. [<a href="https://scholar.google.com/scholar?cluster=11868015824802341463&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://nl2code.github.io/" rel="noopener noreferrer">NL2Code Website</a>]. A paper presenting a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics, suggesting that the key factors contributing to the success of large language models for NL2Code are “Large Size, Premium Data, Expert Tuning”.</p>
</li>
<li><p><a href="https://dl.acm.org/doi/abs/10.1145/3597503.3608128" rel="noopener noreferrer">A Large-Scale Survey on the Usability of AI Programming Assistants: Successes and Challenges</a> - <em><strong>ICSE'24</strong></em>, 2024. [<a href="https://scholar.google.com/scholar?cluster=3696356619002071917&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A survey finding that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2308.10620" rel="noopener noreferrer">Large Language Models for Software Engineering: A Systematic Literature Review</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=10466731638053452642&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A systematic literature review on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="knowledge-representation"><a class="anchor" aria-hidden="true" tabindex="-1" href="#knowledge-representation"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Knowledge Representation</h3><ul>
<li><p><a href="https://1lib.net/book/511192/9eab86" rel="noopener noreferrer">Handbook of Knowledge Representation</a> - <em><strong>Elsevier</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=14732064619564679879&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A pragmatical handbook for all kinds of knowledge representation modes.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/logic-ontology/" rel="noopener noreferrer">Logic and Ontology</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on logic and ontology, mainly about the intersections of logic and ontology in many significant philosophy problems.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/language-thought/" rel="noopener noreferrer">The Language of Thought Hypothesis</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on the laugnage of though hypothesis, which proposes that thinking occurs in a mental language.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/knowledge-analysis/" rel="noopener noreferrer">The Analysis of Knowledge</a> - <em><strong>Plato Stanford</strong></em>.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/scientific-representation/" rel="noopener noreferrer">Scientific Representation</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on scientific representation, focusing on how scientific models represent their target systems.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/self-knowledge/" rel="noopener noreferrer">Self-Knowledge</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on self-knowledge, which standardly refers to knowledge of one's own mental states—that is, of what one is feeling or thinking, or what one believes or desires.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/common-knowledge/" rel="noopener noreferrer">Common Knowledge</a> - <em><strong>Plato Stanford</strong></em>.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/sense-data/" rel="noopener noreferrer">Sense-Data</a> - <em><strong>Plato Stanford</strong></em>.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/supervenience/" rel="noopener noreferrer">Supervenience</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on supervenience, where a set of properties A supervenes upon another set B just in case no two things can differ with respect to A-properties without also differing with respect to their B-properties.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/logic-dialogical/" rel="noopener noreferrer">Dialogical Logic</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on dialogical logic, which is a dialogue-based approach to logic and argumentation rooted in a research tradition that goes back to dialectics in Greek Antiquity, when problems were approached through dialogues in which opposing parties discussed a thesis through questions and answers.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/logic-temporal/" rel="noopener noreferrer">Temporal Logic</a> - <em><strong>Plato Stanford</strong></em>.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/logic-modal/" rel="noopener noreferrer">Modal Logic</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Modal Logic, which is the study of the deductive behavior of the expressions 'it is necessary that' and 'it is possible that'.</p>
</li>
<li><p><a href="https://plato.stanford.edu/entries/logic-epistemic/" rel="noopener noreferrer">Epistemic Logic</a> - <em><strong>Plato Stanford</strong></em>. A computational philosophy account on Epistemic Logic, which is a subfield of epistemology concerned with logical approaches to knowledge, belief and related notions.</p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/Epistemic_modal_logic" rel="noopener noreferrer">Epistemic Modal Logic</a> - <em><strong>Wikipedia</strong></em>.</p>
</li>
<li><p><a href="https://perception.jhu.edu/files/PDFs/21_Relations/HafriFirestone_2021_SeeingRelations_TiCS.pdf" rel="noopener noreferrer">The Perception of Relations</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=12190078466818849725&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>]. Chaz Firestone's review on the perception of relation, in constrast to the conventional reasoning view.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/0004370284900390" rel="noopener noreferrer">Commonsense reasoning about causality: Deriving behavior from structure</a> - <em><strong>Artificial Intelligence</strong></em>, 1984. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=14940738362673077704" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1023/B:SYNT.0000024912.56773.5e" rel="noopener noreferrer">Logics for Epistemic Programs</a> - <em><strong>Synthese</strong></em>, 2004. [<a href="https://scholar.google.com/scholar?cluster=11403619699670839488&amp;hl=en&amp;as_sdt=0,5&amp;as_vis=1" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://tomgruber.org/writing/ontolingua-kaj-1993.pdf" rel="noopener noreferrer">A Translation Approach to Portable Ontology Specifications</a> - <em><strong>Knowledge Acquisition</strong></em>, 1993. [<a href="https://scholar.google.com/scholar?cluster=14668658395073605123&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://www.cs.ox.ac.uk/activities/ieg/e-library/sources/harnad90_sgproblem.pdf" rel="noopener noreferrer">The Symbolic Grounding Problem</a> - <em><strong>Physica D: Nonlinear Phenomena</strong></em>, 1990. [<a href="https://scholar.google.com/scholar?cluster=6279614024681929496&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-7687.2007.00585.x?__cf_chl_captcha_tk__=pmd_Q6xVT1AstoEUxA7xS3_10HyDVsk8W_DzWgOPho_Njnw-1635210931-0-gqNtZGzNA1CjcnBszQvl" rel="noopener noreferrer">Learning overhypotheses with hierarchical Bayesian models</a> - <em><strong>Developmental Science</strong></em>, 2007. [<a href="https://scholar.google.com/scholar?cluster=18041836774924845900&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://escholarship.org/content/qt19v2r2ws/qt19v2r2ws.pdf" rel="noopener noreferrer">Learning Causal Schemata</a> - <em><strong>CogSci'07</strong></em>, 2007, [<a href="https://scholar.google.com/scholar?cluster=5008191267417189643&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.pnas.org/content/105/31/10687" rel="noopener noreferrer">The discovery of structural form</a> - <em><strong>Proceedings of the National Academy of Sciences</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=10433149156915110486&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Chales Kemp's review on theory induction.</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/full/10.1080/03640210701802071" rel="noopener noreferrer">A Rational Analysis of Rule-Based Concept Learning</a> - <em><strong>Cognitive Science</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=7765061503727822620&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://escholarship.org/content/qt50r1c7qh/qt50r1c7qh.pdf" rel="noopener noreferrer">Modeling semantic cognition as logical dimensionality reduction</a> - <em><strong>CogSci'08</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=17061801746839695691&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://www.charleskemp.com/papers/KempGT08.pdf" rel="noopener noreferrer">Theory Acquisition and the Language of Thought</a> - <em><strong>CogSci'08</strong></em>, 2008. [<a href="https://scholar.google.com/scholar?cluster=1839916602381147749&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://web.mit.edu/tomeru/www/papers/tlss2010.pdf" rel="noopener noreferrer">Theory Acquisition as Stochastic Search</a> - <em><strong>CogSci'10</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=16324634056226561429&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://www.charleskemp.com/papers/kemptng09.pdf" rel="noopener noreferrer">A probabilistic model of theory formation</a> - <em><strong>Cognition</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=7705799129887482041&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://core.ac.uk/display/78064072" rel="noopener noreferrer">Bootstrapping in a language of thought: A formal model of numerical concept learning</a> - <em><strong>Cognition</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=13046606910781656302&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://cbmm-dev.mit.edu/sites/default/files/publications/CBMM-Memo-010.pdf" rel="noopener noreferrer">Concepts in a Probabilistic Language of Thought</a> - <em><strong>Center for Brains, Minds, and Machines MEMO No.010</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=14593712389828476130" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://www.charleskemp.com/papers/kemp_exploringtheconceptualuniverse.pdf" rel="noopener noreferrer">Exploring the Conceptual Universe</a> - <em><strong>Psychological Review</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=17824067813343816306&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://www.charleskemp.com/papers/kempj_ataxonomyofinductiveproblems.pdf" rel="noopener noreferrer">A taxonomy of inductive problems</a> - <em><strong>Psychonomic Bulletin &amp; Review</strong></em>, 2014. [<a href="https://scholar.google.com/scholar?cluster=2571009743105592927&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://colala.berkeley.edu/papers/piantadosi2016logical.pdf" rel="noopener noreferrer">The Logical Primitives of Thought: Empirical Foundations for Compositional Cognitive Models</a> - <em><strong>Psychological Review</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=5316027496661813145&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12580" rel="noopener noreferrer">The Emergence of Organizing Structure in Conceptual Representation</a> - <em><strong>Cognitive Science</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=4986316323923233074&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://cogtoolslab.github.io/pdf/wang_cogsci_2021b.pdf" rel="noopener noreferrer">Theory Acquisition as Constraint-Based Program Synthesis</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=525148607069840280&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://escholarship.org/uc/item/9j00x928" rel="noopener noreferrer">Connecting perceptual and procedural abstractions in physical construction</a> - <em><strong>CogSci'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Connecting+perceptual+and+procedural+abstractions+in+physical+construction&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.biorxiv.org/content/10.1101/2021.03.19.385641v1.full.pdf" rel="noopener noreferrer">Invariant representation of physical stability in the human brain</a> - 2021. [<a href="https://scholar.google.com/scholar?cluster=17431019238600295521&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.4086&amp;rep=rep1&amp;type=pdf" rel="noopener noreferrer">Introduction to The Fluent Calculus</a> - <em><strong>Linkoeping University Electronic Press</strong></em>, 1998. [<a href="https://scholar.google.com/scholar?cluster=12069059079023496731&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0004370299000338" rel="noopener noreferrer">From situation calculus to fluent calculus: State update axioms as a solution to the inferential frame problem</a> - <em><strong>Artificial Intelligence</strong></em>, 1999. [<a href="https://scholar.google.com/scholar?cluster=10854895617698839149&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://www.stat.ucla.edu/~sczhu/papers/Conf_2013/Learning_AoG_NeurIPS_2013.pdf" rel="noopener noreferrer">Unsupervised Structure Learning of Stochastic And-Or Grammars</a> - <em><strong>NeurIPS'13</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=4354984630817844670" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://psyarxiv.com/ysndt" rel="noopener noreferrer">Algorithms of Adaptation in Inductive Inference</a> - <em><strong>Cognitive Psychology</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=16222039361294164246&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/0010027795006743" rel="noopener noreferrer">A representational analysis of numeration systems</a> - <em><strong>Cognition</strong></em>, 1995. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=8852566070856662412" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Papadopoulos_Learning_Program_Representations_for_Food_Images_and_Cooking_Recipes_CVPR_2022_paper.html" rel="noopener noreferrer">Learning Program Representations for Food Images and Cooking Recipes</a> - <em><strong>CVPR'22</strong></em>, 2022. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=7690010749576063125" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.07455" rel="noopener noreferrer">Reasoning about Procedures with Natural Language Processing: A Tutorial</a> - 2023. [<a href="https://scholar.google.com/scholar?cluster=11364086808527515615&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="cognitive-development"><a class="anchor" aria-hidden="true" tabindex="-1" href="#cognitive-development"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Cognitive Development</h3><ul>
<li><p><a href="https://arxiv.org/abs/1810.07528" rel="noopener noreferrer">Machine Common Sense Concept Paper</a> - <em><strong>DARPA</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=1603121108181262769&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. DARPA's perspective on integrating core knowledge from development psychology into machine intelligence systems.</p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/Cognitive_development" rel="noopener noreferrer">Cognitive Development</a> - <em><strong>Wikipedia</strong></em>.</p>
</li>
<li><p><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Cognitive+Development%3A+an+information+processing+approach&amp;btnG=" rel="noopener noreferrer">Cognitive development: An information processing approach</a> - <em><strong>B.Blackwell</strong></em>, 1991. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Cognitive+development%3A+An+information+processing+approach&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/2012-12791-001" rel="noopener noreferrer">Reconstructing constructivism: Causal models, Bayesian learning mechanisms, and the theory theory</a> - <em><strong>Psychological Bulletin</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=11218217347365817167&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Alison Gopnik's review on the constructivism idea of developmental research.</p>
</li>
<li><p><a href="https://doi.apa.org/doiLanding?doi=10.1037/rev0000153" rel="noopener noreferrer">Towards a rational constructivist theory of cognitive development</a> - <em><strong>Psychological Review</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=3294824172745724080&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Fei Xu's review extending Gopnik's view of constructivism, with the rationality as constraint.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S1364661312001301" rel="noopener noreferrer">The origins of inquiry: inductive inference and exploration in early childhood</a> - <em><strong>Trends in Cognitive Sciences</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=5189329081728071335&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Laura Schulz's review on children's exploratory play.</p>
</li>
<li><p><a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-devpsych-070120-014806" rel="noopener noreferrer">Play, Curiosity, and Cognition</a> - <em><strong>Annual Review of Developmental Psychology</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=10278208468154249192&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>]. Laura Schulz's review on children's exploratory play, which proposes a new perspective on exploratory play to explain the emergence of irrational behaviors in play.</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/1981-32566-001" rel="noopener noreferrer">From exploration to play: A cross-sectional study of infant free play behavior</a> - <em><strong>Developmental Psychology</strong></em>, 1981. [<a href="https://scholar.google.com/scholar?cluster=15547331535034599545&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://srcd.onlinelibrary.wiley.com/doi/abs/10.1111/1467-8624.00224" rel="noopener noreferrer">Detecting Blickets: How Young Children Use Information about Novel Causal Powers in Categorization and Induction</a> - <em><strong>Children Development</strong></em>, 2003. [<a href="https://scholar.google.com/scholar?cluster=9049737233568227380&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://eccl.scripts.mit.edu/papers/bonawitzandschulzseriousfun.pdf" rel="noopener noreferrer">Serious fun: Preschoolers engage in more exploratory play when evidence is confounded</a> - <em><strong>Developmental Psychology</strong></em>, 2007. [<a href="https://scholar.google.com/scholar?cluster=3033619407322882147&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://stahla.pages.tcnj.edu/files/2015/08/Stahl_Feigenson_Science_2015.pdf" rel="noopener noreferrer">Observing the unexpected enhances infants' learning and exploration</a> - <em><strong>Science</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?start=10&amp;hl=en&amp;as_sdt=0,5&amp;cluster=9247917261616759689" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/2008-12114-008" rel="noopener noreferrer">Word, thought, and deed: the role of object categories in children's inductive inferences and exploratory play</a> - <em><strong>Developmental Psychology</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=13947689064550390312&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0010027711000916" rel="noopener noreferrer">Where science starts: Spontaneous experiments in preschoolers' exploratory play</a> - <em><strong>Cognition</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=16321989770180281706" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://alisongopnik.com/Papers_Alison/Scientific%20Thinking%20in%20young%20Children.pdf" rel="noopener noreferrer">Scientific thinking in young children: Theoretical advances, empirical research, and policy implications</a> - <em><strong>Science</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=9103846738385460508&amp;hl=en&amp;as_sdt=2005" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://eccl.scripts.mit.edu/papers/Finding%20New%20Facts_%20Thinking%20New%20Thoughts.pdf" rel="noopener noreferrer">Finding New Facts; Thinking New Thoughts</a> - <em><strong>Advances in Child Development and Behavior</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Finding+new+facts%3B+thinking+new+thoughts&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0885201412000445" rel="noopener noreferrer">Theory learning as stochastic search in the language of thought</a> - <em><strong>Cognitive Development</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=8036476579458645432&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.science.org/doi/abs/10.1126/science.aan2317" rel="noopener noreferrer">Infants make more attempts to achieve a goal when they see adults persist</a> - <em><strong>Science</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=2617011825272996810&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://cognitivesciencesociety.org/cogsci20/papers/0716/0716.pdf" rel="noopener noreferrer">Knowing when to quit: Children consider access to solutions when deciding whether to persist</a> - <em><strong>CogSci'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=15997297570269958414&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://psyarxiv.com/aq3rp/" rel="noopener noreferrer">Bayesian Models of Conceptual Development: Learning as Building Models of the World</a> - <em><strong>Annual Review of Developmental Psychology</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=646614032563248495&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12765" rel="noopener noreferrer">Sticking to the Evidence? A Behavioral and Computational Case Study of Micro-Theory Change in the Domain of Magnetism</a> - <em><strong>Cognitive Science</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=4409900195679222965&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://junyichu.mit.edu/sites/default/files/documents/2018-05-14%20CogSci%20Final.pdf" rel="noopener noreferrer">Cognitive pragmatism: Children flexibly choose between facts and conjectures</a> - <em><strong>CogSci'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=6978944437676543728&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://psyarxiv.com/9yra2/" rel="noopener noreferrer">Exploratory play, rational action, and efficient search</a> - <em><strong>CogSci'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=17529638197045429028&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://srcd.onlinelibrary.wiley.com/doi/full/10.1111/cdev.13647?saml_referrer" rel="noopener noreferrer">Children selectively endorse speculative conjectures</a> - <em><strong>Child Development</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=5672344544260882286&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://psycnet.apa.org/buy/2017-12497-003" rel="noopener noreferrer">Learning higher-order generalizations through free play: Evidence from 2- and 3-year-old children</a> - <em><strong>Developmental Psychology</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=4386474921214936914&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0502" rel="noopener noreferrer">Childhood as a solution to explore–exploit tensions</a> - <em><strong>Philosophical Transactions of the Royal Society B: Biological Sciences</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=11960188575664977017&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41467-021-23431-2" rel="noopener noreferrer">Children's exploratory play tracks the discriminability of hypotheses</a> - <em><strong>Nature Communications</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=12389351553206792907&amp;hl=en&amp;as_sdt=0,5&amp;as_ylo=2020" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://srcd.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-8624.2010.01499.x?saml_referrer" rel="noopener noreferrer">A Developmental Perspective on Executive Function</a> - <em><strong>Child Development</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=11347590808138984649&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://journals.sagepub.com/doi/pdf/10.1177/1745691620904771" rel="noopener noreferrer">Rethinking Executive Function and Its Development</a> - <em><strong>Psychological Science</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16570230278367237499&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.harvardlds.org/wp-content/uploads/2017/01/Perception-of-partly-occluded-objects-in-infancy-1.pdf" rel="noopener noreferrer">Perception of partly occluded objects in infancy</a> - <em><strong>Cognitive Psychology</strong></em>, 1983. [<a href="https://scholar.google.com/scholar?cluster=4182861116190610992&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/article/10.3758/s13428-012-0210-4" rel="noopener noreferrer">Age-of-acquisition ratings for 30,000 English words</a> - <em><strong>Behavior Research Methods</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=6752414178722956940&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="http://crr.ugent.be/archives/806" rel="noopener noreferrer">Project</a>]. A database for age-of-acquisition ratings for over 30k English words.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="learning-in-the-open-world"><a class="anchor" aria-hidden="true" tabindex="-1" href="#learning-in-the-open-world"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Learning in the Open World</h3><ul>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S002224961730010X" rel="noopener noreferrer">Online learning of symbolic concepts</a> - <em><strong>Journal of Mathematical Psychology</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?start=20&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5&amp;cites=8036476579458645432&amp;scipsc=" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8413121" rel="noopener noreferrer">Zero-Shot Learning—A Comprehensive Evaluation of the Good, the Bad and the Ugly</a> - <em><strong>IEEE Transactions on Pattern Analysis and Machine Intelligence</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=11909080239486864961&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A comprehensive review on zero-shot learning.</p>
</li>
<li><p><a href="https://www.4paradigm.com/upload/file/20210427/20210427225045_12063.pdf" rel="noopener noreferrer">Generalizing from a few examples: A survey on few-shot learning</a> - <em><strong>ACM Computing Survey</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7932202448069313464&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/7298799" rel="noopener noreferrer">Towards Open World Recognition</a> - <em><strong>CVPR'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=856704237994181529&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The first paper introducing the problem of open-world recognition.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7780542" rel="noopener noreferrer">Towards Open Set Deep Networks</a> - <em><strong>CVPR'16</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=3571743951915089896&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2007.02519.pdf" rel="noopener noreferrer">In the Wild: From ML Models to Pragmatic ML Systems</a> - <em><strong>ICLR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=15243890330014986346&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A comprehensive review on incremental machine learning.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2002.04108.pdf" rel="noopener noreferrer">Adversarial Filters of Dataset Biases</a> - <em><strong>ICML'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=11617966867048191189&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2009.01797.pdf" rel="noopener noreferrer">A Wholistic View of Continual Learning with Deep Neural Networks: Forgotten Lessons and the Bridge to Active and Open World Learning</a> - 2020. [<a href="https://scholar.google.com/scholar?cluster=2640432662088551010&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2011.12216.pdf" rel="noopener noreferrer">Energy-Based Models for Continual Learning</a> - <em><strong>NeurIPS'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7094884707139778576&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://energy-based-model.github.io/Energy-Based-Models-for-Continual-Learning/" rel="noopener noreferrer">Project</a>].</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Learning_to_Learn_Image_Classifiers_With_Visual_Analogy_CVPR_2019_paper.pdf" rel="noopener noreferrer">Learning to Learn Image Classifiers with Visual Analogy</a> - <em><strong>CVPR'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=6285495755337309034&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1804.04340v2.pdf" rel="noopener noreferrer">Zero-Shot Object Detection</a> - <em><strong>ECCV'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=2027060030559987993&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2103.02603v1.pdf" rel="noopener noreferrer">Towards Open World Object Detection</a> - <em><strong>CVPR'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=9715328489246217151&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. [<a href="https://github.com/JosephKJ/OWOD" rel="noopener noreferrer">Project (⭐1.1k)</a>].</p>
</li>
<li><p><a href="https://dl.acm.org/doi/pdf/10.1145/3123266.3123323" rel="noopener noreferrer">Learning to Recognise Unseen Classes by A Few Similes</a> - <em><strong>MM'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?q=related:FZZr2BK0U6YJ:scholar.google.com/&amp;scioq=Learning+to+Recognise+Unseen+Classes+by+A+Few+Similes&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.kr.org/2020/87/kr2020-0087-chen-et-al.pdf" rel="noopener noreferrer">Ontology-guided Semantic Composition for Zero-Shot Learning</a> - <em><strong>KR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=1825132732653262003&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2102.07339.pdf" rel="noopener noreferrer">OntoZSL: Ontology-enhanced Zero-shot Learning</a> - <em><strong>WWW'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=1042573079110416209&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.00070" rel="noopener noreferrer">Knowledge-aware Zero-Shot Learning: Survey and Perspective</a> - <em><strong>IJCAI'21</strong></em> 2021. [<a href="https://scholar.google.com/scholar?cluster=2596179801089642923&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/8099612" rel="noopener noreferrer">From Red Wine to Red Tomato: Composition with Context</a> - <em><strong>CVPR'17</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=6959320578989247472&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://link.springer.com/chapter/10.1007%2F978-3-030-01246-5_11" rel="noopener noreferrer">Attributes as Operators: Factorizing Unseen Attribute-Object Compositions</a> - <em><strong>ECCV'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=11627198158637727139&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/9010671" rel="noopener noreferrer">Learning Compositional Representations for Few-Shot Recognition</a> - <em><strong>CVPR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7363445845219257348&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/9156505" rel="noopener noreferrer">Symmetry and Group in Attribute-Object Compositions</a> - <em><strong>CVPR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=16870815556752021056&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2020/file/1010cedf85f6a7e24b087e63235dc12e-Paper.pdf" rel="noopener noreferrer">A causal view of compositional zero-shot recognition</a> - <em><strong>NeurIPS'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=2543173389101020482&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://dl.acm.org/doi/10.1145/3394171.3413849" rel="noopener noreferrer">Compositional Few-Shot Recognition with Primitive Discovery and Enhancing</a> - <em><strong>MM'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=15817839338790433509&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/9156655" rel="noopener noreferrer">Learning Unseen Concepts via Hierarchical Decomposition and Composition</a> - <em><strong>CVPR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=14161656227038242300&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="learning-with-cognitive-plausibility"><a class="anchor" aria-hidden="true" tabindex="-1" href="#learning-with-cognitive-plausibility"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Learning with Cognitive Plausibility</h3><ul>
<li><p><a href="https://en.wikipedia.org/wiki/Accuracy_and_precision" rel="noopener noreferrer">Accuracy and Precision</a> - <em><strong>Wikipedia</strong></em>. Wikipedia on the distinctions and the trade-off between accuracy and precision.</p>
</li>
<li><p><a href="https://www.annualreviews.org/doi/abs/10.1146/annurev.ps.40.020189.003131" rel="noopener noreferrer">Cognitive Science: Definition, Status, and Questions</a> - <em><strong>Annual Review of Psychology</strong></em>, 1989. [<a href="https://scholar.google.com/scholar?cluster=8549671583307260475&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://people.csail.mit.edu/torralba/courses/6.870/papers/Biederman_RBC_1987.pdf" rel="noopener noreferrer">Recognition-by-Components: A Theory of Human Image Understanding</a> - <em><strong>Psychological Review</strong></em>, 1987. [<a href="https://scholar.google.com/scholar?cluster=16522931798979362446&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on the recognition-by-components theory.</p>
</li>
<li><p><a href="https://www.nature.com/articles/s41586-019-1138-y" rel="noopener noreferrer">Machine Behaviour</a> - <em><strong>Nature</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=7881171273277686092&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://yzhu.io/publication/dark2020engineering/paper.pdf" rel="noopener noreferrer">Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense</a> - <em><strong>Engineering</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=12292747257300299161&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Yixin Zhu and Song-Chun Zhu's review on visual commonsense.</p>
</li>
<li><p><a href="https://cims.nyu.edu/~brenden/papers/OrhanEtAl2020NeurIPS.pdf" rel="noopener noreferrer">Self-supervised Learning Through the eyes of a Child</a> - <em><strong>NeurIPS'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=5608715260418451299&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Concept learning through near-natural co-occurrence frequency estimation.</p>
</li>
<li><p><a href="https://arxiv.org/abs/1910.01442" rel="noopener noreferrer">CLEVRER: CoLlision Events for Video REpresentation and Reasoning</a> - <em><strong>ICLR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=4352064462350202338&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2020/hash/bf15e9bbff22c7719020f9df4badc20a-Abstract.html" rel="noopener noreferrer">BONGARD-LOGO: A New Benchmark for Human-Level Concept Learning and Reasoning</a> - <em><strong>NeurIPS'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=9164011458889391917&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://dl.acm.org/doi/10.1145/1143844.1143874" rel="noopener noreferrer">The relationship between Precision-Recall and ROC curves</a> - <em><strong>ICML'06</strong></em>, 2006. [<a href="https://scholar.google.com/scholar?cluster=10708180947310062390&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://export.arxiv.org/pdf/2009.08092" rel="noopener noreferrer">Distributional Generalization: A New Kind of Generalization</a> - 2020. [<a href="https://scholar.google.com/scholar?cluster=6190621467796247477&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/0010027793900584" rel="noopener noreferrer">Learning and development in networks: The importance of starting small.</a> - <em><strong>Cognition</strong></em>, 1993. [<a href="https://scholar.google.com/scholar?cluster=5133345254007462915&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on the idea of curriculum learning.</p>
</li>
<li><p><a href="https://www.sciencedirect.com/science/article/pii/S0010027799000311" rel="noopener noreferrer">Language acquisition in the absence of explicit negative evidence: how important is starting small?</a> - <em><strong>Cognition</strong></em>, 1999. [<a href="https://scholar.google.com/scholar?cluster=11813578367725362166&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://dl.acm.org/doi/pdf/10.1145/1553374.1553380" rel="noopener noreferrer">Curriculum Learning</a> - <em><strong>ICML'09</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=8740915934335425405&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper applying the idea of curriculum learning to machine learning.</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/6126279" rel="noopener noreferrer">Parsing video events with goal inference and intent prediction</a> - <em><strong>ICCV'11</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?cluster=5979196784405021658&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/6751387" rel="noopener noreferrer">Inferring "Dark Matter" and "Dark Energy" from Videos</a> - <em><strong>ICCV'13</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=3467068307444498624&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. The original paper on latent state discovery from videos.</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Explainable_and_Explicit_Visual_Reasoning_Over_Scene_Graphs_CVPR_2019_paper.pdf" rel="noopener noreferrer">Explainable and Explicit Visual Reasoning over Scene Graphs</a> - <em><strong>CVPR'19</strong></em>, 2019. [<a href="https://scholar.google.com/scholar?cluster=8517395712319798436&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2021/hash/4c26774d852f62440fc746ea4cdd57f6-Abstract.html" rel="noopener noreferrer">Attention over Learned Object Embeddings Enables Complex Visual Reasoning</a> - <em><strong>NeurIPS'21</strong></em>, 2021. [<a href="https://scholar.google.com/scholar?cluster=127829313460149801&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://papers.NeurIPS.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf" rel="noopener noreferrer">Distributed Representations of Words and Phrases and their Compositionality</a> - <em><strong>NeurIPS'13</strong></em>, 2013. [<a href="https://scholar.google.com/scholar?cluster=2410615501856807729&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9197172" rel="noopener noreferrer">Motion Reasoning for Goal-Based Imitation Learning</a> - <em><strong>ICRA'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=7519230802512388210&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Ji_Action_Genome_Actions_As_Compositions_of_Spatio-Temporal_Scene_Graphs_CVPR_2020_paper.pdf" rel="noopener noreferrer">Action Genome: Actions as Compositions of Spatio-temporal Scene Graphs</a> - <em><strong>CVPR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=388714326304810525&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper/2020/file/64dcf3c521a00dbb4d2a10a27a95a9d8-Paper.pdf" rel="noopener noreferrer">Refactoring Policy for Compositional Generalizability using Self-Supervised Object Proposals</a> - <em><strong>NeurIPS'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=2255457416066730255&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Materzynska_Something-Else_Compositional_Action_Recognition_With_Spatial-Temporal_Interaction_Networks_CVPR_2020_paper.pdf" rel="noopener noreferrer">Something-Else: Compositional Action Recognition with Spatial-Temporal Interaction Networks</a> - <em><strong>CVPR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=17469863154797360929&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Putting_Visual_Object_Recognition_in_Context_CVPR_2020_paper.pdf" rel="noopener noreferrer">Putting visual object recognition in context</a> - <em><strong>CVPR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=6207193649298787857&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.13884" rel="noopener noreferrer">Multimodal Few-Shot Learning with Frozen Language Models</a> - 2021. [<a href="https://scholar.google.com/scholar?cluster=16154696122208258147&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5206772" rel="noopener noreferrer">Describing Objects by their Attributes</a> - <em><strong>CVPR'09</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=6853730684095116174&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.07783" rel="noopener noreferrer">Panoramic Learning with A Standardized Machine Learning Formalism</a> - 2021. [<a href="https://scholar.google.com/scholar?cluster=14222434793711614257&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://psycnet.apa.org/record/1996-10319-001" rel="noopener noreferrer">Graininess of judgment under uncertainty: An accuracy-informativeness trade-off</a> - <em><strong>Journal of Experimental Psychology</strong></em>, 1995. [<a href="https://scholar.google.com/scholar?cluster=15366302654259490472&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://openreview.net/forum?id=GFsU8a0sGB" rel="noopener noreferrer">Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms</a> - <em><strong>ICLR'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=2486025806014234529&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.biorxiv.org/content/10.1101/2022.01.29.478330v2.abstract" rel="noopener noreferrer">Interplay between rule learning and rule switching in a perceptual categorization task</a> - 2022. [<a href="https://scholar.google.com/scholar?cluster=7461559646167397406&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>


<h2 id="academic-tools"><a class="anchor" aria-hidden="true" tabindex="-1" href="#academic-tools"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Academic Tools</h2><h3 id="courses"><a class="anchor" aria-hidden="true" tabindex="-1" href="#courses"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Courses</h3><ul>
<li><p><a href="https://cbmm.mit.edu/education/courses/computational-cognitive-science" rel="noopener noreferrer">Computational Cognitive Science Courses</a> - <em><strong>MIT</strong></em>. Courses on computational cognitive science from MIT, Harvard, and Stanford.</p>
</li>
<li><p><a href="https://people.csail.mit.edu/asolar/SynthesisCourse/index.htm" rel="noopener noreferrer">Introduction to Program Synthesis</a> - <em><strong>MIT</strong></em>. Armando Solar-Lezama's elementary course on program synthesis.</p>
</li>
<li><p><a href="https://web.mit.edu/6.001/6.037/" rel="noopener noreferrer">Structure and Interpretation of Computer Programs</a> - <em><strong>MIT</strong></em>. [<a href="https://web.mit.edu/6.001/6.037/sicp.pdf" rel="noopener noreferrer">Book: SICP</a>]. [<a href="https://scholar.google.com/scholar?cluster=7488066943428166450&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Classic course on applying structural, procedural, and meta-linguistic abstraction to solve computational problems.</p>
</li>
<li><p><a href="https://faculty.ksu.edu.sa/sites/default/files/rosen_discrete_mathematics_and_its_applications_7th_edition.pdf" rel="noopener noreferrer">Discrete Mathematics and Its Applications</a>. Classic course on basic discrete mathematics, including matheatical logic, set theory, graph theory, formal language (and automata), basic number theory (e.g., counting), and other related topics.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="programming"><a class="anchor" aria-hidden="true" tabindex="-1" href="#programming"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Programming</h3><ul>
<li><a href="https://probmods.org/" rel="noopener noreferrer">Probabilistic Models of Cognition</a> - <em><strong>MIT</strong></em>. The probabilistic approach to cognitive science, which models learning and reasoning as inference in complex probabilistic models.</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="paper-writing"><a class="anchor" aria-hidden="true" tabindex="-1" href="#paper-writing"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Paper Writing</h3><ul>
<li><p><a href="https://github.com/SHI-Yu-Zhe/awesome-agi-cocosci/blob/master/README.md/LaTex/config.sty" rel="noopener noreferrer">LaTex Configuration</a> - <em><strong>LaTex</strong></em>. LaTex template for configuration file with elegant reference style (gray-colored reference, page backward reference).</p>
</li>
<li><p><a href="https://github.com/SHI-Yu-Zhe/awesome-agi-cocosci/blob/master/README.md/BibTex/references_header.bib" rel="noopener noreferrer">BibTex Template</a> - <em><strong>BibTex</strong></em>. BibTex template for including abbreviations of journals and conferences in AI, Mathematics, and Cognitive Sciences.</p>
</li>
<li><p><a href="https://www.biorender.com/" rel="noopener noreferrer">bioRender</a> - <em><strong>bioRender</strong></em>. Create professional science figures in minutes by browsing thousands of pre-made icons and templates from more than 30 fields of life sciences.</p>
</li>
<li><p><a href="https://www.nature.com/documents/nature-summary-paragraph.pdf" rel="noopener noreferrer">How to construct a Nature summary paragraph</a> - <em><strong>Nature</strong></em>. Nature official guidelines for composing abstracts.</p>
</li>
<li><p><a href="https://www.nature.com/articles/d41586-020-03422-x" rel="noopener noreferrer">How to write a superb literature review</a> - <em><strong>Nature</strong></em>, 2020. Nature speaks to old hands and first timers about the work they did to make their reviews sing.</p>
</li>
<li><p><a href="https://www.nature.com/scitable/topicpage/scientific-papers-13815490/" rel="noopener noreferrer">Scientific Papers</a> - <em><strong>Nature</strong></em>. Nature guidance on writing scientific papers.</p>
</li>
<li><p><a href="https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf" rel="noopener noreferrer">The Machine Learning Reproducibility Checklist</a> - <em><strong>McGill University</strong></em>. Guidelines for introducing a machine learning algorithm with guarantee of reproducibility.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="paper-reading"><a class="anchor" aria-hidden="true" tabindex="-1" href="#paper-reading"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Paper Reading</h3><ul>
<li><p><a href="https://www.cs.uni-potsdam.de/bs/teaching/docs/courses/ss2020/scn/material/p83-keshavA.pdf" rel="noopener noreferrer">How to Read a Paper</a> - <em><strong>ACM SIGCOMM Computer Communication Review</strong></em>, 2007. [<a href="https://scholar.google.com/scholar?cluster=7234542241721187587&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A comprehensive tutorial on reading scientific papers.</p>
</li>
<li><p><a href="https://www.science.org/content/article/how-seriously-read-scientific-paper" rel="noopener noreferrer">How to (seriously) read a scientific paper</a> - <em><strong>Science</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=How+to+%28seriously%29+read+a+scientific+paper&amp;btnG=" rel="noopener noreferrer">All Versions</a>]. Science interview on reading scientific papers.</p>
</li>
<li><p><a href="https://www.nature.com/articles/nature.2017.21751" rel="noopener noreferrer">It's not just you: science papers are getting harder to read</a> - <em><strong>Nature</strong></em>, 2017. [<a href="https://scholar.google.com/scholar?cluster=4409814498614719804&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Nature perspective on reading scientific papers.</p>
</li>
<li><p><a href="https://be.mit.edu/sites/default/files/documents/HowToReadAScientificPaper.pdf" rel="noopener noreferrer">How to navigate a scientific paper with time constraints: a graphics approach</a> - <em><strong>MIT</strong></em>. MIT guidance on strategies for reading papers given different time constraints.</p>
</li>
<li><p><a href="https://textvis.lnu.se/" rel="noopener noreferrer">Text Visualization Browser</a> - <em><strong>ISOVIS group</strong></em>, 2015. [<a href="https://cs.lnu.se/isovis/pubs/docs/kucher-pacificvis15-postprint.pdf" rel="noopener noreferrer">Paper</a>]. [<a href="https://scholar.google.com/scholar?cluster=7000995325728444282&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A Hub of Text Visualization Techniques.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="literature-management"><a class="anchor" aria-hidden="true" tabindex="-1" href="#literature-management"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Literature Management</h3><ul>
<li><p><a href="https://www.science.org/content/article/how-keep-scientific-literature" rel="noopener noreferrer">How to keep up with the scientific literature</a> - <em><strong>Science</strong></em>, 2016. Science interview on organizing scientific papers.</p>
</li>
<li><p><a href="https://www.nature.com/articles/nj7612-457a" rel="noopener noreferrer">Scientific literature: Information overload</a> - <em><strong>Nature</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=9898832432826237365&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Perspective on handling overloaded information from scientific literature.</p>
</li>
<li><p><a href="https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/" rel="noopener noreferrer">Microsoft Academic Graph</a> - <em><strong>Microsoft Research</strong></em>. Heterogeneous graph containing scientific publication records, citation relationships between those publications, as well as authors, institutions, journals, conferences, and fields of study.</p>
</li>
<li><p><a href="http://sonyis.me/paperpdf/Microsoft%20Academic%20Graph%20WWW%202015.pdf" rel="noopener noreferrer">An Overview of Microsoft Academic Service (MAS) and Applications</a> - <em><strong>WWW'15</strong></em>, 2015. [<a href="https://scholar.google.com/scholar?cluster=9075899176667058496&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versios</a>]. Original paper on Microsoft Academic Graph.</p>
</li>
<li><p><a href="https://blogs.lse.ac.uk/impactofsocialsciences/2021/05/27/goodbye-microsoft-academic-hello-open-research-infrastructure/" rel="noopener noreferrer">Goodbye, Microsoft Academic – Hello, open research infrastructure?</a> - <em><strong>LSE Impact Blog</strong></em>, 2021. An interpretation of Microsoft's strategy on research infrastructure.</p>
</li>
<li><p><a href="https://www.semanticscholar.org/" rel="noopener noreferrer">Semantic Scholar</a> - <em><strong>Allen Institute for AI Research</strong></em>. AI-powered scientific literature research tool.</p>
</li>
<li><p><a href="https://aclanthology.org/N18-3011/" rel="noopener noreferrer">Construction of the Literature Graph in Semantic Scholar</a> - <em><strong>NAACL'18</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=5500969515339734950&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. Semantic Scholar with extracting feature and metadata from raw paper data.</p>
</li>
<li><p><a href="https://aclanthology.org/2020.acl-main.447/" rel="noopener noreferrer">S2ORC: The Semantic Scholar Open Research Corpus</a> - <em><strong>ACL'20</strong></em>, 2020. [<a href="https://scholar.google.com/scholar?cluster=11978464475399626925&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. An open corpus of academic papers released by Semantic Scholar.</p>
</li>
<li><p><a href="https://www.litmaps.com/" rel="noopener noreferrer">Litmaps</a> - <em><strong>Litmap Ltd</strong></em>. For interactive literature map construction and linked document management.</p>
</li>
<li><p><a href="https://www.vosviewer.com/" rel="noopener noreferrer">VOSviewer</a> - <em><strong>Leiden University</strong></em>. For constructing and visualizing bibliometric networks.</p>
</li>
<li><p><a href="https://www.stateoftheart.ai/" rel="noopener noreferrer">StateOfTheArt.AI</a> - <em><strong>StateOfTheArtAI</strong></em>. For tracking, collecting and visualizing the development of AI research.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="knowledge-management"><a class="anchor" aria-hidden="true" tabindex="-1" href="#knowledge-management"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Knowledge Management</h3><ul>
<li><p><a href="https://www.loc.gov/aba/publications/FreeLCC/freelcc.html" rel="noopener noreferrer">Library of Congress Classification</a> - <em><strong>Library of Congress</strong></em>. Classification system of USA (PDF only).</p>
</li>
<li><p><a href="http://cct.nlc.cn/" rel="noopener noreferrer">Chinese Library Classification</a> - <em><strong>National Library of China</strong></em>. Classification system of P. R. China (online user interface in Chinese). [<a href="https://www.isko.org/cyclo/clc" rel="noopener noreferrer">English introduction at ISKO</a>]. [<a href="https://en.wikipedia.org/wiki/Chinese_Library_Classification" rel="noopener noreferrer">Wikipedia-EN</a>].</p>
</li>
<li><p><a href="https://rvk.uni-regensburg.de/regensburger-verbundklassifikation-online" rel="noopener noreferrer">DDC at German National Library</a> - <em><strong>Deutsche National Bibliothek</strong></em>. Deway Decimal Classification (DDC) based classification system of Germany (online user interface). [<a href="https://www.dnb.de/EN/Professionell/DDC-Deutsch/DDCinDNB/ddcindnb_node.html" rel="noopener noreferrer">DNB Website</a>].</p>
</li>
<li><p><a href="https://www.ndl.go.jp/jp/data/catstandards/classification_subject/ndlc.html" rel="noopener noreferrer">National Dite Library Classification</a> - <em><strong>National Diet Library of Japan</strong></em>. Classification system of Japan (PDF only).</p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/List_of_Dewey_Decimal_classes" rel="noopener noreferrer">DDC at OCLC (Wikipedia)</a> - <em><strong>Online Computer Library Center (OCLC)</strong></em>. [<a href="https://www.oclc.org/en/home.html" rel="noopener noreferrer">OCLC Website</a>]. [<a href="https://www.oclc.org/content/dam/oclc/dewey/versions/print/intro.pdf" rel="noopener noreferrer">Introduction to DDC</a>]. [<a href="https://www.oclc.org/content/dam/oclc/webdewey/help/full_manual.pdf" rel="noopener noreferrer">DDC Manual</a>]. Dewey Decimal Classification (DDC) system for worldwide library resouce construction. [<a href="https://www.oclc.org/content/dam/oclc/webdewey/help/000.pdf" rel="noopener noreferrer">DDC Class 000 (PDF only)</a>]. [<a href="https://www.oclc.org/content/dam/oclc/webdewey/help/100.pdf" rel="noopener noreferrer">DDC Class 100 (PDF only)</a>]. [<a href="https://www.oclc.org/content/dam/oclc/webdewey/help/200.pdf" rel="noopener noreferrer">DDC Class 200 (PDF only)</a>]. [<a href="https://www.oclc.org/content/dam/oclc/webdewey/help/300.pdf" rel="noopener noreferrer">DDC Class 300 (PDF only)</a>]. [<a href="https://www.oclc.org/content/dam/oclc/webdewey/help/400.pdf" rel="noopener noreferrer">DDC Class 400 (PDF only)</a>]. [<a href="https://www.oclc.org/content/dam/oclc/webdewey/help/500.pdf" rel="noopener noreferrer">DDC Class 500 (PDF only)</a>]. [<a href="https://www.oclc.org/content/dam/oclc/webdewey/help/600.pdf" rel="noopener noreferrer">DDC Class 600 (PDF only)</a>]. [<a href="https://www.oclc.org/content/dam/oclc/webdewey/help/700.pdf" rel="noopener noreferrer">DDC Class 700 (PDF only)</a>]. [<a href="https://www.oclc.org/content/dam/oclc/webdewey/help/800.pdf" rel="noopener noreferrer">DDC Class 800 (PDF only)</a>]. [<a href="https://www.oclc.org/content/dam/oclc/webdewey/help/900.pdf" rel="noopener noreferrer">DDC Class 900 (PDF only)</a>].</p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/Knowledge_organization" rel="noopener noreferrer">Knowledge organization</a> - <em><strong>Wikipedia</strong></em>. Wikipedia on knowledge organization methods.</p>
</li>
<li><p><a href="https://zettelkasten.de/" rel="noopener noreferrer">The Zettelkasten Method</a> - <em><strong>Bielefeld University</strong></em>. Relating ideas in graphs and multi-labels.</p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/Zettelkasten" rel="noopener noreferrer">Zettelkasten</a> - <em><strong>Wikipedia</strong></em>. Wikipedia on the Zettelkasten method.</p>
</li>
<li><p><a href="https://roamresearch.com/" rel="noopener noreferrer">Roam Research</a> - <em><strong>Roam Research</strong></em>. For linked document management, visualization, and sharing.</p>
</li>
<li><p><a href="https://foambubble.github.io/foam/" rel="noopener noreferrer">Foam</a> - <em><strong>Foambubble</strong></em>. For linked document management, visualization, and sharing, opensourced softward built on VSCode.</p>
</li>
<li><p><a href="https://www.buildingasecondbrain.com/" rel="noopener noreferrer">Building a Second Brain</a> - <em><strong>Forte Labs, LLC</strong></em>. Connecting ideas in graphs.</p>
</li>
<li><p><a href="https://www.zotero.org/" rel="noopener noreferrer">Zotero</a> - <em><strong>Digital Scholar</strong></em>. For reference management to manage bibliographic data and research related materials.</p>
</li>
<li><p><a href="https://pdfs.semanticscholar.org/88f8/fa9dfbc0c2b296758dd932b871917c5c775a.pdf%C2%A0" rel="noopener noreferrer">Niklas Luhmann's Card Index: Thinking Tool, Communication Partner, Publication Machine</a> - <em><strong>Forgetting Machines: Knowledge Management Evolution in Early Modern Europe, Brill</strong></em>, 2016. [<a href="https://scholar.google.com/scholar?cluster=1786807670077004336&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://culturemachine.net/wp-content/uploads/2019/01/373-604-1-PB.pdf" rel="noopener noreferrer">The card index as creativity machine</a> - <em><strong>Culture Machine</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?cluster=9767873312286889264&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://www.researchgate.net/profile/Alberto-Cevolini/publication/328624186_Where_Does_Niklas_Luhmann%27s_Card_Index_Come_From/links/609f818e299bf147699a401d/Where-Does-Niklas-Luhmanns-Card-Index-Come-From.pdf" rel="noopener noreferrer">Where Does Niklas Luhmann's Card Index Come From?</a> - <em><strong>Erudition and the Republic of Letters</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=8279465066043884141&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>]. A simplified introduction on Luhmann's Zettelkasten.</p>
</li>
<li><p><a href="https://www.uni-bielefeld.de/fakultaeten/soziologie/forschung/luhmann-archiv/pdf/jschmidt_niklas-luhmanns-card-index_-sociologica_2018_12-1.pdf" rel="noopener noreferrer">Niklas Luhmann's Card Index: The Fabrication of Serendipity</a> - <em><strong>Sociologica</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=12440286698665929622&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://luhmann.surge.sh/communicating-with-slip-boxes" rel="noopener noreferrer">Communicating with Slip Boxes</a> - 2019. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Communicating+with+slip+boxes+luhmann&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h2 id="institute--researcher"><a class="anchor" aria-hidden="true" tabindex="-1" href="#institute--researcher"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Institute &amp; Researcher</h2><h3 id="mit"><a class="anchor" aria-hidden="true" tabindex="-1" href="#mit"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MIT</h3><ul>
<li><p><a href="https://cbmm.mit.edu/" rel="noopener noreferrer">Center for Brains, Minds and Machines (CBMM)</a> - <em><strong>MIT</strong></em>.</p>
</li>
<li><p><a href="https://cocosci.mit.edu/josh" rel="noopener noreferrer">Josh Tenenbaum</a> - <em><strong>Department of Brain and Cognitive Sciences, CSAIL, MIT</strong></em>, <a href="https://cocosci.mit.edu/" rel="noopener noreferrer">Computational Cognitive Science Group (CoCoSci Group)</a> - <em><strong>MIT</strong></em>.</p>
</li>
<li><p><a href="https://saxelab.mit.edu/people/rebecca-saxe" rel="noopener noreferrer">Rebecca Saxe</a> - <em><strong>Department of Brain and Cognitive Sciences, MIT</strong></em>, <a href="https://saxelab.mit.edu/" rel="noopener noreferrer">Social Cognitive Neuroscience Laboratory (SaxeLab)</a> - <em><strong>MIT</strong></em>.</p>
</li>
<li><p><a href="https://cbmm.mit.edu/about/people/schulz" rel="noopener noreferrer">Laura Schulz</a> - <em><strong>Department of Brain and Cognitive Sciences, MIT</strong></em>, <a href="https://eccl.mit.edu/" rel="noopener noreferrer">Early Childhood Cognition Lab</a> - <em><strong>MIT</strong></em>.</p>
</li>
<li><p><a href="https://people.csail.mit.edu/lpk/" rel="noopener noreferrer">Leslie Kaelbling</a> - <em><strong>Department of Electrical Engineering and Computer Science, CSAIL, MIT</strong></em>, <a href="https://lis.csail.mit.edu/" rel="noopener noreferrer">The Learning &amp; Intelligent Systems Group</a> - <em><strong>MIT</strong></em>.</p>
</li>
<li><p><a href="https://people.csail.mit.edu/asolar/" rel="noopener noreferrer">Armando Solar-Lezama</a> - <em><strong>Department of Electrical Engineering and Computer Science, CSAIL, MIT</strong></em>, <a href="http://groups.csail.mit.edu/cap/" rel="noopener noreferrer">Computer-Aided Programming Group</a> - <em><strong>MIT</strong></em>.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="stanford"><a class="anchor" aria-hidden="true" tabindex="-1" href="#stanford"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Stanford</h3><ul>
<li><p><a href="https://profiles.stanford.edu/fei-fei-li" rel="noopener noreferrer">Li Fei-Fei</a> - <em><strong>Computer Science Department, Human-Centered AI Institute, Stanford</strong></em>, <a href="https://svl.stanford.edu/" rel="noopener noreferrer">Stanford Vision and Learning Lab</a> - <em><strong>Stanford</strong></em>.</p>
</li>
<li><p><a href="https://cocolab.stanford.edu/ndg.html" rel="noopener noreferrer">Noah Goodman</a> - <em><strong>Department of Psychology, Computer Science Department, Stanford</strong></em>, <a href="https://cocolab.stanford.edu/" rel="noopener noreferrer">Computation &amp; Cognition Lab (CoCoLab)</a> - <em><strong>Stanford</strong></em>.</p>
</li>
<li><p><a href="https://web.stanford.edu/~mcfrank/" rel="noopener noreferrer">Michael Frank</a> - <em><strong>Department of Psychology, Stanford</strong></em>, <a href="http://langcog.stanford.edu/" rel="noopener noreferrer">The Stanford Language and Cognition Lab</a> - <em><strong>Stanford</strong></em>.</p>
</li>
<li><p><a href="https://cicl.stanford.edu/member/tobias_gerstenberg/" rel="noopener noreferrer">Tobias Gerstenberg</a> - <em><strong>Department of Psychology, Stanford</strong></em>, <a href="https://cicl.stanford.edu/" rel="noopener noreferrer">Causality in Cognition Lab (CICL)</a> - <em><strong>Stanford</strong></em>.</p>
</li>
<li><p><a href="http://ai.stanford.edu/~cbfinn/" rel="noopener noreferrer">Chelsea Finn</a> - <em><strong>Computer Science Department, Stanford</strong></em>, <a href="https://irislab.stanford.edu/" rel="noopener noreferrer">Intelligence through Robotic Interaction at Scale (IRIS Group)</a> - <em><strong>Stanford</strong></em>.</p>
</li>
<li><p><a href="https://comm.stanford.edu/faculty-bailenson/" rel="noopener noreferrer">Jeremy Bailenson</a> - <em><strong>Department of Communication, Stanford</strong></em>, <a href="https://stanfordvr.com/" rel="noopener noreferrer">Virtual Human Interaction Lab (VHIL)</a> - <em><strong>Stanford</strong></em>.</p>
</li>
<li><p><a href="https://jiajunwu.com/" rel="noopener noreferrer">Jiajun Wu</a> - <em><strong>Computer Science Department, Stanford</strong></em>.</p>
</li>
<li><p><a href="https://profiles.stanford.edu/judith-fan" rel="noopener noreferrer">Judith Fan</a> - <em><strong>Department of Psychology, Stanford</strong></em>, <a href="https://cogtoolslab.github.io/" rel="noopener noreferrer">Cognitive Tools Lab</a> - <em><strong>Stanford</strong></em>.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="princeton"><a class="anchor" aria-hidden="true" tabindex="-1" href="#princeton"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Princeton</h3><ul>
<li><p><a href="https://psych.princeton.edu/person/tania-lombrozo" rel="noopener noreferrer">Tania Lombrozo</a> - <em><strong>Department of Psychology, Princeton</strong></em>, <a href="https://cognition.princeton.edu/" rel="noopener noreferrer">Concepts &amp; Cognition Lab</a> - <em><strong>Princeton</strong></em>.</p>
</li>
<li><p><a href="https://cocosci.princeton.edu/tom/index.php" rel="noopener noreferrer">Thomas Griffiths</a> - <em><strong>Department of Psychology, Department of Computer Science, Princeton</strong></em>, <a href="https://cocosci.princeton.edu/index.php" rel="noopener noreferrer">Computational Cognitive Science Lab</a> - <em><strong>Princeton</strong></em>.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="harvard"><a class="anchor" aria-hidden="true" tabindex="-1" href="#harvard"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Harvard</h3><ul>
<li><p><a href="https://psychology.fas.harvard.edu/people/elizabeth-s-spelke" rel="noopener noreferrer">Elizabeth Spelke</a> - <em><strong>Department of Psychology, Harvard</strong></em>, <a href="https://www.harvardlds.org/" rel="noopener noreferrer">Harvard Laboratory for Developmental Studies</a> - <em><strong>Harvard</strong></em>.</p>
</li>
<li><p><a href="https://www.tomerullman.org/" rel="noopener noreferrer">Tomer Ullman</a> - <em><strong>Department of Psychology, Harvard</strong></em>, <a href="https://cocodev.fas.harvard.edu/" rel="noopener noreferrer">Computation, Cognition, and Development Lab (CoCoDev)</a> - <em><strong>Harvard</strong></em>.</p>
</li>
<li><p><a href="https://psychology.fas.harvard.edu/people/samuel-j-gershman" rel="noopener noreferrer">Samuel Gershman</a> - <em><strong>Department of Psychology, Harvard</strong></em>, <a href="https://gershmanlab.com/" rel="noopener noreferrer">Computational Cognitive Neuroscience Lab (CCN Lab)</a> - <em><strong>Harvard</strong></em>.</p>
</li>
<li><p><a href="https://psychology.fas.harvard.edu/people/fiery-cushman" rel="noopener noreferrer">Fiery Cushman</a> - <em><strong>Department of Psychology, Harvard</strong></em>, <a href="https://cushmanlab.fas.harvard.edu/" rel="noopener noreferrer">Moral Psychology Research Lab</a> - <em><strong>Harvard</strong></em>.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="ucla"><a class="anchor" aria-hidden="true" tabindex="-1" href="#ucla"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>UCLA</h3><ul>
<li><p><a href="http://vcla.stat.ucla.edu/" rel="noopener noreferrer">Center for Vision, Cognition, Learning and Autonomy (VCLA)</a> - <em><strong>Department of Statistics, UCLA</strong></em>.</p>
</li>
<li><p><a href="http://www.stat.ucla.edu/~ywu/" rel="noopener noreferrer">Ying Nian Wu</a> - <em><strong>Department of Statistics, UCLA</strong></em>.</p>
</li>
<li><p><a href="http://www.stat.ucla.edu/~taogao/Taogao.html" rel="noopener noreferrer">Tao Gao</a> - <em><strong>Department of Statistics, Department of Psychology, UCLA</strong></em>, <a href="http://www.stat.ucla.edu/~taogao/index.html" rel="noopener noreferrer">Visual Intelligence Lab</a> - <em><strong>UCLA</strong></em>.</p>
</li>
<li><p><a href="https://www.psych.ucla.edu/faculty/page/hongjing" rel="noopener noreferrer">Hongjing Lu</a> - <em><strong>Department of Psychology, Department of Statistics, UCLA</strong></em>, <a href="http://cvl.psych.ucla.edu/" rel="noopener noreferrer">Computational Vision and Learning Lab (CVL)</a> - <em><strong>UCLA</strong></em>.</p>
</li>
<li><p><a href="http://web.cs.ucla.edu/~guyvdb/" rel="noopener noreferrer">Guy Van den Broeck</a> - <em><strong>Department of Computer Science, UCLA</strong></em>, <a href="http://starai.cs.ucla.edu/#" rel="noopener noreferrer">StarAI Lab</a> - <em><strong>UCLA</strong></em>.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="uc-berkeley"><a class="anchor" aria-hidden="true" tabindex="-1" href="#uc-berkeley"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>UC Berkeley</h3><ul>
<li><p><a href="https://people.eecs.berkeley.edu/~anca/index.html" rel="noopener noreferrer">Anca Dragan</a> - <em><strong>Department of Electrical Engineering and Computer Science, UC Berkeley</strong></em>, <a href="http://interact.berkeley.edu/" rel="noopener noreferrer">Interactive Autonomy and Collaborative Technologies Laboratory (InterACT)</a> - <em><strong>UC Berkeley</strong></em>.</p>
</li>
<li><p><a href="https://psychology.berkeley.edu/people/fei-xu" rel="noopener noreferrer">Fei Xu</a> - <em><strong>Department of Psychology, UC Berkeley</strong></em>, <a href="https://babylab5.wixsite.com/bell" rel="noopener noreferrer">Berkeley Early Learning Lab (Xu Lab)</a> - <em><strong>UC Berkeley</strong></em>.</p>
</li>
<li><p><a href="http://alisongopnik.com/" rel="noopener noreferrer">Alison Gopnik</a> - <em><strong>Department of Psychology, UC Berkeley</strong></em>, <a href="http://www.gopniklab.berkeley.edu/" rel="noopener noreferrer">Cognitive Development &amp; Learning Lab (Gopnik Lab)</a> - <em><strong>UC Berkeley</strong></em>.</p>
</li>
<li><p><a href="http://colala.berkeley.edu/people/piantadosi/" rel="noopener noreferrer">Steve Piantadosi</a> - <em><strong>Department of Psychology, UC Berkeley</strong></em>, <a href="http://colala.berkeley.edu/" rel="noopener noreferrer">The computation and language lab (colala)</a> - <em><strong>UC Berkeley</strong></em>.</p>
</li>
<li><p><a href="http://www.celestekidd.com/" rel="noopener noreferrer">Celeste Kidd</a> - <em><strong>Department of Psychology, UC Berkeley</strong></em>, <a href="https://www.kiddlab.com/" rel="noopener noreferrer">Kidd Lab</a> - <em><strong>UC Berkeley</strong></em>.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="bnu"><a class="anchor" aria-hidden="true" tabindex="-1" href="#bnu"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>BNU</h3><ul>
<li><a href="https://brain.bnu.edu.cn/English/Faculty/CurrentFaculty/Bzz/a552402e529a4f27b979378abd42c10e.htm" rel="noopener noreferrer">Yanchao Bi</a> - <em><strong>IDG/McGovern Institute for Brain Research and the State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University (BNU)</strong></em>, <a href="http://bilab.bnu.edu.cn/" rel="noopener noreferrer">Yanchao Bi's Concept Lab (Bi Lab)</a> - <em><strong>BNU</strong></em>.</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="pku"><a class="anchor" aria-hidden="true" tabindex="-1" href="#pku"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>PKU</h3><ul>
<li><p><a href="https://zhusongchun.net/" rel="noopener noreferrer">Song-Chun Zhu</a> - <em><strong>School of AI and Institute for AI, Peking University (PKU)</strong></em>.</p>
</li>
<li><p><a href="https://yzhu.io/" rel="noopener noreferrer">Yixin Zhu</a> - <em><strong>School of AI and Institute for AI, Peking University (PKU)</strong></em>, <a href="https://pku.ai/" rel="noopener noreferrer">Cognitive Reasoning Lab (CoRe Lab)</a> - <em><strong>PKU</strong></em>.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="ucsd"><a class="anchor" aria-hidden="true" tabindex="-1" href="#ucsd"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>UCSD</h3><ul>
<li><p><a href="https://pages.ucsd.edu/~ztu/" rel="noopener noreferrer">Zhuowen Tu</a> - <em><strong>Department of Computer Science, UCSD</strong></em>, <a href="https://pages.ucsd.edu/~ztu/Group.htm" rel="noopener noreferrer">Machine Learning, Perception, and Cognition Lab (mlPC)</a> - <em><strong>UCSD</strong></em>.</p>
</li>
<li><p><a href="https://psychology.ucsd.edu/people/profiles/evul.html" rel="noopener noreferrer">Ed Vul</a> - <em><strong>Department of Psychology, UCSD</strong></em>, <a href="http://www.evullab.org/index.html" rel="noopener noreferrer">Computational Cognition Lab</a> - <em><strong>UCSD</strong></em>.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="nyu"><a class="anchor" aria-hidden="true" tabindex="-1" href="#nyu"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>NYU</h3><ul>
<li><p><a href="https://cs.nyu.edu/~davise/" rel="noopener noreferrer">Ernest Davis</a> - <em><strong>Department of Computer Science, Courant Institute of Mathematical Sciences, NYU</strong></em>.</p>
</li>
<li><p><a href="http://garymarcus.com/index.html" rel="noopener noreferrer">Gary Marcus</a> - <em><strong>Department of Psychology, NYU</strong></em>.</p>
</li>
<li><p><a href="https://cims.nyu.edu/~brenden/" rel="noopener noreferrer">Brenden Lake</a> - <em><strong>Department of Psychology, NYU</strong></em>, <a href="https://lake-lab.github.io/" rel="noopener noreferrer">Human &amp; Machine Learning Lab (Lake Lab)</a> - <em><strong>NYU</strong></em>.</p>
</li>
<li><p><a href="https://as.nyu.edu/faculty/todd-gureckis.html" rel="noopener noreferrer">Todd Gureckis</a> - <em><strong>Department of Psychology, NYU</strong></em>, <a href="http://gureckislab.org/" rel="noopener noreferrer">Computation &amp; Cognition Lab</a> - <em><strong>NYU</strong></em>.</p>
</li>
<li><p><a href="http://www.cns.nyu.edu/malab/people.html" rel="noopener noreferrer">Wei Ji Ma</a> - <em><strong>Department of Psychology, Center for Neural Science, NYU</strong></em>, <a href="http://www.cns.nyu.edu/malab/" rel="noopener noreferrer">Wei Ji Ma Lab</a> - <em><strong>NYU</strong></em>.</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="jhu"><a class="anchor" aria-hidden="true" tabindex="-1" href="#jhu"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>JHU</h3><ul>
<li><a href="https://perception.jhu.edu/chaz/" rel="noopener noreferrer">Chaz Firestone</a> - <em><strong>Department of Psychological and Brain Sciences, Johns Hopkins University (JHU)</strong></em>, <a href="https://perception.jhu.edu/" rel="noopener noreferrer">Hopkins Perception &amp; Mind Lab</a> - <em><strong>JHU</strong></em>.</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="sit"><a class="anchor" aria-hidden="true" tabindex="-1" href="#sit"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SIT</h3><ul>
<li><a href="https://markkho.github.io/" rel="noopener noreferrer">Mark Ho</a> - <em><strong>Department of Computer Science, Stevens Institute of Technology (SIT)</strong></em>, <a href="https://codec-lab.github.io/" rel="noopener noreferrer">Computation and Decision-Making Lab</a> - <em><strong>SIT</strong></em>.</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h2 id="people--book"><a class="anchor" aria-hidden="true" tabindex="-1" href="#people--book"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>People &amp; Book</h2><h3 id="john-hopcroft"><a class="anchor" aria-hidden="true" tabindex="-1" href="#john-hopcroft"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>John Hopcroft</h3><p>Theoretical computer scientist.</p>
<ul>
<li><p><a href="http://elib.vku.udn.vn/bitstream/123456789/2543/1/2007.%20Introduction%20to%20Automata%20Theory%2C%20Languages%2C%20and%20Computations%20%283rd%20edition%29.pdf" rel="noopener noreferrer">Introduction to Automata Theory, Languages, and Computation</a> - <em><strong>Pearson</strong></em>, 2007. [<a href="https://scholar.google.com/scholar?cluster=326269839585842480" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="http://www.cs.cornell.edu/jeh/book%20no%20so;utions%20March%202019.pdf" rel="noopener noreferrer">Foundations of Data Science</a> - <em><strong>Cambridge University Press</strong></em>. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=1802704438630899850" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="ulf-grenander"><a class="anchor" aria-hidden="true" tabindex="-1" href="#ulf-grenander"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Ulf Grenander</h3><p>Applied mathematician, the founder of General Pattern Theory.</p>
<ul>
<li><p><a href="https://www.dam.brown.edu/ptg/REPORTS/calculustext.PDF" rel="noopener noreferrer">A Calculus of Ideas: A Mathematical Study of Thinking</a> - <em><strong>World Scientific Publishing Company</strong></em>, 2012. [<a href="https://scholar.google.com/scholar?cluster=12182416000849265255&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://global.oup.com/academic/product/general-pattern-theory-9780198536710?cc=lt&amp;lang=de#" rel="noopener noreferrer">General Pattern Theory: A Mathematical Study of Regular Structures</a> - <em><strong>Oxford University Press</strong></em>, 1993. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=General+Pattern+Theory&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="david-marr"><a class="anchor" aria-hidden="true" tabindex="-1" href="#david-marr"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>David Marr</h3><p>Computational Cognitive Neuroscientist, the establisher of the Levels of Analysis.</p>
<ul>
<li><a href="https://usa1lib.org/book/1223444/8e5ca8" rel="noopener noreferrer">Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</a> - <em><strong>MIT Press</strong></em>, 1982. [<a href="https://scholar.google.com/scholar?cluster=14386368570811483142&amp;hl=en&amp;as_sdt=0,44" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="michael-tomasello"><a class="anchor" aria-hidden="true" tabindex="-1" href="#michael-tomasello"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Michael Tomasello</h3><p>Cognitive scientist, set up the foundations of studying human communications.</p>
<ul>
<li><p><a href="https://1lib.net/book/541274/39859f" rel="noopener noreferrer">Origins of human communication</a> - <em><strong>MIT Press</strong></em>, 2010. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=2553369883266458474" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://hk1lib.org/book/541275/1452f8?id=541275&amp;secret=1452f8" rel="noopener noreferrer">The cultural origins of human cognition</a> - <em><strong>Havard University Press</strong></em>, 2000. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=5000469061641945144" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="judea-pearl"><a class="anchor" aria-hidden="true" tabindex="-1" href="#judea-pearl"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Judea Pearl</h3><p>Applied mathematician, proposed causal intervention on siamese bayesian networks.</p>
<ul>
<li><p><a href="http://bayes.cs.ucla.edu/WHY/" rel="noopener noreferrer">The Book of Why: The New Science of Cause and Effect</a> - <em><strong>Basic Books</strong></em>, 2018. [<a href="https://scholar.google.com/scholar?cluster=2505901292485349932&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://hk1lib.org/book/2780725/2ec8f1?id=2780725&amp;secret=2ec8f1" rel="noopener noreferrer">Causality: Models, Reasoning and Inference</a> - <em><strong>Cambridge University Press</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=10996260119229499611&amp;hl=en&amp;as_sdt=0,5&amp;as_vis=1" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="susan-carey"><a class="anchor" aria-hidden="true" tabindex="-1" href="#susan-carey"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Susan Carey</h3><p>Developmental psychologist, proposed <em>object</em> as a core knowledge of human intelligence.</p>
<ul>
<li><p><a href="https://hk1lib.org/book/844457/42178f?id=844457&amp;secret=42178f" rel="noopener noreferrer">The Origin of Concepts</a> - <em><strong>Oxford University Press</strong></em>, 2009. [<a href="https://scholar.google.com/scholar?cluster=11493102398422813821&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://hk1lib.org/book/3659332/11fa44" rel="noopener noreferrer">Conceptual Change in Childhood</a> - <em><strong>MIT Press</strong></em>, 1985. [<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=conceptual+change+in+childhood+susan+carey&amp;btnG=" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="daniel-kahneman"><a class="anchor" aria-hidden="true" tabindex="-1" href="#daniel-kahneman"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Daniel Kahneman</h3><p>Computational cognitive scientist and Economist, set up the foundations for Decision Theory.</p>
<ul>
<li><a href="https://hk1lib.org/book/2181569/f5e85a?id=2181569&amp;secret=f5e85a" rel="noopener noreferrer">Thinking, fast and slow</a> - <em><strong>Farrar Straus Giroux</strong></em>, 2011. [<a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cluster=3255681708785115121" rel="noopener noreferrer">All Versions</a>].</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h3 id="karl-popper"><a class="anchor" aria-hidden="true" tabindex="-1" href="#karl-popper"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Karl Popper</h3><p>Scientific philosophor, the founder of scientific verification theories.</p>
<ul>
<li><p><a href="https://hk1lib.org/book/511214/299596" rel="noopener noreferrer">The logic of scientific discovery</a> - <em><strong>Routledge</strong></em>, 2005. [<a href="https://scholar.google.com/scholar?cluster=5836864564733788424&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
<li><p><a href="https://hk1lib.org/book/2773070/c48f60" rel="noopener noreferrer">All Life is Problem Solving</a> - <em><strong>Routledge</strong></em>, 2001. [<a href="https://scholar.google.com/scholar?cluster=9799073870888093350&amp;hl=en&amp;as_sdt=0,5" rel="noopener noreferrer">All Versions</a>].</p>
</li>
</ul>
<p>*<a href="#c">Back to Top</a></p>
<h2 id="about"><a class="anchor" aria-hidden="true" tabindex="-1" href="#about"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>About</h2><p>The initiator of this repo has been struggling to taxonomize related topics, since there are so many perspectives to follow, such as task-oriented, technique-oriented, and metaphysics-oriented. Finally he decided to focus on the perspective of <em><strong>The Sciences of Intelligence</strong></em>---each topic describes a phenomenon of intelligence, or an intelligent behavior---they show the objectives of reverse-engineering human intelligence for computational methods. These topics are never restricted to specific technical methods or tasks, but are trying to organize the nature of intelligence---from both <em>the software perspective</em> and <em>the hardware perspective</em>.</p>
<p>Obviously, this reading list is far from covering the every aspect of AGI and CoCoSci. Since the list is a by-product of the literature reviews when the initiator is working on Abduction and Bayesian modeling, other topics are also collected with biases, more or less. Abduction may be the way humans explain the world with the known, and discover the unknown, requiring much more investigations into its computational basis, cognitive underpinnings, and applications to AI. Please feel free to reach out!</p>
<p>*<a href="#c">Back to Top</a></p>


    </main>
  </body>
</html>
