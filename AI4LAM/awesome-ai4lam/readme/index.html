<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <title>Awesome Ai4lam (AI4LAM/awesome-ai4lam) Overview - Track Awesome List</title>
    <meta property="og:url" content="https://www.trackawesomelist.com/AI4LAM/awesome-ai4lam/readme/" />
    <meta property="og:type" content="summary" />
    <meta property="og:title" content="Awesome Ai4lam Overview" />
    <meta property="og:description" content="A list of awesome AI in libraries, archives, and museum collections from around the world 🕶️" />
    <meta property="og:site_name" content="Track Awesome List" />
    <style>
      main {
        max-width: 1024px;
        margin: 0 auto;
        padding: 0 0.5em;
      }
      :root,[data-color-mode=light][data-light-theme=light],[data-color-mode=dark][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}[data-color-mode=light][data-light-theme=dark],[data-color-mode=dark][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}.markdown-body{word-wrap:break-word;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;font-size:16px;line-height:1.5}.markdown-body:before{content:"";display:table}.markdown-body:after{clear:both;content:"";display:table}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:var(--color-danger-fg)}.markdown-body .anchor{float:left;margin-left:-20px;padding-right:4px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body p,.markdown-body blockquote,.markdown-body ul,.markdown-body ol,.markdown-body dl,.markdown-body table,.markdown-body pre,.markdown-body details{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;background-color:var(--color-border-default);border:0;margin:24px 0;padding:0}.markdown-body blockquote{color:var(--color-fg-muted);border-left:.25em solid var(--color-border-default);padding:0 1em}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:var(--color-fg-default);vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 tt,.markdown-body h1 code,.markdown-body h2 tt,.markdown-body h2 code,.markdown-body h3 tt,.markdown-body h3 code,.markdown-body h4 tt,.markdown-body h4 code,.markdown-body h5 tt,.markdown-body h5 code,.markdown-body h6 tt,.markdown-body h6 code{font-size:inherit;padding:0 .2em}.markdown-body h1{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:2em}.markdown-body h2{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{color:var(--color-fg-muted);font-size:.85em}.markdown-body summary h1,.markdown-body summary h2,.markdown-body summary h3,.markdown-body summary h4,.markdown-body summary h5,.markdown-body summary h6{display:inline-block}.markdown-body summary h1 .anchor,.markdown-body summary h2 .anchor,.markdown-body summary h3 .anchor,.markdown-body summary h4 .anchor,.markdown-body summary h5 .anchor,.markdown-body summary h6 .anchor{margin-left:-40px}.markdown-body summary h1,.markdown-body summary h2{border-bottom:0;padding-bottom:0}.markdown-body ul,.markdown-body ol{padding-left:2em}.markdown-body ul.no-list,.markdown-body ol.no-list{padding:0;list-style-type:none}.markdown-body ol[type="1"]{list-style-type:decimal}.markdown-body ol[type=a]{list-style-type:lower-alpha}.markdown-body ol[type=i]{list-style-type:lower-roman}.markdown-body div>ol:not([type]){list-style-type:decimal}.markdown-body ul ul,.markdown-body ul ol,.markdown-body ol ol,.markdown-body ol ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{margin-top:16px;padding:0;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{margin-bottom:16px;padding:0 16px}.markdown-body table{width:100%;width:-webkit-max-content;width:-webkit-max-content;width:max-content;max-width:100%;display:block;overflow:auto}.markdown-body table th{font-weight:600}.markdown-body table th,.markdown-body table td{border:1px solid var(--color-border-default);padding:6px 13px}.markdown-body table tr{background-color:var(--color-canvas-default);border-top:1px solid var(--color-border-muted)}.markdown-body table tr:nth-child(2n){background-color:var(--color-canvas-subtle)}.markdown-body table img{background-color:rgba(0,0,0,0)}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:var(--color-canvas-default)}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:rgba(0,0,0,0)}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{float:left;width:auto;border:1px solid var(--color-border-default);margin:13px 0 0;padding:7px;display:block;overflow:hidden}.markdown-body span.frame span img{float:left;display:block}.markdown-body span.frame span span{clear:both;color:var(--color-fg-default);padding:5px 0 0;display:block}.markdown-body span.align-center{clear:both;display:block;overflow:hidden}.markdown-body span.align-center>span{text-align:center;margin:13px auto 0;display:block;overflow:hidden}.markdown-body span.align-center span img{text-align:center;margin:0 auto}.markdown-body span.align-right{clear:both;display:block;overflow:hidden}.markdown-body span.align-right>span{text-align:right;margin:13px 0 0;display:block;overflow:hidden}.markdown-body span.align-right span img{text-align:right;margin:0}.markdown-body span.float-left{float:left;margin-right:13px;display:block;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{float:right;margin-left:13px;display:block;overflow:hidden}.markdown-body span.float-right>span{text-align:right;margin:13px auto 0;display:block;overflow:hidden}.markdown-body code,.markdown-body tt{background-color:var(--color-neutral-muted);border-radius:6px;margin:0;padding:.2em .4em;font-size:85%}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{-webkit-text-decoration:inherit;-webkit-text-decoration:inherit;text-decoration:inherit}.markdown-body samp{font-size:85%}.markdown-body pre{word-wrap:normal}.markdown-body pre code{font-size:100%}.markdown-body pre>code{word-break:normal;white-space:pre;background:0 0;border:0;margin:0;padding:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{word-break:normal;margin-bottom:0}.markdown-body .highlight pre,.markdown-body pre{background-color:var(--color-canvas-subtle);border-radius:6px;padding:16px;font-size:85%;line-height:1.45;overflow:auto}.markdown-body pre code,.markdown-body pre tt{max-width:auto;line-height:inherit;word-wrap:normal;background-color:rgba(0,0,0,0);border:0;margin:0;padding:0;display:inline;overflow:visible}.markdown-body .csv-data td,.markdown-body .csv-data th{text-align:left;white-space:nowrap;padding:5px;font-size:12px;line-height:1;overflow:hidden}.markdown-body .csv-data .blob-num{text-align:right;background:var(--color-canvas-default);border:0;padding:10px 8px 9px}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{background:var(--color-canvas-subtle);border-top:0;font-weight:600}.markdown-body [data-footnote-ref]:before{content:"["}.markdown-body [data-footnote-ref]:after{content:"]"}.markdown-body .footnotes{color:var(--color-fg-muted);border-top:1px solid var(--color-border-default);font-size:12px}.markdown-body .footnotes ol{padding-left:16px}.markdown-body .footnotes li{position:relative}.markdown-body .footnotes li:target:before{pointer-events:none;content:"";border:2px solid var(--color-accent-emphasis);border-radius:6px;position:absolute;top:-8px;bottom:-8px;left:-24px;right:-8px}.markdown-body .footnotes li:target{color:var(--color-fg-default)}.markdown-body .footnotes .data-footnote-backref g-emoji{font-family:monospace}.markdown-body{background-color:var(--color-canvas-default);color:var(--color-fg-default)}.markdown-body a{color:var(--color-accent-fg);text-decoration:none}.markdown-body a:hover{text-decoration:underline}.markdown-body iframe{background-color:#fff;border:0;margin-bottom:16px}.markdown-body svg.octicon{fill:currentColor}.markdown-body .anchor>.octicon{display:inline}.markdown-body .highlight .token.keyword,.gfm-highlight .token.keyword{color:var(--color-prettylights-syntax-keyword)}.markdown-body .highlight .token.tag .token.class-name,.markdown-body .highlight .token.tag .token.script .token.punctuation,.gfm-highlight .token.tag .token.class-name,.gfm-highlight .token.tag .token.script .token.punctuation{color:var(--color-prettylights-syntax-storage-modifier-import)}.markdown-body .highlight .token.operator,.markdown-body .highlight .token.number,.markdown-body .highlight .token.boolean,.markdown-body .highlight .token.tag .token.punctuation,.markdown-body .highlight .token.tag .token.script .token.script-punctuation,.markdown-body .highlight .token.tag .token.attr-name,.gfm-highlight .token.operator,.gfm-highlight .token.number,.gfm-highlight .token.boolean,.gfm-highlight .token.tag .token.punctuation,.gfm-highlight .token.tag .token.script .token.script-punctuation,.gfm-highlight .token.tag .token.attr-name{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.function,.gfm-highlight .token.function{color:var(--color-prettylights-syntax-entity)}.markdown-body .highlight .token.string,.gfm-highlight .token.string{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.comment,.gfm-highlight .token.comment{color:var(--color-prettylights-syntax-comment)}.markdown-body .highlight .token.class-name,.gfm-highlight .token.class-name{color:var(--color-prettylights-syntax-variable)}.markdown-body .highlight .token.regex,.gfm-highlight .token.regex{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.regex .regex-delimiter,.gfm-highlight .token.regex .regex-delimiter{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.tag .token.tag,.markdown-body .highlight .token.property,.gfm-highlight .token.tag .token.tag,.gfm-highlight .token.property{color:var(--color-prettylights-syntax-entity-tag)}
    </style>
  </head>
  <body>
    <main data-color-mode="light" data-light-theme="light" data-dark-theme="dark" class="markdown-body">
      <h1>Awesome Ai4lam Overview</h1>
<p>A list of awesome AI in libraries, archives, and museum collections from around the world 🕶️</p>
<p><a href="/">🏠 Home</a><span> · </span><a href="https://www.trackawesomelist.com/AI4LAM/awesome-ai4lam/rss.xml">🔥 Feed</a><span> · </span><a href="https://trackawesomelist.us17.list-manage.com/subscribe?u=d2f0117aa829c83a63ec63c2f&id=36a103854c">📮 Subscribe</a><span> · </span><a href="https://github.com/sponsors/theowenyoung">❤️  Sponsor</a><span> · </span><a href="https://github.com/AI4LAM/awesome-ai4lam">😺 AI4LAM/awesome-ai4lam</a><span> · </span><span>⭐ 121</span><span> · </span><span>🏷️ Library systems</span></p>
<p><span>[ </span><a href="/AI4LAM/awesome-ai4lam/">Daily</a><span> / </span><a href="/AI4LAM/awesome-ai4lam/week/">Weekly</a><span> / </span><span>Overview</span><span> ]</span></p>
<h1 id="awesome-ai-for-lam-awesome"><a class="anchor" aria-hidden="true" tabindex="-1" href="#awesome-ai-for-lam-awesome"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Awesome AI for LAM <a href="https://github.com/sindresorhus/awesome" rel="noopener noreferrer"><img src="https://raw.githubusercontent.com/AI4LAM/awesome-ai4lam/main/.graphics/awesome-list-badge.svg" alt="Awesome" /></a></h1><p>A curated list of resources, projects, and tools for using Artificial Intelligence in Libraries, Archives, and Museums.</p>
<p><a href="https://github.com/AI4LAM/awesome-ai4lam/blob/master/LICENSE" rel="noopener noreferrer"><img src="https://img.shields.io/badge/License-CC0-blue.svg?style=flat-square" alt="License" /></a>
<img src="https://img.shields.io/badge/Maintained%3F%2dyes-forestgreen.svg?style=flat-square" alt="Maintained?" />
<a href="https://github.com/AI4LAM/awesome-ai4lam/commits/main/" rel="noopener noreferrer"><img src="https://img.shields.io/github/last-commit/AI4LAM/awesome-ai4lam.svg?style=flat-square&amp;color=orange&amp;label=Last%20commit" alt="Last commit" /></a>
<img src="https://img.shields.io/github/contributors/AI4LAM/awesome-ai4lam?label=Contributors&amp;style=flat-square&amp;color=a44e88" alt="GitHub contributors" />
<a href="https://glammr.us/@AI4LAM" rel="noopener noreferrer"><img src="https://img.shields.io/badge/Mastodon-7334cF?style=flat-square&amp;logo=Mastodon&amp;logoColor=white" alt="Mastodon" /></a>
<a href="https://ai4lam.slack.com/join/shared_invite/zt-1omthldn8-9vrGySjIRdija1nKQm0ltA#/" rel="noopener noreferrer"><img src="https://img.shields.io/badge/Slack-5A255B?style=flat-square&amp;logo=slack&amp;logoColor=white" alt="Slack" /></a></p>
<h2 id="contents"><a class="anchor" aria-hidden="true" tabindex="-1" href="#contents"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contents</h2><ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#learning-resources">Learning Resources</a><ul>
<li><a href="#introductions-to-ai">Introductions to AI</a></li>
<li><a href="#computer-vision">Computer vision</a></li>
<li><a href="#natural-language-processing">Natural language processing</a></li>
<li><a href="#generative-ai">Generative AI</a></li>
<li><a href="#ai-in-galleries-libraries-archives-and-museums">AI in galleries, libraries, archives and museums</a></li>
<li><a href="#other-awesome-lists-in-ai-and-ml">Other "awesome" lists in AI and ML</a></li>
</ul>
</li>
<li><a href="#tools-and-frameworks">Tools and Frameworks</a><ul>
<li><a href="#document-analysis-transcription-and-labeling">Document analysis, transcription, and labeling</a></li>
<li><a href="#audio-and-video-analysis-transcription-and-labeling">Audio and video analysis, transcription, and labeling</a></li>
<li><a href="#indexing-and-classification">Indexing and classification</a></li>
<li><a href="#search-and-retrieval">Search and retrieval</a></li>
<li><a href="#applications-of-transformers-llms-and-gpt">Applications of Transformers, LLMs, and GPT</a></li>
</ul>
</li>
<li><a href="#datasets">Datasets</a><ul>
<li><a href="#datasets-available-on-hugging-face">Datasets available on Hugging Face</a></li>
<li><a href="#datasets-available-elsewhere">Datasets available elsewhere</a></li>
</ul>
</li>
<li><a href="#projects-initiatives-and-case-studies">Projects, Initiatives, and Case Studies</a><ul>
<li><a href="#project-lists--directories">Project lists &amp; directories</a></li>
<li><a href="#select-individual-projects">Select individual projects</a></li>
</ul>
</li>
<li><a href="#policies-and-recommendations">Policies and recommendations</a><ul>
<li><a href="#statements-by-organizations-and-government-bodies">Statements by organizations and government bodies</a></li>
<li><a href="#surveys-of-policies-and-recommendations">Surveys of policies and recommendations</a></li>
<li><a href="#frameworks">Frameworks</a></li>
</ul>
</li>
<li><a href="#conferences-and-workshops">Conferences and Workshops</a><ul>
<li><a href="#upcoming-conferences-and-workshops">Upcoming Conferences and Workshops</a></li>
<li><a href="#past-conferences-and-workshops">Past Conferences and Workshops</a></li>
</ul>
</li>
<li><a href="#publications-and-news-sources">Publications and News Sources</a><ul>
<li><a href="#journals-and-magazines">Journals and Magazines</a></li>
<li><a href="#news-sources">News sources</a></li>
</ul>
</li>
<li><a href="#community">Community</a></li>
<li><a href="#contributions">Contributions</a></li>
<li><a href="#license">License</a></li>
</ul>
<h2 id="introduction"><a class="anchor" aria-hidden="true" tabindex="-1" href="#introduction"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Introduction</h2><p>This list is a collection of resources, tools, projects, and other materials for professionals and enthusiasts in the Libraries, Archives, and Museums (LAM) sector. You might also know this as the GLAM (galleries, libraries, archives and museums) or CHI (cultural heritage institutions) sector, or be more familiar with the term 'memory institutions'. However you describe the field, if you know of an AI, machine learning, big data or data science project, event or resource related to collections, please share it here!</p>
<p>This list is maintained by the <a href="https://www.ai4lam.org/" rel="noopener noreferrer">AI4LAM</a> community. Its aim is to support knowledge sharing, innovation, and collaboration in applying AI to LAM.</p>
<h2 id="learning-resources"><a class="anchor" aria-hidden="true" tabindex="-1" href="#learning-resources"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Learning Resources<a href="https://forms.gle/aPA41GT5AmbxrTwq5"><img alt="Click button to suggest an addition" align="right" src="https://raw.githubusercontent.com/AI4LAM/awesome-ai4lam/main/.graphics/suggest-addition-small.svg" /></a></h2><p>Please note: the appearance of a resource on this list does not constitute an official endorsement by AI4LAM.</p>
<h3 id="introductions-to-ai"><a class="anchor" aria-hidden="true" tabindex="-1" href="#introductions-to-ai"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Introductions to AI</h3><ul>
<li><a href="https://www.elementsofai.com/" rel="noopener noreferrer">Elements of AI</a> – free course by MinnaLearn &amp; University of Helsinki</li>
<li><a href="https://carpentries-incubator.github.io/machine-learning-librarians-archivists/" rel="noopener noreferrer">Introduction to AI for GLAM</a> – by Library Carpentries</li>
<li><a href="https://aipedagogy.org" rel="noopener noreferrer">AI Guide by the AI Pedagogy Project</a> – collection of materials by <a href="https://mlml.io/about/" rel="noopener noreferrer">metaLAB</a></li>
<li><a href="https://docs.google.com/presentation/d/1dVdS3u-XS2RDexNm3RlwICCsh5gBmdi1pBARgIGnPN8" rel="noopener noreferrer">Slides from FF23 workshop on <em>Intro to AI for GLAM</em></a> and <a href="https://pad.carpentries.org/intro-ai-ff2023" rel="noopener noreferrer">shared notes</a></li>
<li><a href="https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/edit#slide=id.g168a3288f7_0_58" rel="noopener noreferrer">Machine Learning 101</a> – by Jason Mayes from Google</li>
<li><a href="https://www.codecademy.com/catalog/subject/artificial-intelligence" rel="noopener noreferrer">Codecademy AI Courses</a> – many topics; some lessons are free, some are for-fee</li>
<li><a href="https://sebastianraschka.com/blog/2021/dl-course.html" rel="noopener noreferrer">Introduction to Deep Learning</a>, by Sebastian Raschka</li>
<li><a href="https://d2l.ai/index.html" rel="noopener noreferrer">Dive into Deep Learning</a>, by Zhang et al.</li>
<li><a href="https://exploreai.jisc.ac.uk/" rel="noopener noreferrer">A Collection of AI Demos to Discover and Explore</a></li>
<li><a href="https://www.deeplearning.ai/short-courses/" rel="noopener noreferrer">DeepLearning.AI Short Courses</a>, a free courses from a platform created by Andrew Ng</li>
<li><a href="https://www.codecademy.com/learn/intro-to-hugging-face" rel="noopener noreferrer">Introduction to Hugging Face</a>, a free course by Codecademy</li>
</ul>
<h3 id="computer-vision"><a class="anchor" aria-hidden="true" tabindex="-1" href="#computer-vision"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Computer vision</h3><ul>
<li><a href="https://machinelearningmastery.com/what-is-computer-vision/" rel="noopener noreferrer">A Gentle Introduction to Computer Vision</a> – from Machine Learning Mastery</li>
<li><a href="https://docs.google.com/document/d/1FpKfX4hI38ZKG81osa3bXgitEArxK45NKmT6NOyrnJk" rel="noopener noreferrer">Computer Vision for Heritage Collections</a> – French-language 2 hr workshop designed to introduce computer vision applications to cultural heritage professionals</li>
<li><a href="https://programminghistorian.org/en/lessons/computer-vision-deep-learning-pt1" rel="noopener noreferrer">Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification</a> – two-part intro by the Programming Historian</li>
</ul>
<h3 id="natural-language-processing"><a class="anchor" aria-hidden="true" tabindex="-1" href="#natural-language-processing"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Natural language processing</h3><ul>
<li><a href="https://www.fast.ai/posts/2019-07-08-fastai-nlp.html" rel="noopener noreferrer">A Code-First Introduction to NLP</a> – by Rachel Thomas of fast.ai</li>
<li><a href="https://lena-voita.github.io/nlp_course.html" rel="noopener noreferrer">NLP course</a> and associated <a href="https://github.com/yandexdataschool/nlp_course#readme" rel="noopener noreferrer">GitHub repo</a> – by Elena Voita</li>
<li><a href="https://www.youtube.com/playlist?list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw" rel="noopener noreferrer">NLP accelerated class</a> – by Machine Learning University</li>
<li><a href="https://nlpoverview.com/index.html" rel="noopener noreferrer">Overview of deep learning techniques applied to NLP (2018)</a></li>
<li><a href="https://machinelearningmastery.com/category/natural-language-processing/" rel="noopener noreferrer">Deep Learning for NLP</a> – from Machine Learning Mastery</li>
<li><a href="https://github.com/hb20007/hands-on-nltk-tutorial#readme" rel="noopener noreferrer">Hands-on NLTK Tutorial</a></li>
<li><a href="https://github.com/NirantK/NLP_Quickbook#readme" rel="noopener noreferrer">NLP in Python - Quickstart Guide</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html" rel="noopener noreferrer">Deep Learning for NLP With Pytorch</a></li>
</ul>
<h3 id="generative-ai"><a class="anchor" aria-hidden="true" tabindex="-1" href="#generative-ai"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Generative AI</h3><ul>
<li><a href="https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e" rel="noopener noreferrer">A Very Gentle Introduction to LLMs without the Hype</a> – by Mark Riedl</li>
<li><a href="https://www.youtube.com/watch?v=iR2O2GPbB0E" rel="noopener noreferrer">What are large language models (LLMs)?</a> – (YouTube) by Google for Developers</li>
<li><a href="https://docs.google.com/presentation/d/1X3VpadTOsUe2neFts24pURy3nNQ2k64k4d3MEqHlEgk/edit#slide=id.g25b6aed46c6_0_492" rel="noopener noreferrer">A brief introduction to GenAI</a> – by U. Michigan MIDAS</li>
<li><a href="https://www.coursera.org/learn/generative-ai-for-everyone?irclickid=S5hzeGTIExyPTTOSNn2PRyfHUkHzH8TNw0Bo1c0&amp;irgwc=1" rel="noopener noreferrer">Generative AI for Everyone</a> – free Coursera course by Andrew Ng</li>
<li><a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/" rel="noopener noreferrer">What Is ChatGPT Doing … and Why Does It Work?</a> – by Stephen Wolfram</li>
<li><a href="https://towardsdatascience.com/the-map-of-transformers-e14952226398" rel="noopener noreferrer">The Map Of Transformers</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/" rel="noopener noreferrer">The Illustrated Transformer</a>, a visual introduction to transformers</li>
<li><a href="https://www.cloudskillsboost.google/paths/118" rel="noopener noreferrer">Introduction to Generative AI</a>, by Google</li>
<li><a href="https://microsoft.github.io/generative-ai-for-beginners/#/" rel="noopener noreferrer">Generative AI for Beginners - A Course</a>, by Microsoft</li>
<li><a href="https://sebastianraschka.com/blog/2023/llm-reading-list.html" rel="noopener noreferrer">Understanding LLMs – A Transformative Reading List</a></li>
<li><a href="https://github.com/mlabonne/llm-course#readme" rel="noopener noreferrer">Large Language Model Course</a></li>
<li><a href="https://nationalcentreforai.jiscinvolve.org/wp/2023/10/16/generative-ai-primer/" rel="noopener noreferrer">A Generative AI Primer</a>, by the UK's National Centre for AI</li>
</ul>
<h3 id="ai-in-galleries-libraries-archives-and-museums"><a class="anchor" aria-hidden="true" tabindex="-1" href="#ai-in-galleries-libraries-archives-and-museums"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>AI in galleries, libraries, archives and museums</h3><ul>
<li>The <a href="https://www.youtube.com/@ai4lam120/videos" rel="noopener noreferrer">AI4LAM YouTube channel</a> has introductory presentations on many topics</li>
<li>The <a href="https://www.cenl.org/networkgroups/ai-in-libraries-network-group/" rel="noopener noreferrer">CENL "AI in Libraries" network group</a> is also organizing webinars on AI implementation in GLAM.</li>
</ul>
<h3 id="other-awesome-lists-in-ai-and-ml"><a class="anchor" aria-hidden="true" tabindex="-1" href="#other-awesome-lists-in-ai-and-ml"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Other "awesome" lists in AI and ML</h3><ul>
<li><a href="https://github.com/jbhuang0604/awesome-computer-vision#readme" rel="noopener noreferrer">Awesome Computer Vision</a></li>
<li><a href="https://github.com/brianspiering/awesome-dl4nlp#readme" rel="noopener noreferrer">Awesome Deep Learning for Natural Language Processing (NLP)</a></li>
<li><a href="https://github.com/ChristosChristofidis/awesome-deep-learning#readme" rel="noopener noreferrer">Awesome Deep Learning</a></li>
<li><a href="https://github.com/guillaume-chevalier/awesome-deep-learning-resources#readme" rel="noopener noreferrer">Awesome Deep Learning Resources</a></li>
<li><a href="https://github.com/kjw0612/awesome-deep-vision#readme" rel="noopener noreferrer">Awesome Deep Vision</a></li>
<li><a href="https://github.com/tstanislawek/awesome-document-understanding#readme" rel="noopener noreferrer">Awesome Document Understanding</a></li>
<li><a href="https://github.com/steven2358/awesome-generative-ai#readme" rel="noopener noreferrer">Awesome Generative AI</a></li>
<li><a href="https://github.com/weiaicunzai/awesome-image-classification#readme" rel="noopener noreferrer">Awesome Image Classification</a></li>
<li><a href="https://github.com/LibraryCarpentry/awesome-jupyter-glam#readme" rel="noopener noreferrer">Awesome Jupyter GLAM</a></li>
<li><a href="https://github.com/Hannibal046/Awesome-LLM#readme" rel="noopener noreferrer">Awesome LLM</a></li>
<li><a href="https://github.com/josephmisiti/awesome-machine-learning#readme" rel="noopener noreferrer">Awesome Machine Learning</a></li>
<li><a href="https://github.com/ujjwalkarn/Machine-Learning-Tutorials#readme" rel="noopener noreferrer">Awesome Machine Learning &amp; Deep Learning Tutorials</a></li>
<li><a href="https://github.com/accelerated-text/awesome-nlg#readme" rel="noopener noreferrer">Awesome Natural Language Generation</a></li>
<li><a href="https://github.com/keon/awesome-nlp#readme" rel="noopener noreferrer">Awesome NLP</a></li>
<li><a href="https://github.com/EthicalML/awesome-production-machine-learning#readme" rel="noopener noreferrer">Awesome Production Machine Learning</a></li>
<li><a href="https://github.com/SE-ML/awesome-seml#readme" rel="noopener noreferrer">Awesome Software Engineering for Machine Learning</a></li>
<li><a href="https://github.com/dk-liang/Awesome-Visual-Transformer#readme" rel="noopener noreferrer">Awesome Visual Transformer</a></li>
<li><a href="https://github.com/altamiracorp/awesome-xai#readme" rel="noopener noreferrer">Awesome XAI</a></li>
<li><a href="https://index.quantumstat.com" rel="noopener noreferrer">The NLP Index</a></li>
</ul>
<h2 id="tools-and-frameworks"><a class="anchor" aria-hidden="true" tabindex="-1" href="#tools-and-frameworks"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tools and Frameworks<a href="https://forms.gle/aPA41GT5AmbxrTwq5"><img alt="Click button to suggest an addition" align="right" src="https://raw.githubusercontent.com/AI4LAM/awesome-ai4lam/main/.graphics/suggest-addition-small.svg" /></a></h2><p>Note: datasets for training and testing are listed in a <a href="#datasets">separate section</a> of this document.</p>
<h3 id="document-analysis-transcription-and-labeling"><a class="anchor" aria-hidden="true" tabindex="-1" href="#document-analysis-transcription-and-labeling"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Document analysis, transcription, and labeling</h3><ul>
<li><a href="https://teklia.com/blog/arkindex-goes-open-source/" rel="noopener noreferrer">Arkindex</a> – open-source platform for managing &amp; processing collections of digitized documents</li>
<li><a href="https://teklia.com/blog/open-sourcing-callico/" rel="noopener noreferrer">Callico</a> – open-source web platform for document annotation</li>
<li><a href="https://www.coconut-libtool.com" rel="noopener noreferrer">Coconut Libtool</a> – web-based textual analysis tool designed to assist social scientists, librarians, or anyone in data analysis</li>
<li><a href="https://github.com/CLARIAH/DANE#readme" rel="noopener noreferrer">Distributed Annotation 'n' Enrichment (DANE)</a> – compute task assignment &amp; file storage for automatic annotation of content (<a href="https://www.clariah.nl/about-clariah" rel="noopener noreferrer">CLARIAH</a>, Norway)</li>
<li><a href="https://huggingface.co/spaces/Riksarkivet/htr_demo" rel="noopener noreferrer">HTRFLOW demo</a> and associated <a href="https://github.com/Swedish-National-Archives-AI-lab/htrflow_app" rel="noopener noreferrer">GitHub repo</a> – explore AI models for Handwritten Text Recogntion (Swedish National Archives)</li>
<li><a href="https://labelstud.io" rel="noopener noreferrer">Label Studio</a> – data labeling platform to fine-tune LLMs, prepare training data, or validate AI models</li>
<li><a href="https://bnl.public.lu/en.html" rel="noopener noreferrer">OCR correction</a> – OCR correction tools (Bibliothèque nationale, Luxembourg)</li>
<li><a href="https://github.com/VikParuchuri/surya#readme" rel="noopener noreferrer">Surya</a> – multilingual document OCR toolkit with line-level text detection</li>
<li><a href="https://huggingface.co/KBLab" rel="noopener noreferrer">Text models from the National Library of Sweden</a> – available on Hugging Face</li>
<li><a href="https://readcoop.eu/transkribus/" rel="noopener noreferrer">Transkribus</a> – transcription, recognition, &amp; searching of historical documents</li>
</ul>
<h3 id="audio-and-video-analysis-transcription-and-labeling"><a class="anchor" aria-hidden="true" tabindex="-1" href="#audio-and-video-analysis-transcription-and-labeling"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Audio and video analysis, transcription, and labeling</h3><ul>
<li><a href="https://huggingface.co/KBLab" rel="noopener noreferrer">Acoustic models from the National Library of Sweden</a> – available on Hugging Face</li>
<li><a href="https://annotorious.github.io" rel="noopener noreferrer">Annotorious</a> – JavaScript image annotation library</li>
<li><a href="https://uisapp2.iu.edu/confluence-prd/display/AMP/AMP%3A+Audiovisual+Metadata+Platform" rel="noopener noreferrer">Audiovisual Metadata Platform (AMP)</a> – generation of metadata for discovery &amp; use of digital audio &amp; video collections (Indiana U., USA)</li>
<li><a href="https://kilthub.cmu.edu/articles/preprint/CAMPI_Computer-Aided_Metadata_Generation_for_Photo_archives_Initiative/12791807" rel="noopener noreferrer">CAMPI</a> – Computer-Aided Metadata Generation for Photo archives Initiative (Carnegie Mellonw U., USA)</li>
<li><a href="https://archive.mpi.nl/tla/elan" rel="noopener noreferrer">ELAN</a> – addS textual annotations to audio and/or video recordings (Max Planck Institute for Psycholinguistics, The Netherlands)</li>
<li><a href="https://github.com/ina-foss/inaFaceAnalyzer#readme" rel="noopener noreferrer">inaFaceAnalyzer</a> – Python toolbox for face-based description of gender representation in media (Institut National de l'Audiovisuel, France)</li>
<li><a href="https://labs.loc.gov/work/experiments/newspaper-navigator/" rel="noopener noreferrer">Newspaper Navigator</a> – explore visual &amp; textual content in the <em>Chronicling America</em> digitized newspaper collection (Library of Congress, USA)</li>
<li><a href="https://medium.com/headai-customer-stories/customer-story-oodi-1d1ef2554bb6" rel="noopener noreferrer">Oodi</a> – virtual information assistant (Helsinki Central Library)</li>
<li><a href="https://retv-project.eu" rel="noopener noreferrer">ReTV</a> – video analysis &amp; summarization (Modul Univesrity, Austria)</li>
<li><a href="https://www.robots.ox.ac.uk/~vgg/software/via/" rel="noopener noreferrer">VGG Image Annotator</a> – manual annotation software for image, audio and video</li>
</ul>
<h3 id="indexing-and-classification"><a class="anchor" aria-hidden="true" tabindex="-1" href="#indexing-and-classification"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Indexing and classification</h3><ul>
<li><a href="https://annif.org" rel="noopener noreferrer">Annif</a> and <a href="https://github.com/NatLibFi/Annif-tutorial" rel="noopener noreferrer">associated tutorial</a> – tool for automated subject indexing and classification (National Library of Finland)</li>
</ul>
<h3 id="search-and-retrieval"><a class="anchor" aria-hidden="true" tabindex="-1" href="#search-and-retrieval"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Search and retrieval</h3><ul>
<li><a href="https://gallicapix.bnf.fr/rest?run=findIllustrations-form.xq" rel="noopener noreferrer">GallicaPix</a> – retrieval of heritage images (Bibliothèque nationale de France)</li>
<li><a href="https://www.bnf.fr/sites/default/files/2022-05/Poster_Gallica_Snoop.pdf" rel="noopener noreferrer">GallicaSNOOP</a> – framework for large-scale content-based image retrieval (Bibliothèque nationale de France)</li>
<li><a href="https://www.nb.no/maken/" rel="noopener noreferrer">Maken Similarity Service</a> – tools for alternative reading &amp; finding similar photographs (National Library of Norway)</li>
<li><a href="https://beta.nasjonalmuseet.no/2023/08/add-semantic-search-to-a-online-collection/" rel="noopener noreferrer">Semantic search for Nasjonalmuseet’s online collection</a> – open beta test (National Museum of Norway)</li>
<li><a href="https://www.robots.ox.ac.uk/~vgg/software/vts/" rel="noopener noreferrer">VGG Text Search (VTS) Engine</a> – search for text strings over a user-defined image set</li>
</ul>
<h3 id="applications-of-transformers-llms-and-gpt"><a class="anchor" aria-hidden="true" tabindex="-1" href="#applications-of-transformers-llms-and-gpt"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Applications of Transformers, LLMs, and GPT</h3><ul>
<li><a href="https://maartengr.github.io/BERTopic/index.html" rel="noopener noreferrer">BERTopic</a> – topic modeling technique that leverages Transformers and c-TF-IDF</li>
<li><a href="https://chat.eluxemburgensia.lu/login?next=/" rel="noopener noreferrer">Chatbot for Luxembourgish newspapers</a> – uses ChatGPT and understands French, German and English (Bibliothèque nationale de Luxembourg)</li>
<li><a href="https://github.com/NBAiLab/notram#readme" rel="noopener noreferrer">Norwegian Transformer Model (NoTraM)</a> – transformer model for Norwegian and Nordic languages (National Library of Norway)</li>
<li><a href="https://github.com/Kungbib/swedish-bert-models#readme" rel="noopener noreferrer">Swedish BERT</a> – BERT model for the Swedish language (Royal Library of Sweden)</li>
<li><a href="https://www.robots.ox.ac.uk/~vgg/projects/visualai/index.html" rel="noopener noreferrer">Visual AI</a> – open-world interpretable visual transformer (UK)</li>
</ul>
<h2 id="datasets"><a class="anchor" aria-hidden="true" tabindex="-1" href="#datasets"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Datasets<a href="https://forms.gle/aPA41GT5AmbxrTwq5"><img alt="Click button to suggest an addition" align="right" src="https://raw.githubusercontent.com/AI4LAM/awesome-ai4lam/main/.graphics/suggest-addition-small.svg" /></a></h2><h3 id="datasets-available-on-hugging-face"><a class="anchor" aria-hidden="true" tabindex="-1" href="#datasets-available-on-hugging-face"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Datasets available on Hugging Face</h3><p>There are many (G)LAM-related datasets on Hugging Face. The following links will perform live searches directly in Hugging Face for datasets tagged with the given terms:</p>
<ul>
<li><a href="https://huggingface.co/search/full-text?q=handwritten%20text%20recognition&amp;type=dataset" rel="noopener noreferrer">Full-text search for "handwritten text recognition"</a></li>
<li><a href="https://huggingface.co/search/full-text?q=optical%20character%20recognition&amp;type=dataset" rel="noopener noreferrer">Full-text search for "optical text recognition"</a></li>
<li><a href="https://huggingface.co/datasets?task_categories=task_categories%3Asummarization&amp;type=dataset" rel="noopener noreferrer">Datasets tagged "summarization"</a></li>
<li><a href="https://huggingface.co/datasets?task_categories=task_categories%3Afeature-extraction&amp;type=dataset" rel="noopener noreferrer">Datasets tagged "feature extraction"</a></li>
<li><a href="https://huggingface.co/datasets?task_categories=task_categories%3Aimage-classification&amp;type=dataset&amp;type=dataset" rel="noopener noreferrer">Datasets tagged "image classification"</a></li>
<li><a href="https://huggingface.co/datasets?task_categories=task_categories%3Avideo-classification&amp;type=dataset" rel="noopener noreferrer">Datasets tagged "video classification"</a></li>
<li><a href="https://huggingface.co/datasets?task_categories=task_categories%3Atext-classification&amp;type=dataset" rel="noopener noreferrer">Datasets tagged "text classification"</a></li>
<li><a href="https://huggingface.co/datasets?task_categories=task_categories%3Aaudio-classification&amp;type=dataset" rel="noopener noreferrer">Datasets tagged "audio classification"</a></li>
</ul>
<h3 id="datasets-available-elsewhere"><a class="anchor" aria-hidden="true" tabindex="-1" href="#datasets-available-elsewhere"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Datasets available elsewhere</h3><ul>
<li><a href="https://github.com/piskvorky/gensim-data#readme" rel="noopener noreferrer">Gensim datasets</a> – repository of datasets for unstructured text processing</li>
<li><a href="https://zenodo.org/search?q=metadata.subjects.subject%3A%22handwritten%20text%20recognition%22&amp;l=list&amp;p=1&amp;s=10&amp;sort=bestmatch" rel="noopener noreferrer">HTR datasets in Zenodo</a> – subject search in Zenodo</li>
<li><a href="https://htr-united.github.io" rel="noopener noreferrer">HTR-United</a> – datasets for training transcription or segmentation models</li>
<li><a href="https://www.kaggle.com/datasets" rel="noopener noreferrer">Kaggle datasets</a></li>
<li><a href="https://github.com/niderhoff/nlp-datasets#readme" rel="noopener noreferrer">nlp-datasets</a> – free/public domain datasets with text data for use in NLP</li>
<li><a href="https://openlibrary.org/developers/dumps" rel="noopener noreferrer">Open Library data dumps</a> – from the Internet Archive</li>
<li><a href="https://data.nls.uk/" rel="noopener noreferrer">Open data collections from the National Library of Scotland</a></li>
<li><a href="https://registry.opendata.aws" rel="noopener noreferrer">Registry of Open Data on AWS</a> – datasets tagged by topic</li>
</ul>
<h2 id="projects-initiatives-and-case-studies"><a class="anchor" aria-hidden="true" tabindex="-1" href="#projects-initiatives-and-case-studies"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Projects, Initiatives, and Case Studies<a href="https://forms.gle/aPA41GT5AmbxrTwq5"><img alt="Click button to suggest an addition" align="right" src="https://raw.githubusercontent.com/AI4LAM/awesome-ai4lam/main/.graphics/suggest-addition-small.svg" /></a></h2><h3 id="project-lists--directories"><a class="anchor" aria-hidden="true" tabindex="-1" href="#project-lists--directories"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Project lists &amp; directories</h3><ul>
<li><a href="https://www.archives.gov/data/ai-inventory" rel="noopener noreferrer">Inventory of NARA Artificial Intelligence (AI) Use Cases</a> - the US National Archives and Records Administration (NARA)'s inventory of AI use cases</li>
<li><a href="https://docs.google.com/spreadsheets/d/1A7IVnucQZ0ICxYSOCjqQ1oV3xGgNzDKtIYGrk6smV7w/edit#gid=0" rel="noopener noreferrer">List of Artificial Intelligence (AI) initiatives in museums</a> – compiled in 2021 by Elena Villaespesa, Oonagh Murphy and Kate Nadel for the <a href="https://themuseumsai.network" rel="noopener noreferrer">Museums+AI Network</a> project.</li>
<li><a href="https://libraries.ou.edu/content/project-highlight-projects-ai-registry-pair" rel="noopener noreferrer">Projects in AI Registry (PAIR)</a> – registry of AI projects in higher education (U. Oklahoma Libraries, USA)</li>
</ul>
<h3 id="select-individual-projects"><a class="anchor" aria-hidden="true" tabindex="-1" href="#select-individual-projects"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Select individual projects</h3><ul>
<li><a href="https://huggingface.co/spaces/DIBT/prompt-collective" rel="noopener noreferrer">Argilla prompt-collective</a> – crowdsourcing effort to rank 50,000 prompts, on Hugging Face</li>
<li><a href="https://huggingface.co/biglam" rel="noopener noreferrer">BigLAM</a> – BigScience Libraries, Archives and Museums on Hugging Face</li>
<li><a href="https://livingwithmachines.ac.uk" rel="noopener noreferrer">Living with Machines</a> – Turing Institute &amp; British Library</li>
<li><a href="https://blog.archiveshub.jisc.ac.uk/2022/02/28/machine-learning-with-archive-collections/" rel="noopener noreferrer">Machine Learning with Archive Collections</a></li>
<li><a href="https://huggingface.co/NbAiLab" rel="noopener noreferrer">Nasjonalbiblioteket AI Lab</a> – National Library of Norway on Hugging Face</li>
<li><a href="https://huggingface.co/KBLab" rel="noopener noreferrer">KBLab</a> – National Library of Sweden on Hugging Face</li>
<li><a href="https://www.youtube.com/watch?v=8khPUtwaVaw" rel="noopener noreferrer">Vatican Manuscripts</a> – machine transcription in the Vatican Secret Archive</li>
<li><a href="https://huggingface.co/PleIAs" rel="noopener noreferrer">PleIAs</a> – French organization training LLMs with an open science approach</li>
</ul>
<h2 id="policies-and-recommendations"><a class="anchor" aria-hidden="true" tabindex="-1" href="#policies-and-recommendations"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Policies and recommendations<a href="https://forms.gle/aPA41GT5AmbxrTwq5"><img alt="Click button to suggest an addition" align="right" src="https://raw.githubusercontent.com/AI4LAM/awesome-ai4lam/main/.graphics/suggest-addition-small.svg" /></a></h2><h3 id="statements-by-organizations-and-government-bodies"><a class="anchor" aria-hidden="true" tabindex="-1" href="#statements-by-organizations-and-government-bodies"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Statements by organizations and government bodies</h3><ul>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3626110" rel="noopener noreferrer">ACM TechBrief on Generative AI, by the ACM Technology Policy Council</a></li>
<li><a href="https://www.priv.gc.ca/en/privacy-topics/technology/artificial-intelligence/gd_principles_ai/" rel="noopener noreferrer">Canadian Government <em>Principles for responsible, trustworthy and privacy-protective generative AI technologies</em></a></li>
<li><a href="https://repository.ifla.org/bitstream/123456789/1646/1/ifla_statement_on_libraries_and_artificial_intelligence-full-text.pdf" rel="noopener noreferrer">IFLA Statement on Libraries and Artificial Intelligence</a></li>
<li><a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/" rel="noopener noreferrer">US Government <em>Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence</em></a></li>
</ul>
<h3 id="surveys-of-policies-and-recommendations"><a class="anchor" aria-hidden="true" tabindex="-1" href="#surveys-of-policies-and-recommendations"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Surveys of policies and recommendations</h3><ul>
<li><a href="https://www.brookings.edu/articles/a-cluster-analysis-of-national-ai-strategies/" rel="noopener noreferrer">A cluster analysis of national AI strategies</a> – Brookings Institute analysis of different countries’ national AI strategies, Dec. 2023</li>
<li><a href="https://link.springer.com/article/10.1007/s43681-022-00205-0" rel="noopener noreferrer">A principled governance for emerging AI regimes: lessons from China, the European Union, and the United States</a> by R. B. L. Dixon in <em>AI and Ethics</em>, 3, 793–810, 2023</li>
<li><a href="https://www.weforum.org/publications/ai-governance-alliance-briefing-paper-series/" rel="noopener noreferrer">AI Governance Alliance: Briefing Paper Series</a> – by the World Economic Forum, Jan. 2024</li>
<li><a href="https://doi.org/10.1177/0340035223119617" rel="noopener noreferrer">AI policies across the globe: Implications and recommendations for libraries</a> by L. S. Lo in <em>IFLA Journal</em>, 49(4), 645–649, 2023</li>
<li><a href="http://dx.doi.org/10.2139/ssrn.3518482" rel="noopener noreferrer">Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI</a> by Fjeld et al, Berkman Klein Center Research Publication No. 2020-1, 2020</li>
<li><a href="https://www.muchaduabout.com/post/what-ethics-do-i-need-to-consider-when-using-ai" rel="noopener noreferrer">What ethics do I need to consider when using AI?</a> – blog posting by Livi Adu, Nov. 2023</li>
<li><a href="https://www.lib.montana.edu/responsible-ai/" rel="noopener noreferrer">Responsible AI in Libraries and Archives</a> - IMLS funded project to produce tools and strategies that support responsible use of AI in the field (2022-2025)</li>
</ul>
<h3 id="frameworks"><a class="anchor" aria-hidden="true" tabindex="-1" href="#frameworks"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Frameworks</h3><ul>
<li><a href="https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-023-00408-3" rel="noopener noreferrer">A Comprehensive AI Policy Education Framework for University Teaching and Learning</a> by C. K. Y. Chan in <em>International Journal of Educational Technology in Higher Education</em>, 20(38), 2023.</li>
<li><a href="https://computing.mit.edu/wp-content/uploads/2023/11/AIPolicyBrief.pdf" rel="noopener noreferrer">A Framework for U.S. AI Governance: Creating a Safe and Thriving AI Sector</a> white paper by the MIT Schwarzman College of Computing, Dec. 11, 2023. (See also <a href="https://news.mit.edu/2023/mit-group-releases-white-papers-governance-ai-1211" rel="noopener noreferrer">related article in MIT News</a>.)</li>
<li><a href="https://blogs.loc.gov/thesignal/2023/11/introducing-the-lc-labs-artificial-intelligence-planning-framework/" rel="noopener noreferrer">LC Labs Artificial Intelligence Planning Framework</a> – US Library of Congress planning framework for responsible exploration and adoption of AI<ul>
<li>French translation: <a href="https://github.com/altomator/Planification-de-projets-IA" rel="noopener noreferrer">Planification de projets IA dans les GLAM</a></li>
</ul>
</li>
</ul>
<h2 id="conferences-and-workshops"><a class="anchor" aria-hidden="true" tabindex="-1" href="#conferences-and-workshops"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Conferences and Workshops<a href="https://forms.gle/aPA41GT5AmbxrTwq5"><img alt="Click button to suggest an addition" align="right" src="https://raw.githubusercontent.com/AI4LAM/awesome-ai4lam/main/.graphics/suggest-addition-small.svg" /></a></h2><p>The annual <em>Fantastic Futures</em> conference is the main conference series for the AI4LAM community. Various other conferences and workshops are relevant to the community and may be included in the list below.</p>
<h3 id="upcoming-conferences-and-workshops"><a class="anchor" aria-hidden="true" tabindex="-1" href="#upcoming-conferences-and-workshops"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Upcoming Conferences and Workshops</h3><p>👋🏻 <strong>Note</strong>: AI4LAM's <a href="https://docs.google.com/spreadsheets/d/1jO8dKt0CuhZKq382OZRdMSGOU3OpJhgK2nJ-FAynbeo/edit#gid=1287495458" rel="noopener noreferrer">conferences tracker Google sheet</a> has a more complete list of events. The following is a list of larger and/or especially relevant events for AI4LAM.</p>
<ul>
<li><a href="https://bitcuratorconsortium.org/forum" rel="noopener noreferrer">BitCurator Forum</a> – Mar. 19–22 virtual event on digital forensics, digital archives, and related digital analysis workflows</li>
<li><a href="https://netpreserve.org/ga2024/" rel="noopener noreferrer">IIPC General Assembly &amp; Web Archiving Conference</a> – Apr. 24–26 at the Bibliothèque nationale de France, Paris, France.</li>
<li><a href="https://forum2024.diglib.org" rel="noopener noreferrer">Digital Library Federation (DLF) 2024 Forum</a> – Jul. 29–31 at Michigan State U., East Lansing, Michigan, USA.</li>
<li><a href="https://icdar2024.net" rel="noopener noreferrer">International Conference on Document Analysis and Recognition (ICDAR) 2024</a> – Aug. 30–Sep. 4 in Athens, Greece.</li>
<li><a href="https://ipres2024.pubpub.org" rel="noopener noreferrer">International Conference on Digital Preservation (iPRES) 2024</a> – Sep. 16–20 in Ghent &amp; Flanders, Belgium.</li>
<li><a href="https://www.nfsa.gov.au/fantastic-futures-canberra-2024-artificial-intelligence-libraries-archives-and-museums" rel="noopener noreferrer">Fantastic Futures 2024</a> – Oct. 16–18 at the National Film and Sound Archive of Australia (NFSA), Canberra, Australia.</li>
<li><a href="https://www.ai4libraries.org" rel="noopener noreferrer">ai4Libraries Conference</a> – Oct. 23 and/or 24 virtual event hosted by Georgia Tech Library, Atlanta, Georgia, USA.</li>
</ul>
<h3 id="past-conferences-and-workshops"><a class="anchor" aria-hidden="true" tabindex="-1" href="#past-conferences-and-workshops"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Past Conferences and Workshops</h3><ul>
<li><a href="https://www.nb.no/hva-skjer/ai-conference/" rel="noopener noreferrer">Fantastic Futures 2018</a> – Dec. 5 at the National Library of Norway, Oslo, Norway.</li>
<li><a href="https://wayback.stanford.edu/was/20230508165810/http://library.stanford.edu/projects/fantastic-futures" rel="noopener noreferrer">Fantastic Futures 2019</a> – Dec. 4–6 at Stanford University, Stanford, California, USA.</li>
<li><a href="https://www.bnf.fr/fr/captations-et-supports-de-la-conference-2021" rel="noopener noreferrer">Fantastic Futures 2021</a> – Dec. 8–10 at the Bibliothèque nationale de France, Paris, France.</li>
<li><a href="https://sites.google.com/view/ai4lam/ai4lam-2022-virtual-event" rel="noopener noreferrer">Fantastic Futures 2022</a> – Nov. 30–Dec. 2 virtual event hosted by the British Library, London, England.</li>
<li><a href="https://www.ai4libraries.org" rel="noopener noreferrer">ai4Libraries Conference</a> – Oct. 19 virtual event hosted by Georgia Tech Library, Atlanta, Georgia, USA.</li>
<li><a href="https://ff2023.archive.org" rel="noopener noreferrer">Fantastic Futures 2023</a> – Nov. 15–17 at Internet Archive Canada Headquarters, Vancouver, British Columbia, Canada.</li>
<li><a href="https://www.nfsa.gov.au/fantastic-futures-conference-canberra-2024" rel="noopener noreferrer">Fantastic Futures 2024</a> – Oct. 15–18 at The National Film and Sound Archive of Australia (NFSA) in Canberra, Australia.</li>
</ul>
<h2 id="publications-and-news-sources"><a class="anchor" aria-hidden="true" tabindex="-1" href="#publications-and-news-sources"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Publications and News Sources<a href="https://forms.gle/aPA41GT5AmbxrTwq5"><img alt="Click button to suggest an addition" align="right" src="https://raw.githubusercontent.com/AI4LAM/awesome-ai4lam/main/.graphics/suggest-addition-small.svg" /></a></h2><h3 id="journals-and-magazines"><a class="anchor" aria-hidden="true" tabindex="-1" href="#journals-and-magazines"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Journals and Magazines</h3><ul>
<li><a href="https://link.springer.com/journal/146/articles" rel="noopener noreferrer">AI &amp; Society</a></li>
<li><a href="https://onlinelibrary.wiley.com/loi/23719621" rel="noopener noreferrer">AI Magazine</a></li>
<li><a href="https://link.springer.com/journal/10502" rel="noopener noreferrer">Archival Science</a></li>
<li><a href="https://journals.sagepub.com/home/bds" rel="noopener noreferrer">Big Data &amp; Society</a></li>
<li><a href="https://www.dukeupress.edu/critical-ai" rel="noopener noreferrer">Critical AI</a></li>
<li><a href="https://digitalhumanities.org/dhq/" rel="noopener noreferrer">Digital Humanities Quarterly</a></li>
<li><a href="https://academic.oup.com/dsh" rel="noopener noreferrer">Digital Scholarship in the Humanities</a></li>
<li><a href="https://link.springer.com/journal/799" rel="noopener noreferrer">International Journal on Digital Libraries</a></li>
<li><a href="https://www.sciencedirect.com/journal/the-journal-of-academic-librarianship/special-issue/10WVZWS842J" rel="noopener noreferrer">Journal of Academic Librarianship</a></li>
<li><a href="https://culturalanalytics.org" rel="noopener noreferrer">Journal of Cultural Analytics</a></li>
<li><a href="https://www.emerald.com/insight/publication/issn/0022-0418" rel="noopener noreferrer">Journal of Documentation</a></li>
<li><a href="https://journals.sagepub.com/home/LIS" rel="noopener noreferrer">Journal of Librarianship and Information Science</a></li>
<li><a href="https://openhumanitiesdata.metajnl.com" rel="noopener noreferrer">Journal of Open Humanities Data</a></li>
<li><a href="https://asistdl.onlinelibrary.wiley.com/journal/23301643" rel="noopener noreferrer">Journal of the Association for Information Science and Technology</a></li>
<li><a href="https://dl.acm.org/journal/jocch" rel="noopener noreferrer">Journal on Computing and Cultural Heritage</a></li>
<li><a href="https://www.emerald.com/insight/publication/issn/0737-8831" rel="noopener noreferrer">Library Hi Tech</a></li>
<li><a href="https://journals.ala.org/index.php/lrts" rel="noopener noreferrer">Library Resources &amp; Technical Services</a></li>
<li><a href="https://dl.acm.org/journal/lilc" rel="noopener noreferrer">Literary and Linguistic Computing</a></li>
<li><a href="https://journals.sagepub.com/description/SSC" rel="noopener noreferrer">Social Science Computer Review</a></li>
<li><a href="https://content.iospress.com/journals/world-digital-libraries-an-international-journal" rel="noopener noreferrer">World Digital Libraries – An International Journal</a></li>
</ul>
<h3 id="news-sources"><a class="anchor" aria-hidden="true" tabindex="-1" href="#news-sources"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>News sources</h3><ul>
<li><a href="https://www.arl.org/category/day-in-review" rel="noopener noreferrer">ARL Day in Review</a></li>
</ul>
<h2 id="community"><a class="anchor" aria-hidden="true" tabindex="-1" href="#community"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Community</h2><p>The AI4LAM community's home page is <a href="https://ai4lam.org/" rel="noopener noreferrer">https://ai4lam.org</a>. The secretariat and other contact addresses can be found at the <a href="https://sites.google.com/view/ai4lam/about" rel="noopener noreferrer"><em>About</em></a> page.</p>
<h2 id="contributions"><a class="anchor" aria-hidden="true" tabindex="-1" href="#contributions"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributions</h2><p>Your help and participation in enhancing this awesome list are very much welcome! Please use the <a href="https://github.com/AI4LAM/awesome-ai4lam/issues" rel="noopener noreferrer">issue ticket system</a> to request additions or changes, or to make other contributions to this repository. For more information, please visit the <a href="https://github.com/AI4LAM/awesome-ai4lam/blob/main/README.md/CONTRIBUTING.md" rel="noopener noreferrer">guidelines for contributing</a>.</p>
<p>
<a href="https://forms.gle/aPA41GT5AmbxrTwq5">
  <img alt="Click button to suggest an addition" src="https://raw.githubusercontent.com/AI4LAM/awesome-ai4lam/main/.graphics/suggest-addition.svg" />
 </a>
</p>

<h2 id="license"><a class="anchor" aria-hidden="true" tabindex="-1" href="#license"><svg class="octicon octicon-link" viewbox="0 0 16 16" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>License</h2><img align="right" alt="CC0 Logo" src="https://raw.githubusercontent.com/AI4LAM/awesome-ai4lam/main/.graphics/cc0.jpg" width="100rem" />

<p>The contents of this page are licensed under the <a href="https://creativecommons.org/public-domain/cc0/" rel="noopener noreferrer">Creative Commons CC0 1.0 Universal license</a>. CC0 is a “no rights reserved” license; the authors relinquish copyright and similar rights to the contents of the Awesome AI for LAM list.</p>


    </main>
  </body>
</html>
