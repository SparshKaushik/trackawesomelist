{
  "version": "https://jsonfeed.org/version/1",
  "icon": "https://www.trackawesomelist.com/icon.png",
  "favicon": "https://www.trackawesomelist.com/favicon.ico",
  "language": "en",
  "title": "Track Awesome Xai Updates Daily",
  "_seo_title": "Track Awesome Xai (altamiracorp/awesome-xai) Updates Daily - Track Awesome List",
  "_site_title": "Track Awesome List",
  "description": "Awesome Explainable AI (XAI) and Interpretable ML Papers and Resources",
  "home_page_url": "https://www.trackawesomelist.com/altamiracorp/awesome-xai/",
  "feed_url": "https://www.trackawesomelist.com/altamiracorp/awesome-xai/feed.json",
  "items": [
    {
      "id": "https://www.trackawesomelist.com/2021/05/04/",
      "title": "Awesome Xai Updates on May 04, 2021",
      "_short_title": "May 04, 2021",
      "_slug": "2021/05/04/",
      "summary": "2 awesome projects updated on May 04, 2021",
      "_filepath": "/content/2021/05/04/README.md",
      "url": "https://www.trackawesomelist.com/2021/05/04/",
      "date_published": "2021-05-04T11:37:06.000Z",
      "date_modified": "2021-05-04T13:47:03.000Z",
      "content_text": "\n\n### Repositories / Critiques\n\n*   [MAIF/shapash (⭐2.9k)](https://github.com/MAIF/shapash) - SHAP and LIME-based front-end explainer.\n*   [slundberg/shap (⭐24k)](https://github.com/slundberg/shap) - A Python module for using Shapley Additive Explanations.",
      "content_html": "<h3><p>Repositories / Critiques</p>\n</h3>\n<ul>\n<li><a href=\"https://github.com/MAIF/shapash\" rel=\"noopener noreferrer\">MAIF/shapash (⭐2.9k)</a> - SHAP and LIME-based front-end explainer.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://github.com/slundberg/shap\" rel=\"noopener noreferrer\">slundberg/shap (⭐24k)</a> - A Python module for using Shapley Additive Explanations.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2021/04/05/",
      "title": "Awesome Xai Updates on Apr 05, 2021",
      "_short_title": "Apr 05, 2021",
      "_slug": "2021/04/05/",
      "summary": "1 awesome projects updated on Apr 05, 2021",
      "_filepath": "/content/2021/04/05/README.md",
      "url": "https://www.trackawesomelist.com/2021/04/05/",
      "date_published": "2021-04-05T14:17:46.000Z",
      "date_modified": "2021-04-05T14:17:46.000Z",
      "content_text": "\n\n### Follow / Critiques\n\n*   [Rich Caruana](https://www.microsoft.com/en-us/research/people/rcaruana/) - The man behind Explainable Boosting Machines.",
      "content_html": "<h3><p>Follow / Critiques</p>\n</h3>\n<ul>\n<li><a href=\"https://www.microsoft.com/en-us/research/people/rcaruana/\" rel=\"noopener noreferrer\">Rich Caruana</a> - The man behind Explainable Boosting Machines.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2021/03/23/",
      "title": "Awesome Xai Updates on Mar 23, 2021",
      "_short_title": "Mar 23, 2021",
      "_slug": "2021/03/23/",
      "summary": "9 awesome projects updated on Mar 23, 2021",
      "_filepath": "/content/2021/03/23/README.md",
      "url": "https://www.trackawesomelist.com/2021/03/23/",
      "date_published": "2021-03-23T15:33:08.000Z",
      "date_modified": "2021-03-23T15:33:08.000Z",
      "content_text": "\n\n### Papers / Interpretable Models\n\n*   [Decision List](https://christophm.github.io/interpretable-ml-book/rules.html) - Like a decision tree with no branches.\n*   [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree) - The tree provides an interpretation.\n*   [Explainable Boosting Machine](https://www.youtube.com/watch?v=MREiHgHgl0k) - Method that predicts based on learned vector graphs of features.\n*   [k-Nearest Neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) - The prototypical clustering method.\n*   [Linear Regression](https://en.wikipedia.org/wiki/Linear_regression) - Easily plottable and understandable regression.\n*   [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) - Easily plottable and understandable classification.\n*   [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) - Good classification, poor estimation using conditional probabilities.\n*   [RuleFit](https://christophm.github.io/interpretable-ml-book/rulefit.html) - Sparse linear model as decision rules including feature interactions.\n\n### Videos / Critiques\n\n*   [Debate: Interpretability is necessary for ML](https://www.youtube.com/watch?v=93Xv8vJ2acI) - A debate on whether interpretability is necessary for ML with Rich Caruana and Patrice Simard for and Kilian Weinberger and Yann LeCun against.",
      "content_html": "<h3><p>Papers / Interpretable Models</p>\n</h3>\n<ul>\n<li><a href=\"https://christophm.github.io/interpretable-ml-book/rules.html\" rel=\"noopener noreferrer\">Decision List</a> - Like a decision tree with no branches.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Decision_tree\" rel=\"noopener noreferrer\">Decision Trees</a> - The tree provides an interpretation.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=MREiHgHgl0k\" rel=\"noopener noreferrer\">Explainable Boosting Machine</a> - Method that predicts based on learned vector graphs of features.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\" rel=\"noopener noreferrer\">k-Nearest Neighbors</a> - The prototypical clustering method.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Linear_regression\" rel=\"noopener noreferrer\">Linear Regression</a> - Easily plottable and understandable regression.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" rel=\"noopener noreferrer\">Logistic Regression</a> - Easily plottable and understandable classification.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier\" rel=\"noopener noreferrer\">Naive Bayes</a> - Good classification, poor estimation using conditional probabilities.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://christophm.github.io/interpretable-ml-book/rulefit.html\" rel=\"noopener noreferrer\">RuleFit</a> - Sparse linear model as decision rules including feature interactions.</li>\n</ul>\n<h3><p>Videos / Critiques</p>\n</h3>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=93Xv8vJ2acI\" rel=\"noopener noreferrer\">Debate: Interpretability is necessary for ML</a> - A debate on whether interpretability is necessary for ML with Rich Caruana and Patrice Simard for and Kilian Weinberger and Yann LeCun against.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2021/03/17/",
      "title": "Awesome Xai Updates on Mar 17, 2021",
      "_short_title": "Mar 17, 2021",
      "_slug": "2021/03/17/",
      "summary": "87 awesome projects updated on Mar 17, 2021",
      "_filepath": "/content/2021/03/17/README.md",
      "url": "https://www.trackawesomelist.com/2021/03/17/",
      "date_published": "2021-03-17T14:38:10.000Z",
      "date_modified": "2021-03-17T17:37:11.000Z",
      "content_text": "\n\n### Papers / Landmarks\n\n*   [Explanation in Artificial Intelligence: Insights from the Social Sciences](https://arxiv.org/abs/1706.07269) - This paper provides an introduction to the social science research into explanations. The author provides 4 major findings: (1) explanations are constrastive, (2) explanations are selected, (3) probabilities probably don't matter, (4) explanations are social. These fit into the general theme that explanations are -contextual-.\n*   [Sanity Checks for Saliency Maps](https://arxiv.org/abs/1810.03292) - An important read for anyone using saliency maps. This paper proposes two experiments to determine whether saliency maps are useful: (1) model parameter randomization test compares maps from trained and untrained models, (2) data randomization test compares maps from models trained on the original dataset and models trained on the same dataset with randomized labels. They find that \"some widely deployed saliency methods are independent of both the data the model was trained on, and the model parameters\".\n\n### Papers / Surveys\n\n*   [Explainable Deep Learning: A Field Guide for the Uninitiated](https://arxiv.org/abs/2004.14545) - An in-depth description of XAI focused on technqiues for deep learning.\n\n### Papers / Evaluations\n\n*   [Quantifying Explainability of Saliency Methods in Deep Neural Networks](https://arxiv.org/abs/2009.02899) - An analysis of how different heatmap-based saliency methods perform based on experimentation with a generated dataset.\n\n### Papers / XAI Methods\n\n*   [Ada-SISE](https://arxiv.org/abs/2102.07799) - Adaptive semantice inpute sampling for explanation.\n*   [ALE](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12377) - Accumulated local effects plot.\n*   [ALIME](https://link.springer.com/chapter/10.1007/978-3-030-33607-3_49) - Autoencoder Based Approach for Local Interpretability.\n*   [Anchors](https://ojs.aaai.org/index.php/AAAI/article/view/11491) - High-Precision Model-Agnostic Explanations.\n*   [Auditing](https://link.springer.com/article/10.1007/s10115-017-1116-3) - Auditing black-box models.\n*   [BayLIME](https://arxiv.org/abs/2012.03058) - Bayesian local interpretable model-agnostic explanations.\n*   [Break Down](http://ema.drwhy.ai/breakDown.html#BDMethod) - Break down plots for additive attributions.\n*   [CAM](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf) - Class activation mapping.\n*   [CDT](https://ieeexplore.ieee.org/abstract/document/4167900) - Confident interpretation of Bayesian decision tree ensembles.\n*   [CICE](https://christophm.github.io/interpretable-ml-book/ice.html) - Centered ICE plot.\n*   [CMM](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.40.2710\\&rep=rep1\\&type=pdf) - Combined multiple models metalearner.\n*   [Conj Rules](https://www.sciencedirect.com/science/article/pii/B9781558603356500131) - Using sampling and queries to extract rules from trained neural networks.\n*   [CP](https://ieeexplore.ieee.org/abstract/document/6597214) - Contribution propogation.\n*   [DecText](https://dl.acm.org/doi/abs/10.1145/775047.775113) - Extracting decision trees from trained neural networks.\n*   [DeepLIFT](https://ieeexplore-ieee-org.ezproxy.libraries.wright.edu/abstract/document/9352498) - Deep label-specific feature learning for image annotation.\n*   [DTD](https://www.sciencedirect.com/science/article/pii/S0031320316303582) - Deep Taylor decomposition.\n*   [ExplainD](https://www.aaai.org/Papers/IAAI/2006/IAAI06-018.pdf) - Explanations of evidence in additive classifiers.\n*   [FIRM](https://link.springer.com/chapter/10.1007/978-3-642-04174-7_45) - Feature importance ranking measure.\n*   [Fong, et. al.](https://openaccess.thecvf.com/content_iccv_2017/html/Fong_Interpretable_Explanations_of_ICCV_2017_paper.html) - Meaninful perturbations model.\n*   [G-REX](https://www.academia.edu/download/51462700/s0362-546x_2896_2900267-220170122-9600-1njrpyx.pdf) - Rule extraction using genetic algorithms.\n*   [Gibbons, et. al.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3977175/) - Explain random forest using decision tree.\n*   [GoldenEye](https://link-springer-com.ezproxy.libraries.wright.edu/article/10.1007/s10618-014-0368-8) - Exploring classifiers by randomization.\n*   [GPD](https://arxiv.org/abs/0912.1128) - Gaussian process decisions.\n*   [GPDT](https://ieeexplore.ieee.org/abstract/document/4938655) - Genetic program to evolve decision trees.\n*   [GradCAM](https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html) - Gradient-weighted Class Activation Mapping.\n*   [GradCAM++](https://ieeexplore.ieee.org/abstract/document/8354201/) - Generalized gradient-based visual explanations.\n*   [Hara, et. al.](https://arxiv.org/abs/1606.05390) - Making tree ensembles interpretable.\n*   [ICE](https://www.tandfonline.com/doi/abs/10.1080/10618600.2014.907095) - Individual conditional expectation plots.\n*   [IG](http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf) - Integrated gradients.\n*   [inTrees](https://link.springer.com/article/10.1007/s41060-018-0144-8) - Interpreting tree ensembles with inTrees.\n*   [IOFP](https://arxiv.org/abs/1611.04967) - Iterative orthoganol feature projection.\n*   [IP](https://arxiv.org/abs/1703.00810) - Information plane visualization.\n*   [KL-LIME](https://arxiv.org/abs/1810.02678) - Kullback-Leibler Projections based LIME.\n*   [Krishnan, et. al.](https://www.sciencedirect.com/science/article/abs/pii/S0031320398001812) - Extracting decision trees from trained neural networks.\n*   [Lei, et. al.](https://arxiv.org/abs/1606.04155) - Rationalizing neural predictions with generator and encoder.\n*   [LIME](https://dl.acm.org/doi/abs/10.1145/2939672.2939778) - Local Interpretable Model-Agnostic Explanations.\n*   [LOCO](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2017.1307116#.YEkdZ7CSmUk) - Leave-one covariate out.\n*   [LORE](https://arxiv.org/abs/1805.10820) - Local rule-based explanations.\n*   [Lou, et. al.](https://dl.acm.org/doi/abs/10.1145/2487575.2487579) - Accurate intelligibile models with pairwise interactions.\n*   [LRP](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140) - Layer-wise relevance propogation.\n*   [MCR](https://www.jmlr.org/papers/volume20/18-760/18-760.pdf) - Model class reliance.\n*   [MES](https://ieeexplore.ieee.org/abstract/document/7738872) - Model explanation system.\n*   [MFI](https://arxiv.org/abs/1611.07567) - Feature importance measure for non-linear algorithms.\n*   [NID](https://www.sciencedirect.com/science/article/abs/pii/S0304380002000649) - Neural interpretation diagram.\n*   [OptiLIME](https://arxiv.org/abs/2006.05714) - Optimized LIME.\n*   [PALM](https://dl.acm.org/doi/abs/10.1145/3077257.3077271) - Partition aware local model.\n*   [PDA](https://arxiv.org/abs/1702.04595) - Prediction Difference Analysis: Visualize deep neural network decisions.\n*   [PDP](https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451) - Partial dependence plots.\n*   [POIMs](https://academic.oup.com/bioinformatics/article/24/13/i6/233341) - Positional oligomer importance matrices for understanding SVM signal detectors.\n*   [ProfWeight](https://arxiv.org/abs/1807.07506) - Transfer information from deep network to simpler model.\n*   [Prospector](https://dl.acm.org/doi/abs/10.1145/2858036.2858529) - Interactive partial dependence diagnostics.\n*   [QII](https://ieeexplore.ieee.org/abstract/document/7546525) - Quantitative input influence.\n*   [REFNE](https://content.iospress.com/articles/ai-communications/aic272) - Extracting symbolic rules from trained neural network ensembles.\n*   [RETAIN](https://arxiv.org/abs/1608.05745) - Reverse time attention model.\n*   [RISE](https://arxiv.org/abs/1806.07421) - Randomized input sampling for explanation.\n*   [RxREN](https://link.springer.com/article/10.1007%2Fs11063-011-9207-8) - Reverse engineering neural networks for rule extraction.\n*   [SHAP](https://arxiv.org/abs/1705.07874) - A unified approach to interpretting model predictions.\n*   [SIDU](https://arxiv.org/abs/2101.10710) - Similarity, difference, and uniqueness input perturbation.\n*   [Simonynan, et. al](https://arxiv.org/abs/1312.6034) - Visualizing CNN classes.\n*   [Singh, et. al](https://arxiv.org/abs/1611.07579) - Programs as black-box explanations.\n*   [STA](https://arxiv.org/abs/1610.09036) - Interpreting models via Single Tree Approximation.\n*   [Strumbelj, et. al.](https://www.jmlr.org/papers/volume11/strumbelj10a/strumbelj10a.pdf) - Explanation of individual classifications using game theory.\n*   [SVM+P](https://www.academia.edu/download/2471122/3uecwtv9xcwxg6r.pdf) - Rule extraction from support vector machines.\n*   [TCAV](https://openreview.net/forum?id=S1viikbCW) - Testing with concept activation vectors.\n*   [Tolomei, et. al.](https://dl.acm.org/doi/abs/10.1145/3097983.3098039) - Interpretable predictions of tree-ensembles via actionable feature tweaking.\n*   [Tree Metrics](https://www.researchgate.net/profile/Edward-George-2/publication/2610587_Making_Sense_of_a_Forest_of_Trees/links/55b1085d08aec0e5f430eb40/Making-Sense-of-a-Forest-of-Trees.pdf) - Making sense of a forest of trees.\n*   [TreeSHAP](https://arxiv.org/abs/1706.06060) - Consistent feature attribute for tree ensembles.\n*   [TreeView](https://arxiv.org/abs/1611.07429) - Feature-space partitioning.\n*   [TREPAN](http://www.inf.ufrgs.br/~engel/data/media/file/cmp121/TREPAN_craven.nips96.pdf) - Extracting tree-structured representations of trained networks.\n*   [TSP](https://dl.acm.org/doi/abs/10.1145/3412815.3416893) - Tree space prototypes.\n*   [VBP](http://www.columbia.edu/~aec2163/NonFlash/Papers/VisualBackProp.pdf) - Visual back-propagation.\n*   [VEC](https://ieeexplore.ieee.org/abstract/document/5949423) - Variable effect characteristic curve.\n*   [VIN](https://dl.acm.org/doi/abs/10.1145/1014052.1014122) - Variable interaction network.\n*   [X-TREPAN](https://arxiv.org/abs/1508.07551) - Adapted etraction of comprehensible decision tree in ANNs.\n*   [Xu, et. al.](http://proceedings.mlr.press/v37/xuc15) - Show, attend, tell attention model.\n\n### Papers / Critiques\n\n*   [Attention is not Explanation](https://arxiv.org/abs/1902.10186) - Authors perform a series of NLP experiments which argue attention does not provide meaningful explanations. They also demosntrate that different attentions can generate similar model outputs.\n*   [Attention is not --not-- Explanation](https://arxiv.org/abs/1908.04626) - This is a rebutal to the above paper. Authors argue that multiple explanations can be valid and that the and that attention can produce *a* valid explanation, if not -the- valid explanation.\n*   [Do Not Trust Additive Explanations](https://arxiv.org/abs/1903.11420) - Authors argue that addditive explanations (e.g. LIME, SHAP, Break Down) fail to take feature ineractions into account and are thus unreliable.\n*   [Please Stop Permuting Features An Explanation and Alternatives](https://arxiv.org/abs/1905.03151) - Authors demonstrate why permuting features is misleading, especially where there is strong feature dependence. They offer several previously described alternatives.\n*   [Stop Explaining Black Box Machine Learning Models for High States Decisions and Use Interpretable Models Instead](https://www.nature.com/articles/s42256-019-0048-x?fbclid=IwAR3156gP-ntoAyw2sHTXo0Z8H9p-2wBKe5jqitsMCdft7xA0P766QvSthFs) - Authors present a number of issues with explainable ML and challenges to interpretable ML: (1) constructing optimal logical models, (2) constructing optimal sparse scoring systems, (3) defining interpretability and creating methods for specific methods. They also offer an argument for why interpretable models might exist in many different domains.\n*   [The (Un)reliability of Saliency Methods](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_14) - Authors demonstrate how saliency methods vary attribution when adding a constant shift to the input data. They argue that methods should fulfill *input invariance*, that a saliency method mirror the sensistivity of the model with respect to transformations of the input.\n\n### Follow / Critiques\n\n*   [The Institute for Ethical AI & Machine Learning](https://ethical.institute/index.html) - A UK-based research center that performs research into ethical AI/ML, which frequently involves XAI.\n*   [Tim Miller](https://twitter.com/tmiller_unimelb) - One of the preeminent researchers in XAI.",
      "content_html": "<h3><p>Papers / Landmarks</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1706.07269\" rel=\"noopener noreferrer\">Explanation in Artificial Intelligence: Insights from the Social Sciences</a> - This paper provides an introduction to the social science research into explanations. The author provides 4 major findings: (1) explanations are constrastive, (2) explanations are selected, (3) probabilities probably don't matter, (4) explanations are social. These fit into the general theme that explanations are -contextual-.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1810.03292\" rel=\"noopener noreferrer\">Sanity Checks for Saliency Maps</a> - An important read for anyone using saliency maps. This paper proposes two experiments to determine whether saliency maps are useful: (1) model parameter randomization test compares maps from trained and untrained models, (2) data randomization test compares maps from models trained on the original dataset and models trained on the same dataset with randomized labels. They find that \"some widely deployed saliency methods are independent of both the data the model was trained on, and the model parameters\".</li>\n</ul>\n<h3><p>Papers / Surveys</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2004.14545\" rel=\"noopener noreferrer\">Explainable Deep Learning: A Field Guide for the Uninitiated</a> - An in-depth description of XAI focused on technqiues for deep learning.</li>\n</ul>\n<h3><p>Papers / Evaluations</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2009.02899\" rel=\"noopener noreferrer\">Quantifying Explainability of Saliency Methods in Deep Neural Networks</a> - An analysis of how different heatmap-based saliency methods perform based on experimentation with a generated dataset.</li>\n</ul>\n<h3><p>Papers / XAI Methods</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2102.07799\" rel=\"noopener noreferrer\">Ada-SISE</a> - Adaptive semantice inpute sampling for explanation.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12377\" rel=\"noopener noreferrer\">ALE</a> - Accumulated local effects plot.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://link.springer.com/chapter/10.1007/978-3-030-33607-3_49\" rel=\"noopener noreferrer\">ALIME</a> - Autoencoder Based Approach for Local Interpretability.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://ojs.aaai.org/index.php/AAAI/article/view/11491\" rel=\"noopener noreferrer\">Anchors</a> - High-Precision Model-Agnostic Explanations.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://link.springer.com/article/10.1007/s10115-017-1116-3\" rel=\"noopener noreferrer\">Auditing</a> - Auditing black-box models.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/2012.03058\" rel=\"noopener noreferrer\">BayLIME</a> - Bayesian local interpretable model-agnostic explanations.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://ema.drwhy.ai/breakDown.html#BDMethod\" rel=\"noopener noreferrer\">Break Down</a> - Break down plots for additive attributions.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf\" rel=\"noopener noreferrer\">CAM</a> - Class activation mapping.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/abstract/document/4167900\" rel=\"noopener noreferrer\">CDT</a> - Confident interpretation of Bayesian decision tree ensembles.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://christophm.github.io/interpretable-ml-book/ice.html\" rel=\"noopener noreferrer\">CICE</a> - Centered ICE plot.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.40.2710&amp;rep=rep1&amp;type=pdf\" rel=\"noopener noreferrer\">CMM</a> - Combined multiple models metalearner.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.sciencedirect.com/science/article/pii/B9781558603356500131\" rel=\"noopener noreferrer\">Conj Rules</a> - Using sampling and queries to extract rules from trained neural networks.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/abstract/document/6597214\" rel=\"noopener noreferrer\">CP</a> - Contribution propogation.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://dl.acm.org/doi/abs/10.1145/775047.775113\" rel=\"noopener noreferrer\">DecText</a> - Extracting decision trees from trained neural networks.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://ieeexplore-ieee-org.ezproxy.libraries.wright.edu/abstract/document/9352498\" rel=\"noopener noreferrer\">DeepLIFT</a> - Deep label-specific feature learning for image annotation.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.sciencedirect.com/science/article/pii/S0031320316303582\" rel=\"noopener noreferrer\">DTD</a> - Deep Taylor decomposition.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.aaai.org/Papers/IAAI/2006/IAAI06-018.pdf\" rel=\"noopener noreferrer\">ExplainD</a> - Explanations of evidence in additive classifiers.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://link.springer.com/chapter/10.1007/978-3-642-04174-7_45\" rel=\"noopener noreferrer\">FIRM</a> - Feature importance ranking measure.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://openaccess.thecvf.com/content_iccv_2017/html/Fong_Interpretable_Explanations_of_ICCV_2017_paper.html\" rel=\"noopener noreferrer\">Fong, et. al.</a> - Meaninful perturbations model.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.academia.edu/download/51462700/s0362-546x_2896_2900267-220170122-9600-1njrpyx.pdf\" rel=\"noopener noreferrer\">G-REX</a> - Rule extraction using genetic algorithms.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3977175/\" rel=\"noopener noreferrer\">Gibbons, et. al.</a> - Explain random forest using decision tree.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://link-springer-com.ezproxy.libraries.wright.edu/article/10.1007/s10618-014-0368-8\" rel=\"noopener noreferrer\">GoldenEye</a> - Exploring classifiers by randomization.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/0912.1128\" rel=\"noopener noreferrer\">GPD</a> - Gaussian process decisions.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/abstract/document/4938655\" rel=\"noopener noreferrer\">GPDT</a> - Genetic program to evolve decision trees.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html\" rel=\"noopener noreferrer\">GradCAM</a> - Gradient-weighted Class Activation Mapping.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/abstract/document/8354201/\" rel=\"noopener noreferrer\">GradCAM++</a> - Generalized gradient-based visual explanations.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1606.05390\" rel=\"noopener noreferrer\">Hara, et. al.</a> - Making tree ensembles interpretable.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.tandfonline.com/doi/abs/10.1080/10618600.2014.907095\" rel=\"noopener noreferrer\">ICE</a> - Individual conditional expectation plots.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf\" rel=\"noopener noreferrer\">IG</a> - Integrated gradients.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://link.springer.com/article/10.1007/s41060-018-0144-8\" rel=\"noopener noreferrer\">inTrees</a> - Interpreting tree ensembles with inTrees.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1611.04967\" rel=\"noopener noreferrer\">IOFP</a> - Iterative orthoganol feature projection.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1703.00810\" rel=\"noopener noreferrer\">IP</a> - Information plane visualization.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1810.02678\" rel=\"noopener noreferrer\">KL-LIME</a> - Kullback-Leibler Projections based LIME.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0031320398001812\" rel=\"noopener noreferrer\">Krishnan, et. al.</a> - Extracting decision trees from trained neural networks.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1606.04155\" rel=\"noopener noreferrer\">Lei, et. al.</a> - Rationalizing neural predictions with generator and encoder.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://dl.acm.org/doi/abs/10.1145/2939672.2939778\" rel=\"noopener noreferrer\">LIME</a> - Local Interpretable Model-Agnostic Explanations.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2017.1307116#.YEkdZ7CSmUk\" rel=\"noopener noreferrer\">LOCO</a> - Leave-one covariate out.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1805.10820\" rel=\"noopener noreferrer\">LORE</a> - Local rule-based explanations.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://dl.acm.org/doi/abs/10.1145/2487575.2487579\" rel=\"noopener noreferrer\">Lou, et. al.</a> - Accurate intelligibile models with pairwise interactions.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140\" rel=\"noopener noreferrer\">LRP</a> - Layer-wise relevance propogation.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.jmlr.org/papers/volume20/18-760/18-760.pdf\" rel=\"noopener noreferrer\">MCR</a> - Model class reliance.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/abstract/document/7738872\" rel=\"noopener noreferrer\">MES</a> - Model explanation system.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1611.07567\" rel=\"noopener noreferrer\">MFI</a> - Feature importance measure for non-linear algorithms.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0304380002000649\" rel=\"noopener noreferrer\">NID</a> - Neural interpretation diagram.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/2006.05714\" rel=\"noopener noreferrer\">OptiLIME</a> - Optimized LIME.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://dl.acm.org/doi/abs/10.1145/3077257.3077271\" rel=\"noopener noreferrer\">PALM</a> - Partition aware local model.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1702.04595\" rel=\"noopener noreferrer\">PDA</a> - Prediction Difference Analysis: Visualize deep neural network decisions.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451\" rel=\"noopener noreferrer\">PDP</a> - Partial dependence plots.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://academic.oup.com/bioinformatics/article/24/13/i6/233341\" rel=\"noopener noreferrer\">POIMs</a> - Positional oligomer importance matrices for understanding SVM signal detectors.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1807.07506\" rel=\"noopener noreferrer\">ProfWeight</a> - Transfer information from deep network to simpler model.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://dl.acm.org/doi/abs/10.1145/2858036.2858529\" rel=\"noopener noreferrer\">Prospector</a> - Interactive partial dependence diagnostics.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/abstract/document/7546525\" rel=\"noopener noreferrer\">QII</a> - Quantitative input influence.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://content.iospress.com/articles/ai-communications/aic272\" rel=\"noopener noreferrer\">REFNE</a> - Extracting symbolic rules from trained neural network ensembles.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1608.05745\" rel=\"noopener noreferrer\">RETAIN</a> - Reverse time attention model.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1806.07421\" rel=\"noopener noreferrer\">RISE</a> - Randomized input sampling for explanation.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://link.springer.com/article/10.1007%2Fs11063-011-9207-8\" rel=\"noopener noreferrer\">RxREN</a> - Reverse engineering neural networks for rule extraction.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1705.07874\" rel=\"noopener noreferrer\">SHAP</a> - A unified approach to interpretting model predictions.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/2101.10710\" rel=\"noopener noreferrer\">SIDU</a> - Similarity, difference, and uniqueness input perturbation.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1312.6034\" rel=\"noopener noreferrer\">Simonynan, et. al</a> - Visualizing CNN classes.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1611.07579\" rel=\"noopener noreferrer\">Singh, et. al</a> - Programs as black-box explanations.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1610.09036\" rel=\"noopener noreferrer\">STA</a> - Interpreting models via Single Tree Approximation.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.jmlr.org/papers/volume11/strumbelj10a/strumbelj10a.pdf\" rel=\"noopener noreferrer\">Strumbelj, et. al.</a> - Explanation of individual classifications using game theory.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.academia.edu/download/2471122/3uecwtv9xcwxg6r.pdf\" rel=\"noopener noreferrer\">SVM+P</a> - Rule extraction from support vector machines.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://openreview.net/forum?id=S1viikbCW\" rel=\"noopener noreferrer\">TCAV</a> - Testing with concept activation vectors.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://dl.acm.org/doi/abs/10.1145/3097983.3098039\" rel=\"noopener noreferrer\">Tolomei, et. al.</a> - Interpretable predictions of tree-ensembles via actionable feature tweaking.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.researchgate.net/profile/Edward-George-2/publication/2610587_Making_Sense_of_a_Forest_of_Trees/links/55b1085d08aec0e5f430eb40/Making-Sense-of-a-Forest-of-Trees.pdf\" rel=\"noopener noreferrer\">Tree Metrics</a> - Making sense of a forest of trees.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1706.06060\" rel=\"noopener noreferrer\">TreeSHAP</a> - Consistent feature attribute for tree ensembles.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1611.07429\" rel=\"noopener noreferrer\">TreeView</a> - Feature-space partitioning.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://www.inf.ufrgs.br/~engel/data/media/file/cmp121/TREPAN_craven.nips96.pdf\" rel=\"noopener noreferrer\">TREPAN</a> - Extracting tree-structured representations of trained networks.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://dl.acm.org/doi/abs/10.1145/3412815.3416893\" rel=\"noopener noreferrer\">TSP</a> - Tree space prototypes.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://www.columbia.edu/~aec2163/NonFlash/Papers/VisualBackProp.pdf\" rel=\"noopener noreferrer\">VBP</a> - Visual back-propagation.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/abstract/document/5949423\" rel=\"noopener noreferrer\">VEC</a> - Variable effect characteristic curve.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://dl.acm.org/doi/abs/10.1145/1014052.1014122\" rel=\"noopener noreferrer\">VIN</a> - Variable interaction network.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1508.07551\" rel=\"noopener noreferrer\">X-TREPAN</a> - Adapted etraction of comprehensible decision tree in ANNs.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://proceedings.mlr.press/v37/xuc15\" rel=\"noopener noreferrer\">Xu, et. al.</a> - Show, attend, tell attention model.</li>\n</ul>\n<h3><p>Papers / Critiques</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1902.10186\" rel=\"noopener noreferrer\">Attention is not Explanation</a> - Authors perform a series of NLP experiments which argue attention does not provide meaningful explanations. They also demosntrate that different attentions can generate similar model outputs.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1908.04626\" rel=\"noopener noreferrer\">Attention is not --not-- Explanation</a> - This is a rebutal to the above paper. Authors argue that multiple explanations can be valid and that the and that attention can produce <em>a</em> valid explanation, if not -the- valid explanation.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1903.11420\" rel=\"noopener noreferrer\">Do Not Trust Additive Explanations</a> - Authors argue that addditive explanations (e.g. LIME, SHAP, Break Down) fail to take feature ineractions into account and are thus unreliable.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1905.03151\" rel=\"noopener noreferrer\">Please Stop Permuting Features An Explanation and Alternatives</a> - Authors demonstrate why permuting features is misleading, especially where there is strong feature dependence. They offer several previously described alternatives.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.nature.com/articles/s42256-019-0048-x?fbclid=IwAR3156gP-ntoAyw2sHTXo0Z8H9p-2wBKe5jqitsMCdft7xA0P766QvSthFs\" rel=\"noopener noreferrer\">Stop Explaining Black Box Machine Learning Models for High States Decisions and Use Interpretable Models Instead</a> - Authors present a number of issues with explainable ML and challenges to interpretable ML: (1) constructing optimal logical models, (2) constructing optimal sparse scoring systems, (3) defining interpretability and creating methods for specific methods. They also offer an argument for why interpretable models might exist in many different domains.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://link.springer.com/chapter/10.1007/978-3-030-28954-6_14\" rel=\"noopener noreferrer\">The (Un)reliability of Saliency Methods</a> - Authors demonstrate how saliency methods vary attribution when adding a constant shift to the input data. They argue that methods should fulfill <em>input invariance</em>, that a saliency method mirror the sensistivity of the model with respect to transformations of the input.</li>\n</ul>\n<h3><p>Follow / Critiques</p>\n</h3>\n<ul>\n<li><a href=\"https://ethical.institute/index.html\" rel=\"noopener noreferrer\">The Institute for Ethical AI &amp; Machine Learning</a> - A UK-based research center that performs research into ethical AI/ML, which frequently involves XAI.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://twitter.com/tmiller_unimelb\" rel=\"noopener noreferrer\">Tim Miller</a> - One of the preeminent researchers in XAI.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2021/03/02/",
      "title": "Awesome Xai Updates on Mar 02, 2021",
      "_short_title": "Mar 02, 2021",
      "_slug": "2021/03/02/",
      "summary": "2 awesome projects updated on Mar 02, 2021",
      "_filepath": "/content/2021/03/02/README.md",
      "url": "https://www.trackawesomelist.com/2021/03/02/",
      "date_published": "2021-03-02T15:56:11.000Z",
      "date_modified": "2021-03-02T16:01:13.000Z",
      "content_text": "\n\n### Repositories / Critiques\n\n*   [EthicalML/xai (⭐1.2k)](https://github.com/EthicalML/xai) - A toolkit for XAI which is focused exclusively on tabular data. It implements a variety of data and model evaluation techniques.\n*   [PAIR-code/what-if-tool (⭐962)](https://github.com/PAIR-code/what-if-tool) - A tool for Tensorboard or Notebooks which allows investigating model performance and fairness.",
      "content_html": "<h3><p>Repositories / Critiques</p>\n</h3>\n<ul>\n<li><a href=\"https://github.com/EthicalML/xai\" rel=\"noopener noreferrer\">EthicalML/xai (⭐1.2k)</a> - A toolkit for XAI which is focused exclusively on tabular data. It implements a variety of data and model evaluation techniques.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://github.com/PAIR-code/what-if-tool\" rel=\"noopener noreferrer\">PAIR-code/what-if-tool (⭐962)</a> - A tool for Tensorboard or Notebooks which allows investigating model performance and fairness.</li>\n</ul>\n"
    }
  ]
}
