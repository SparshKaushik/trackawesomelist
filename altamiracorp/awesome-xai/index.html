<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <title>Track Awesome Xai (altamiracorp/awesome-xai) Updates Daily - Track Awesome List</title>
    <meta property="og:url" content="https://www.trackawesomelist.com/altamiracorp/awesome-xai/" />
    <meta property="og:type" content="summary" />
    <meta property="og:title" content="Track Awesome Xai Updates Daily" />
    <meta property="og:description" content="Awesome Explainable AI (XAI) and Interpretable ML Papers and Resources" />
    <meta property="og:site_name" content="Track Awesome List" />
    <style>
      main {
        max-width: 1024px;
        margin: 0 auto;
        padding: 0 0.5em;
      }
      :root,[data-color-mode=light][data-light-theme=light],[data-color-mode=dark][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=light]{--color-canvas-default-transparent:rgba(255,255,255,0);--color-prettylights-syntax-comment:#6e7781;--color-prettylights-syntax-constant:#0550ae;--color-prettylights-syntax-entity:#8250df;--color-prettylights-syntax-storage-modifier-import:#24292f;--color-prettylights-syntax-entity-tag:#116329;--color-prettylights-syntax-keyword:#cf222e;--color-prettylights-syntax-string:#0a3069;--color-prettylights-syntax-variable:#953800;--color-prettylights-syntax-string-regexp:#116329;--color-prettylights-syntax-constant-other-reference-link:#0a3069;--color-fg-default:#24292f;--color-fg-muted:#57606a;--color-canvas-default:#fff;--color-canvas-subtle:#f6f8fa;--color-border-default:#d0d7de;--color-border-muted:#d8dee4;--color-neutral-muted:rgba(175,184,193,.2);--color-accent-fg:#0969da;--color-accent-emphasis:#0969da;--color-danger-fg:#cf222e}}[data-color-mode=light][data-light-theme=dark],[data-color-mode=dark][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}@media (prefers-color-scheme:light){[data-color-mode=auto][data-light-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}@media (prefers-color-scheme:dark){[data-color-mode=auto][data-dark-theme=dark]{--color-canvas-default-transparent:rgba(13,17,23,0);--color-prettylights-syntax-comment:#8b949e;--color-prettylights-syntax-constant:#79c0ff;--color-prettylights-syntax-entity:#d2a8ff;--color-prettylights-syntax-storage-modifier-import:#c9d1d9;--color-prettylights-syntax-entity-tag:#7ee787;--color-prettylights-syntax-keyword:#ff7b72;--color-prettylights-syntax-string:#a5d6ff;--color-prettylights-syntax-variable:#ffa657;--color-prettylights-syntax-string-regexp:#7ee787;--color-prettylights-syntax-constant-other-reference-link:#a5d6ff;--color-fg-default:#c9d1d9;--color-fg-muted:#8b949e;--color-canvas-default:#0d1117;--color-canvas-subtle:#161b22;--color-border-default:#30363d;--color-border-muted:#21262d;--color-neutral-muted:rgba(110,118,129,.4);--color-accent-fg:#58a6ff;--color-accent-emphasis:#1f6feb;--color-danger-fg:#f85149}}.markdown-body{word-wrap:break-word;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;font-size:16px;line-height:1.5}.markdown-body:before{content:"";display:table}.markdown-body:after{clear:both;content:"";display:table}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:var(--color-danger-fg)}.markdown-body .anchor{float:left;margin-left:-20px;padding-right:4px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body p,.markdown-body blockquote,.markdown-body ul,.markdown-body ol,.markdown-body dl,.markdown-body table,.markdown-body pre,.markdown-body details{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;background-color:var(--color-border-default);border:0;margin:24px 0;padding:0}.markdown-body blockquote{color:var(--color-fg-muted);border-left:.25em solid var(--color-border-default);padding:0 1em}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:var(--color-fg-default);vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 tt,.markdown-body h1 code,.markdown-body h2 tt,.markdown-body h2 code,.markdown-body h3 tt,.markdown-body h3 code,.markdown-body h4 tt,.markdown-body h4 code,.markdown-body h5 tt,.markdown-body h5 code,.markdown-body h6 tt,.markdown-body h6 code{font-size:inherit;padding:0 .2em}.markdown-body h1{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:2em}.markdown-body h2{border-bottom:1px solid var(--color-border-muted);padding-bottom:.3em;font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{color:var(--color-fg-muted);font-size:.85em}.markdown-body summary h1,.markdown-body summary h2,.markdown-body summary h3,.markdown-body summary h4,.markdown-body summary h5,.markdown-body summary h6{display:inline-block}.markdown-body summary h1 .anchor,.markdown-body summary h2 .anchor,.markdown-body summary h3 .anchor,.markdown-body summary h4 .anchor,.markdown-body summary h5 .anchor,.markdown-body summary h6 .anchor{margin-left:-40px}.markdown-body summary h1,.markdown-body summary h2{border-bottom:0;padding-bottom:0}.markdown-body ul,.markdown-body ol{padding-left:2em}.markdown-body ul.no-list,.markdown-body ol.no-list{padding:0;list-style-type:none}.markdown-body ol[type="1"]{list-style-type:decimal}.markdown-body ol[type=a]{list-style-type:lower-alpha}.markdown-body ol[type=i]{list-style-type:lower-roman}.markdown-body div>ol:not([type]){list-style-type:decimal}.markdown-body ul ul,.markdown-body ul ol,.markdown-body ol ol,.markdown-body ol ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{margin-top:16px;padding:0;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{margin-bottom:16px;padding:0 16px}.markdown-body table{width:100%;width:-webkit-max-content;width:-webkit-max-content;width:max-content;max-width:100%;display:block;overflow:auto}.markdown-body table th{font-weight:600}.markdown-body table th,.markdown-body table td{border:1px solid var(--color-border-default);padding:6px 13px}.markdown-body table tr{background-color:var(--color-canvas-default);border-top:1px solid var(--color-border-muted)}.markdown-body table tr:nth-child(2n){background-color:var(--color-canvas-subtle)}.markdown-body table img{background-color:rgba(0,0,0,0)}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:var(--color-canvas-default)}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:rgba(0,0,0,0)}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{float:left;width:auto;border:1px solid var(--color-border-default);margin:13px 0 0;padding:7px;display:block;overflow:hidden}.markdown-body span.frame span img{float:left;display:block}.markdown-body span.frame span span{clear:both;color:var(--color-fg-default);padding:5px 0 0;display:block}.markdown-body span.align-center{clear:both;display:block;overflow:hidden}.markdown-body span.align-center>span{text-align:center;margin:13px auto 0;display:block;overflow:hidden}.markdown-body span.align-center span img{text-align:center;margin:0 auto}.markdown-body span.align-right{clear:both;display:block;overflow:hidden}.markdown-body span.align-right>span{text-align:right;margin:13px 0 0;display:block;overflow:hidden}.markdown-body span.align-right span img{text-align:right;margin:0}.markdown-body span.float-left{float:left;margin-right:13px;display:block;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{float:right;margin-left:13px;display:block;overflow:hidden}.markdown-body span.float-right>span{text-align:right;margin:13px auto 0;display:block;overflow:hidden}.markdown-body code,.markdown-body tt{background-color:var(--color-neutral-muted);border-radius:6px;margin:0;padding:.2em .4em;font-size:85%}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{-webkit-text-decoration:inherit;-webkit-text-decoration:inherit;text-decoration:inherit}.markdown-body samp{font-size:85%}.markdown-body pre{word-wrap:normal}.markdown-body pre code{font-size:100%}.markdown-body pre>code{word-break:normal;white-space:pre;background:0 0;border:0;margin:0;padding:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{word-break:normal;margin-bottom:0}.markdown-body .highlight pre,.markdown-body pre{background-color:var(--color-canvas-subtle);border-radius:6px;padding:16px;font-size:85%;line-height:1.45;overflow:auto}.markdown-body pre code,.markdown-body pre tt{max-width:auto;line-height:inherit;word-wrap:normal;background-color:rgba(0,0,0,0);border:0;margin:0;padding:0;display:inline;overflow:visible}.markdown-body .csv-data td,.markdown-body .csv-data th{text-align:left;white-space:nowrap;padding:5px;font-size:12px;line-height:1;overflow:hidden}.markdown-body .csv-data .blob-num{text-align:right;background:var(--color-canvas-default);border:0;padding:10px 8px 9px}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{background:var(--color-canvas-subtle);border-top:0;font-weight:600}.markdown-body [data-footnote-ref]:before{content:"["}.markdown-body [data-footnote-ref]:after{content:"]"}.markdown-body .footnotes{color:var(--color-fg-muted);border-top:1px solid var(--color-border-default);font-size:12px}.markdown-body .footnotes ol{padding-left:16px}.markdown-body .footnotes li{position:relative}.markdown-body .footnotes li:target:before{pointer-events:none;content:"";border:2px solid var(--color-accent-emphasis);border-radius:6px;position:absolute;top:-8px;bottom:-8px;left:-24px;right:-8px}.markdown-body .footnotes li:target{color:var(--color-fg-default)}.markdown-body .footnotes .data-footnote-backref g-emoji{font-family:monospace}.markdown-body{background-color:var(--color-canvas-default);color:var(--color-fg-default)}.markdown-body a{color:var(--color-accent-fg);text-decoration:none}.markdown-body a:hover{text-decoration:underline}.markdown-body iframe{background-color:#fff;border:0;margin-bottom:16px}.markdown-body svg.octicon{fill:currentColor}.markdown-body .anchor>.octicon{display:inline}.markdown-body .highlight .token.keyword,.gfm-highlight .token.keyword{color:var(--color-prettylights-syntax-keyword)}.markdown-body .highlight .token.tag .token.class-name,.markdown-body .highlight .token.tag .token.script .token.punctuation,.gfm-highlight .token.tag .token.class-name,.gfm-highlight .token.tag .token.script .token.punctuation{color:var(--color-prettylights-syntax-storage-modifier-import)}.markdown-body .highlight .token.operator,.markdown-body .highlight .token.number,.markdown-body .highlight .token.boolean,.markdown-body .highlight .token.tag .token.punctuation,.markdown-body .highlight .token.tag .token.script .token.script-punctuation,.markdown-body .highlight .token.tag .token.attr-name,.gfm-highlight .token.operator,.gfm-highlight .token.number,.gfm-highlight .token.boolean,.gfm-highlight .token.tag .token.punctuation,.gfm-highlight .token.tag .token.script .token.script-punctuation,.gfm-highlight .token.tag .token.attr-name{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.function,.gfm-highlight .token.function{color:var(--color-prettylights-syntax-entity)}.markdown-body .highlight .token.string,.gfm-highlight .token.string{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.comment,.gfm-highlight .token.comment{color:var(--color-prettylights-syntax-comment)}.markdown-body .highlight .token.class-name,.gfm-highlight .token.class-name{color:var(--color-prettylights-syntax-variable)}.markdown-body .highlight .token.regex,.gfm-highlight .token.regex{color:var(--color-prettylights-syntax-string)}.markdown-body .highlight .token.regex .regex-delimiter,.gfm-highlight .token.regex .regex-delimiter{color:var(--color-prettylights-syntax-constant)}.markdown-body .highlight .token.tag .token.tag,.markdown-body .highlight .token.property,.gfm-highlight .token.tag .token.tag,.gfm-highlight .token.property{color:var(--color-prettylights-syntax-entity-tag)}
    </style>
  </head>
  <body>
    <main data-color-mode="light" data-light-theme="light" data-dark-theme="dark" class="markdown-body">
      <h1>Track Awesome Xai Updates Daily</h1>
<p>Awesome Explainable AI (XAI) and Interpretable ML Papers and Resources</p>
<p><a href="/">🏠 Home</a><span> · </span><a href="https://www.trackawesomelist.com/search/">🔍 Search</a><span> · </span><a href="https://www.trackawesomelist.com/altamiracorp/awesome-xai/rss.xml">🔥 Feed</a><span> · </span><a href="https://trackawesomelist.us17.list-manage.com/subscribe?u=d2f0117aa829c83a63ec63c2f&id=36a103854c">📮 Subscribe</a><span> · </span><a href="https://github.com/sponsors/theowenyoung">❤️  Sponsor</a><span> · </span><a href="https://github.com/altamiracorp/awesome-xai">😺 altamiracorp/awesome-xai</a><span> · </span><span>⭐ 165</span><span> · </span><span>🏷️ Computer Science</span></p>
<p><span>[ </span><span>Daily</span><span> / </span><a href="/altamiracorp/awesome-xai/week/">Weekly</a><span> / </span><a href="/altamiracorp/awesome-xai/readme/">Overview</a><span> ]</span></p>

<h2><a href="https://www.trackawesomelist.com/2021/05/04/">May 04, 2021</a></h2><h3><p>Repositories / Critiques</p>
</h3>
<ul>
<li><a href="https://github.com/MAIF/shapash" rel="noopener noreferrer">MAIF/shapash (⭐2.9k)</a> - SHAP and LIME-based front-end explainer.</li>
</ul>

<ul>
<li><a href="https://github.com/slundberg/shap" rel="noopener noreferrer">slundberg/shap (⭐24k)</a> - A Python module for using Shapley Additive Explanations.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2021/04/05/">Apr 05, 2021</a></h2><h3><p>Follow / Critiques</p>
</h3>
<ul>
<li><a href="https://www.microsoft.com/en-us/research/people/rcaruana/" rel="noopener noreferrer">Rich Caruana</a> - The man behind Explainable Boosting Machines.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2021/03/23/">Mar 23, 2021</a></h2><h3><p>Papers / Interpretable Models</p>
</h3>
<ul>
<li><a href="https://christophm.github.io/interpretable-ml-book/rules.html" rel="noopener noreferrer">Decision List</a> - Like a decision tree with no branches.</li>
</ul>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Decision_tree" rel="noopener noreferrer">Decision Trees</a> - The tree provides an interpretation.</li>
</ul>

<ul>
<li><a href="https://www.youtube.com/watch?v=MREiHgHgl0k" rel="noopener noreferrer">Explainable Boosting Machine</a> - Method that predicts based on learned vector graphs of features.</li>
</ul>

<ul>
<li><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="noopener noreferrer">k-Nearest Neighbors</a> - The prototypical clustering method.</li>
</ul>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener noreferrer">Linear Regression</a> - Easily plottable and understandable regression.</li>
</ul>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener noreferrer">Logistic Regression</a> - Easily plottable and understandable classification.</li>
</ul>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener noreferrer">Naive Bayes</a> - Good classification, poor estimation using conditional probabilities.</li>
</ul>

<ul>
<li><a href="https://christophm.github.io/interpretable-ml-book/rulefit.html" rel="noopener noreferrer">RuleFit</a> - Sparse linear model as decision rules including feature interactions.</li>
</ul>
<h3><p>Videos / Critiques</p>
</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=93Xv8vJ2acI" rel="noopener noreferrer">Debate: Interpretability is necessary for ML</a> - A debate on whether interpretability is necessary for ML with Rich Caruana and Patrice Simard for and Kilian Weinberger and Yann LeCun against.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2021/03/17/">Mar 17, 2021</a></h2><h3><p>Papers / Landmarks</p>
</h3>
<ul>
<li><a href="https://arxiv.org/abs/1706.07269" rel="noopener noreferrer">Explanation in Artificial Intelligence: Insights from the Social Sciences</a> - This paper provides an introduction to the social science research into explanations. The author provides 4 major findings: (1) explanations are constrastive, (2) explanations are selected, (3) probabilities probably don't matter, (4) explanations are social. These fit into the general theme that explanations are -contextual-.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1810.03292" rel="noopener noreferrer">Sanity Checks for Saliency Maps</a> - An important read for anyone using saliency maps. This paper proposes two experiments to determine whether saliency maps are useful: (1) model parameter randomization test compares maps from trained and untrained models, (2) data randomization test compares maps from models trained on the original dataset and models trained on the same dataset with randomized labels. They find that "some widely deployed saliency methods are independent of both the data the model was trained on, and the model parameters".</li>
</ul>
<h3><p>Papers / Surveys</p>
</h3>
<ul>
<li><a href="https://arxiv.org/abs/2004.14545" rel="noopener noreferrer">Explainable Deep Learning: A Field Guide for the Uninitiated</a> - An in-depth description of XAI focused on technqiues for deep learning.</li>
</ul>
<h3><p>Papers / Evaluations</p>
</h3>
<ul>
<li><a href="https://arxiv.org/abs/2009.02899" rel="noopener noreferrer">Quantifying Explainability of Saliency Methods in Deep Neural Networks</a> - An analysis of how different heatmap-based saliency methods perform based on experimentation with a generated dataset.</li>
</ul>
<h3><p>Papers / XAI Methods</p>
</h3>
<ul>
<li><a href="https://arxiv.org/abs/2102.07799" rel="noopener noreferrer">Ada-SISE</a> - Adaptive semantice inpute sampling for explanation.</li>
</ul>

<ul>
<li><a href="https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12377" rel="noopener noreferrer">ALE</a> - Accumulated local effects plot.</li>
</ul>

<ul>
<li><a href="https://link.springer.com/chapter/10.1007/978-3-030-33607-3_49" rel="noopener noreferrer">ALIME</a> - Autoencoder Based Approach for Local Interpretability.</li>
</ul>

<ul>
<li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/11491" rel="noopener noreferrer">Anchors</a> - High-Precision Model-Agnostic Explanations.</li>
</ul>

<ul>
<li><a href="https://link.springer.com/article/10.1007/s10115-017-1116-3" rel="noopener noreferrer">Auditing</a> - Auditing black-box models.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/2012.03058" rel="noopener noreferrer">BayLIME</a> - Bayesian local interpretable model-agnostic explanations.</li>
</ul>

<ul>
<li><a href="http://ema.drwhy.ai/breakDown.html#BDMethod" rel="noopener noreferrer">Break Down</a> - Break down plots for additive attributions.</li>
</ul>

<ul>
<li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf" rel="noopener noreferrer">CAM</a> - Class activation mapping.</li>
</ul>

<ul>
<li><a href="https://ieeexplore.ieee.org/abstract/document/4167900" rel="noopener noreferrer">CDT</a> - Confident interpretation of Bayesian decision tree ensembles.</li>
</ul>

<ul>
<li><a href="https://christophm.github.io/interpretable-ml-book/ice.html" rel="noopener noreferrer">CICE</a> - Centered ICE plot.</li>
</ul>

<ul>
<li><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.40.2710&amp;rep=rep1&amp;type=pdf" rel="noopener noreferrer">CMM</a> - Combined multiple models metalearner.</li>
</ul>

<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/B9781558603356500131" rel="noopener noreferrer">Conj Rules</a> - Using sampling and queries to extract rules from trained neural networks.</li>
</ul>

<ul>
<li><a href="https://ieeexplore.ieee.org/abstract/document/6597214" rel="noopener noreferrer">CP</a> - Contribution propogation.</li>
</ul>

<ul>
<li><a href="https://dl.acm.org/doi/abs/10.1145/775047.775113" rel="noopener noreferrer">DecText</a> - Extracting decision trees from trained neural networks.</li>
</ul>

<ul>
<li><a href="https://ieeexplore-ieee-org.ezproxy.libraries.wright.edu/abstract/document/9352498" rel="noopener noreferrer">DeepLIFT</a> - Deep label-specific feature learning for image annotation.</li>
</ul>

<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0031320316303582" rel="noopener noreferrer">DTD</a> - Deep Taylor decomposition.</li>
</ul>

<ul>
<li><a href="https://www.aaai.org/Papers/IAAI/2006/IAAI06-018.pdf" rel="noopener noreferrer">ExplainD</a> - Explanations of evidence in additive classifiers.</li>
</ul>

<ul>
<li><a href="https://link.springer.com/chapter/10.1007/978-3-642-04174-7_45" rel="noopener noreferrer">FIRM</a> - Feature importance ranking measure.</li>
</ul>

<ul>
<li><a href="https://openaccess.thecvf.com/content_iccv_2017/html/Fong_Interpretable_Explanations_of_ICCV_2017_paper.html" rel="noopener noreferrer">Fong, et. al.</a> - Meaninful perturbations model.</li>
</ul>

<ul>
<li><a href="https://www.academia.edu/download/51462700/s0362-546x_2896_2900267-220170122-9600-1njrpyx.pdf" rel="noopener noreferrer">G-REX</a> - Rule extraction using genetic algorithms.</li>
</ul>

<ul>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3977175/" rel="noopener noreferrer">Gibbons, et. al.</a> - Explain random forest using decision tree.</li>
</ul>

<ul>
<li><a href="https://link-springer-com.ezproxy.libraries.wright.edu/article/10.1007/s10618-014-0368-8" rel="noopener noreferrer">GoldenEye</a> - Exploring classifiers by randomization.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/0912.1128" rel="noopener noreferrer">GPD</a> - Gaussian process decisions.</li>
</ul>

<ul>
<li><a href="https://ieeexplore.ieee.org/abstract/document/4938655" rel="noopener noreferrer">GPDT</a> - Genetic program to evolve decision trees.</li>
</ul>

<ul>
<li><a href="https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html" rel="noopener noreferrer">GradCAM</a> - Gradient-weighted Class Activation Mapping.</li>
</ul>

<ul>
<li><a href="https://ieeexplore.ieee.org/abstract/document/8354201/" rel="noopener noreferrer">GradCAM++</a> - Generalized gradient-based visual explanations.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1606.05390" rel="noopener noreferrer">Hara, et. al.</a> - Making tree ensembles interpretable.</li>
</ul>

<ul>
<li><a href="https://www.tandfonline.com/doi/abs/10.1080/10618600.2014.907095" rel="noopener noreferrer">ICE</a> - Individual conditional expectation plots.</li>
</ul>

<ul>
<li><a href="http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf" rel="noopener noreferrer">IG</a> - Integrated gradients.</li>
</ul>

<ul>
<li><a href="https://link.springer.com/article/10.1007/s41060-018-0144-8" rel="noopener noreferrer">inTrees</a> - Interpreting tree ensembles with inTrees.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1611.04967" rel="noopener noreferrer">IOFP</a> - Iterative orthoganol feature projection.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1703.00810" rel="noopener noreferrer">IP</a> - Information plane visualization.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1810.02678" rel="noopener noreferrer">KL-LIME</a> - Kullback-Leibler Projections based LIME.</li>
</ul>

<ul>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320398001812" rel="noopener noreferrer">Krishnan, et. al.</a> - Extracting decision trees from trained neural networks.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1606.04155" rel="noopener noreferrer">Lei, et. al.</a> - Rationalizing neural predictions with generator and encoder.</li>
</ul>

<ul>
<li><a href="https://dl.acm.org/doi/abs/10.1145/2939672.2939778" rel="noopener noreferrer">LIME</a> - Local Interpretable Model-Agnostic Explanations.</li>
</ul>

<ul>
<li><a href="https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2017.1307116#.YEkdZ7CSmUk" rel="noopener noreferrer">LOCO</a> - Leave-one covariate out.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1805.10820" rel="noopener noreferrer">LORE</a> - Local rule-based explanations.</li>
</ul>

<ul>
<li><a href="https://dl.acm.org/doi/abs/10.1145/2487575.2487579" rel="noopener noreferrer">Lou, et. al.</a> - Accurate intelligibile models with pairwise interactions.</li>
</ul>

<ul>
<li><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140" rel="noopener noreferrer">LRP</a> - Layer-wise relevance propogation.</li>
</ul>

<ul>
<li><a href="https://www.jmlr.org/papers/volume20/18-760/18-760.pdf" rel="noopener noreferrer">MCR</a> - Model class reliance.</li>
</ul>

<ul>
<li><a href="https://ieeexplore.ieee.org/abstract/document/7738872" rel="noopener noreferrer">MES</a> - Model explanation system.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1611.07567" rel="noopener noreferrer">MFI</a> - Feature importance measure for non-linear algorithms.</li>
</ul>

<ul>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0304380002000649" rel="noopener noreferrer">NID</a> - Neural interpretation diagram.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/2006.05714" rel="noopener noreferrer">OptiLIME</a> - Optimized LIME.</li>
</ul>

<ul>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3077257.3077271" rel="noopener noreferrer">PALM</a> - Partition aware local model.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1702.04595" rel="noopener noreferrer">PDA</a> - Prediction Difference Analysis: Visualize deep neural network decisions.</li>
</ul>

<ul>
<li><a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451" rel="noopener noreferrer">PDP</a> - Partial dependence plots.</li>
</ul>

<ul>
<li><a href="https://academic.oup.com/bioinformatics/article/24/13/i6/233341" rel="noopener noreferrer">POIMs</a> - Positional oligomer importance matrices for understanding SVM signal detectors.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1807.07506" rel="noopener noreferrer">ProfWeight</a> - Transfer information from deep network to simpler model.</li>
</ul>

<ul>
<li><a href="https://dl.acm.org/doi/abs/10.1145/2858036.2858529" rel="noopener noreferrer">Prospector</a> - Interactive partial dependence diagnostics.</li>
</ul>

<ul>
<li><a href="https://ieeexplore.ieee.org/abstract/document/7546525" rel="noopener noreferrer">QII</a> - Quantitative input influence.</li>
</ul>

<ul>
<li><a href="https://content.iospress.com/articles/ai-communications/aic272" rel="noopener noreferrer">REFNE</a> - Extracting symbolic rules from trained neural network ensembles.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1608.05745" rel="noopener noreferrer">RETAIN</a> - Reverse time attention model.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1806.07421" rel="noopener noreferrer">RISE</a> - Randomized input sampling for explanation.</li>
</ul>

<ul>
<li><a href="https://link.springer.com/article/10.1007%2Fs11063-011-9207-8" rel="noopener noreferrer">RxREN</a> - Reverse engineering neural networks for rule extraction.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1705.07874" rel="noopener noreferrer">SHAP</a> - A unified approach to interpretting model predictions.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/2101.10710" rel="noopener noreferrer">SIDU</a> - Similarity, difference, and uniqueness input perturbation.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1312.6034" rel="noopener noreferrer">Simonynan, et. al</a> - Visualizing CNN classes.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1611.07579" rel="noopener noreferrer">Singh, et. al</a> - Programs as black-box explanations.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1610.09036" rel="noopener noreferrer">STA</a> - Interpreting models via Single Tree Approximation.</li>
</ul>

<ul>
<li><a href="https://www.jmlr.org/papers/volume11/strumbelj10a/strumbelj10a.pdf" rel="noopener noreferrer">Strumbelj, et. al.</a> - Explanation of individual classifications using game theory.</li>
</ul>

<ul>
<li><a href="https://www.academia.edu/download/2471122/3uecwtv9xcwxg6r.pdf" rel="noopener noreferrer">SVM+P</a> - Rule extraction from support vector machines.</li>
</ul>

<ul>
<li><a href="https://openreview.net/forum?id=S1viikbCW" rel="noopener noreferrer">TCAV</a> - Testing with concept activation vectors.</li>
</ul>

<ul>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3097983.3098039" rel="noopener noreferrer">Tolomei, et. al.</a> - Interpretable predictions of tree-ensembles via actionable feature tweaking.</li>
</ul>

<ul>
<li><a href="https://www.researchgate.net/profile/Edward-George-2/publication/2610587_Making_Sense_of_a_Forest_of_Trees/links/55b1085d08aec0e5f430eb40/Making-Sense-of-a-Forest-of-Trees.pdf" rel="noopener noreferrer">Tree Metrics</a> - Making sense of a forest of trees.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1706.06060" rel="noopener noreferrer">TreeSHAP</a> - Consistent feature attribute for tree ensembles.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1611.07429" rel="noopener noreferrer">TreeView</a> - Feature-space partitioning.</li>
</ul>

<ul>
<li><a href="http://www.inf.ufrgs.br/~engel/data/media/file/cmp121/TREPAN_craven.nips96.pdf" rel="noopener noreferrer">TREPAN</a> - Extracting tree-structured representations of trained networks.</li>
</ul>

<ul>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3412815.3416893" rel="noopener noreferrer">TSP</a> - Tree space prototypes.</li>
</ul>

<ul>
<li><a href="http://www.columbia.edu/~aec2163/NonFlash/Papers/VisualBackProp.pdf" rel="noopener noreferrer">VBP</a> - Visual back-propagation.</li>
</ul>

<ul>
<li><a href="https://ieeexplore.ieee.org/abstract/document/5949423" rel="noopener noreferrer">VEC</a> - Variable effect characteristic curve.</li>
</ul>

<ul>
<li><a href="https://dl.acm.org/doi/abs/10.1145/1014052.1014122" rel="noopener noreferrer">VIN</a> - Variable interaction network.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1508.07551" rel="noopener noreferrer">X-TREPAN</a> - Adapted etraction of comprehensible decision tree in ANNs.</li>
</ul>

<ul>
<li><a href="http://proceedings.mlr.press/v37/xuc15" rel="noopener noreferrer">Xu, et. al.</a> - Show, attend, tell attention model.</li>
</ul>
<h3><p>Papers / Critiques</p>
</h3>
<ul>
<li><a href="https://arxiv.org/abs/1902.10186" rel="noopener noreferrer">Attention is not Explanation</a> - Authors perform a series of NLP experiments which argue attention does not provide meaningful explanations. They also demosntrate that different attentions can generate similar model outputs.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1908.04626" rel="noopener noreferrer">Attention is not --not-- Explanation</a> - This is a rebutal to the above paper. Authors argue that multiple explanations can be valid and that the and that attention can produce <em>a</em> valid explanation, if not -the- valid explanation.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1903.11420" rel="noopener noreferrer">Do Not Trust Additive Explanations</a> - Authors argue that addditive explanations (e.g. LIME, SHAP, Break Down) fail to take feature ineractions into account and are thus unreliable.</li>
</ul>

<ul>
<li><a href="https://arxiv.org/abs/1905.03151" rel="noopener noreferrer">Please Stop Permuting Features An Explanation and Alternatives</a> - Authors demonstrate why permuting features is misleading, especially where there is strong feature dependence. They offer several previously described alternatives.</li>
</ul>

<ul>
<li><a href="https://www.nature.com/articles/s42256-019-0048-x?fbclid=IwAR3156gP-ntoAyw2sHTXo0Z8H9p-2wBKe5jqitsMCdft7xA0P766QvSthFs" rel="noopener noreferrer">Stop Explaining Black Box Machine Learning Models for High States Decisions and Use Interpretable Models Instead</a> - Authors present a number of issues with explainable ML and challenges to interpretable ML: (1) constructing optimal logical models, (2) constructing optimal sparse scoring systems, (3) defining interpretability and creating methods for specific methods. They also offer an argument for why interpretable models might exist in many different domains.</li>
</ul>

<ul>
<li><a href="https://link.springer.com/chapter/10.1007/978-3-030-28954-6_14" rel="noopener noreferrer">The (Un)reliability of Saliency Methods</a> - Authors demonstrate how saliency methods vary attribution when adding a constant shift to the input data. They argue that methods should fulfill <em>input invariance</em>, that a saliency method mirror the sensistivity of the model with respect to transformations of the input.</li>
</ul>
<h3><p>Follow / Critiques</p>
</h3>
<ul>
<li><a href="https://ethical.institute/index.html" rel="noopener noreferrer">The Institute for Ethical AI &amp; Machine Learning</a> - A UK-based research center that performs research into ethical AI/ML, which frequently involves XAI.</li>
</ul>

<ul>
<li><a href="https://twitter.com/tmiller_unimelb" rel="noopener noreferrer">Tim Miller</a> - One of the preeminent researchers in XAI.</li>
</ul>
<h2><a href="https://www.trackawesomelist.com/2021/03/02/">Mar 02, 2021</a></h2><h3><p>Repositories / Critiques</p>
</h3>
<ul>
<li><a href="https://github.com/EthicalML/xai" rel="noopener noreferrer">EthicalML/xai (⭐1.2k)</a> - A toolkit for XAI which is focused exclusively on tabular data. It implements a variety of data and model evaluation techniques.</li>
</ul>

<ul>
<li><a href="https://github.com/PAIR-code/what-if-tool" rel="noopener noreferrer">PAIR-code/what-if-tool (⭐962)</a> - A tool for Tensorboard or Notebooks which allows investigating model performance and fairness.</li>
</ul>

    </main>
  </body>
</html>
