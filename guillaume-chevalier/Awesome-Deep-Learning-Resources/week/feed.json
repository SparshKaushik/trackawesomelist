{
  "version": "https://jsonfeed.org/version/1",
  "icon": "https://www.trackawesomelist.com/icon.png",
  "favicon": "https://www.trackawesomelist.com/favicon.ico",
  "language": "en",
  "title": "Track Awesome Deep Learning Resources Updates Weekly",
  "_seo_title": "Track Awesome Deep Learning Resources (guillaume-chevalier/Awesome-Deep-Learning-Resources) Updates Weekly - Track Awesome List",
  "_site_title": "Track Awesome List",
  "description": "Rough list of my favorite deep learning resources, useful for revisiting topics or for reference. I have got through all of the content listed there, carefully. - Guillaume Chevalier",
  "home_page_url": "https://www.trackawesomelist.com/guillaume-chevalier/Awesome-Deep-Learning-Resources/week/",
  "feed_url": "https://www.trackawesomelist.com/guillaume-chevalier/Awesome-Deep-Learning-Resources/week/feed.json",
  "items": [
    {
      "id": "https://www.trackawesomelist.com/2021/31/",
      "title": "Awesome Deep Learning Resources Updates on Aug 02 - Aug 08, 2021",
      "_short_title": "Aug 02 - Aug 08, 2021",
      "_slug": "2021/31/",
      "summary": "1 awesome projects updated on Aug 02 - Aug 08, 2021",
      "_filepath": "/content/2021/31/README.md",
      "url": "https://www.trackawesomelist.com/2021/31/",
      "date_published": "2021-08-04T19:34:16.000Z",
      "date_modified": "2021-08-04T19:34:16.000Z",
      "content_text": "\n\n### Posts and Articles\n\n*   [SOLID Machine Learning](https://www.umaneo.com/post/the-solid-principles-applied-to-machine-learning) - The SOLID principles applied to Machine Learning.",
      "content_html": "<h3><p>Posts and Articles</p>\n</h3>\n<ul>\n<li><a href=\"https://www.umaneo.com/post/the-solid-principles-applied-to-machine-learning\" rel=\"noopener noreferrer\">SOLID Machine Learning</a> - The SOLID principles applied to Machine Learning.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2021/18/",
      "title": "Awesome Deep Learning Resources Updates on May 03 - May 09, 2021",
      "_short_title": "May 03 - May 09, 2021",
      "_slug": "2021/18/",
      "summary": "1 awesome projects updated on May 03 - May 09, 2021",
      "_filepath": "/content/2021/18/README.md",
      "url": "https://www.trackawesomelist.com/2021/18/",
      "date_published": "2021-05-04T19:16:26.000Z",
      "date_modified": "2021-05-04T19:16:26.000Z",
      "content_text": "\n\n### Online Classes\n\n*   **[DL\\&RNN Course](https://www.dl-rnn-course.neuraxio.com/start?utm_source=github_awesome) - I created this richely dense course on Deep Learning and Recurrent Neural Networks.**",
      "content_html": "<h3><p>Online Classes</p>\n</h3>\n<ul>\n<li><strong><a href=\"https://www.dl-rnn-course.neuraxio.com/start?utm_source=github_awesome\" rel=\"noopener noreferrer\">DL&amp;RNN Course</a> - I created this richely dense course on Deep Learning and Recurrent Neural Networks.</strong></li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2021/7/",
      "title": "Awesome Deep Learning Resources Updates on Feb 15 - Feb 21, 2021",
      "_short_title": "Feb 15 - Feb 21, 2021",
      "_slug": "2021/7/",
      "summary": "1 awesome projects updated on Feb 15 - Feb 21, 2021",
      "_filepath": "/content/2021/7/README.md",
      "url": "https://www.trackawesomelist.com/2021/7/",
      "date_published": "2021-02-20T21:26:23.000Z",
      "date_modified": "2021-02-20T21:26:23.000Z",
      "content_text": "\n\n### Misc. Hubs & Links / Other\n\n*   [Awesome Neuraxle (⭐3)](https://github.com/Neuraxio/Awesome-Neuraxle) - An awesome list for Neuraxle, a ML Framework for coding clean production-level ML pipelines.",
      "content_html": "<h3><p>Misc. Hubs &amp; Links / Other</p>\n</h3>\n<ul>\n<li><a href=\"https://github.com/Neuraxio/Awesome-Neuraxle\" rel=\"noopener noreferrer\">Awesome Neuraxle (⭐3)</a> - An awesome list for Neuraxle, a ML Framework for coding clean production-level ML pipelines.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2020/16/",
      "title": "Awesome Deep Learning Resources Updates on Apr 20 - Apr 26, 2020",
      "_short_title": "Apr 20 - Apr 26, 2020",
      "_slug": "2020/16/",
      "summary": "3 awesome projects updated on Apr 20 - Apr 26, 2020",
      "_filepath": "/content/2020/16/README.md",
      "url": "https://www.trackawesomelist.com/2020/16/",
      "date_published": "2020-04-18T07:24:36.000Z",
      "date_modified": "2020-04-18T07:24:36.000Z",
      "content_text": "\n\n### Books\n\n*   [Clean Code](https://www.amazon.ca/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882) - Get back to the basics you fool! Learn how to do Clean Code for your career. This is by far the best book I've read even if this list is related to Deep Learning.\n*   [Clean Coder](https://www.amazon.ca/Clean-Coder-Conduct-Professional-Programmers/dp/0137081073) - Learn how to be professional as a coder and how to interact with your manager. This is important for any coding career.\n\n### Practical Resources / Librairies and Implementations\n\n*   [Clean Machine Learning, a Coding Kata (⭐17)](https://github.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code) - Learn the good design patterns to use for doing Machine Learning the good way, by practicing.",
      "content_html": "<h3><p>Books</p>\n</h3>\n<ul>\n<li><a href=\"https://www.amazon.ca/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882\" rel=\"noopener noreferrer\">Clean Code</a> - Get back to the basics you fool! Learn how to do Clean Code for your career. This is by far the best book I've read even if this list is related to Deep Learning.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.amazon.ca/Clean-Coder-Conduct-Professional-Programmers/dp/0137081073\" rel=\"noopener noreferrer\">Clean Coder</a> - Learn how to be professional as a coder and how to interact with your manager. This is important for any coding career.</li>\n</ul>\n<h3><p>Practical Resources / Librairies and Implementations</p>\n</h3>\n<ul>\n<li><a href=\"https://github.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code\" rel=\"noopener noreferrer\">Clean Machine Learning, a Coding Kata (⭐17)</a> - Learn the good design patterns to use for doing Machine Learning the good way, by practicing.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2020/12/",
      "title": "Awesome Deep Learning Resources Updates on Mar 23 - Mar 29, 2020",
      "_short_title": "Mar 23 - Mar 29, 2020",
      "_slug": "2020/12/",
      "summary": "1 awesome projects updated on Mar 23 - Mar 29, 2020",
      "_filepath": "/content/2020/12/README.md",
      "url": "https://www.trackawesomelist.com/2020/12/",
      "date_published": "2020-03-20T03:12:51.000Z",
      "date_modified": "2020-03-20T03:12:51.000Z",
      "content_text": "\n\n### YouTube and Videos / Other\n\n*   [Growing Neat Software Architecture from Jupyter Notebooks](https://www.youtube.com/watch?v=K4QN27IKr0g) - A primer on how to structure your Machine Learning projects when using Jupyter Notebooks.",
      "content_html": "<h3><p>YouTube and Videos / Other</p>\n</h3>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=K4QN27IKr0g\" rel=\"noopener noreferrer\">Growing Neat Software Architecture from Jupyter Notebooks</a> - A primer on how to structure your Machine Learning projects when using Jupyter Notebooks.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2020/10/",
      "title": "Awesome Deep Learning Resources Updates on Mar 09 - Mar 15, 2020",
      "_short_title": "Mar 09 - Mar 15, 2020",
      "_slug": "2020/10/",
      "summary": "3 awesome projects updated on Mar 09 - Mar 15, 2020",
      "_filepath": "/content/2020/10/README.md",
      "url": "https://www.trackawesomelist.com/2020/10/",
      "date_published": "2020-03-07T01:19:44.000Z",
      "date_modified": "2020-03-07T01:29:39.000Z",
      "content_text": "\n\n### Online Classes\n\n*   [Deep Learning & Recurrent Neural Networks (DL\\&RNN)](https://www.neuraxio.com/en/time-series-solution) - The most richly dense, accelerated course on the topic of Deep Learning & Recurrent Neural Networks (scroll at the end).\n\n### Practical Resources / Librairies and Implementations\n\n*   [Hyperopt for a Keras CNN on CIFAR-100 (⭐107)](https://github.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100) - Auto (meta) optimizing a neural net (and its architecture) on the CIFAR-100 dataset.\n*   [Smoothly Blend Image Patches (⭐0)](https://github.com/guillaume-chevalier/Smoothly-Blend-Image-Patches) - Smooth patch merger for [semantic segmentation with a U-Net](https://vooban.com/en/tips-articles-geek-stuff/satellite-image-segmentation-workflow-with-u-net/).",
      "content_html": "<h3><p>Online Classes</p>\n</h3>\n<ul>\n<li><a href=\"https://www.neuraxio.com/en/time-series-solution\" rel=\"noopener noreferrer\">Deep Learning &amp; Recurrent Neural Networks (DL&amp;RNN)</a> - The most richly dense, accelerated course on the topic of Deep Learning &amp; Recurrent Neural Networks (scroll at the end).</li>\n</ul>\n<h3><p>Practical Resources / Librairies and Implementations</p>\n</h3>\n<ul>\n<li><a href=\"https://github.com/guillaume-chevalier/Hyperopt-Keras-CNN-CIFAR-100\" rel=\"noopener noreferrer\">Hyperopt for a Keras CNN on CIFAR-100 (⭐107)</a> - Auto (meta) optimizing a neural net (and its architecture) on the CIFAR-100 dataset.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://github.com/guillaume-chevalier/Smoothly-Blend-Image-Patches\" rel=\"noopener noreferrer\">Smoothly Blend Image Patches (⭐0)</a> - Smooth patch merger for <a href=\"https://vooban.com/en/tips-articles-geek-stuff/satellite-image-segmentation-workflow-with-u-net/\" rel=\"noopener noreferrer\">semantic segmentation with a U-Net</a>.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2020/6/",
      "title": "Awesome Deep Learning Resources Updates on Feb 10 - Feb 16, 2020",
      "_short_title": "Feb 10 - Feb 16, 2020",
      "_slug": "2020/6/",
      "summary": "2 awesome projects updated on Feb 10 - Feb 16, 2020",
      "_filepath": "/content/2020/6/README.md",
      "url": "https://www.trackawesomelist.com/2020/6/",
      "date_published": "2020-02-08T02:51:30.000Z",
      "date_modified": "2020-02-08T02:51:30.000Z",
      "content_text": "\n\n### Posts and Articles\n\n*   [Why do 87% of data science projects never make it into production?](https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/) - Data is not to be overlooked, and communication between teams and data scientists is important to integrate solutions properly.\n*   [The real reason most ML projects fail](https://towardsdatascience.com/what-is-the-main-reason-most-ml-projects-fail-515d409a161f) - Focus on clear business objectives, avoid pivots of algorithms unless you have really clean code, and be able to know when what you coded is \"good enough\".",
      "content_html": "<h3><p>Posts and Articles</p>\n</h3>\n<ul>\n<li><a href=\"https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/\" rel=\"noopener noreferrer\">Why do 87% of data science projects never make it into production?</a> - Data is not to be overlooked, and communication between teams and data scientists is important to integrate solutions properly.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://towardsdatascience.com/what-is-the-main-reason-most-ml-projects-fail-515d409a161f\" rel=\"noopener noreferrer\">The real reason most ML projects fail</a> - Focus on clear business objectives, avoid pivots of algorithms unless you have really clean code, and be able to know when what you coded is \"good enough\".</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2019/35/",
      "title": "Awesome Deep Learning Resources Updates on Sep 02 - Sep 08, 2019",
      "_short_title": "Sep 02 - Sep 08, 2019",
      "_slug": "2019/35/",
      "summary": "1 awesome projects updated on Sep 02 - Sep 08, 2019",
      "_filepath": "/content/2019/35/README.md",
      "url": "https://www.trackawesomelist.com/2019/35/",
      "date_published": "2019-08-26T03:01:46.000Z",
      "date_modified": "2019-08-26T03:01:46.000Z",
      "content_text": "\n\n### Practical Resources / Librairies and Implementations\n\n*   [Neuraxle (⭐613)](https://github.com/Neuraxio/Neuraxle) - Neuraxle is a Machine Learning (ML) library for building neat pipelines, providing the right abstractions to both ease research, development, and deployment of your ML applications.",
      "content_html": "<h3><p>Practical Resources / Librairies and Implementations</p>\n</h3>\n<ul>\n<li><a href=\"https://github.com/Neuraxio/Neuraxle\" rel=\"noopener noreferrer\">Neuraxle (⭐613)</a> - Neuraxle is a Machine Learning (ML) library for building neat pipelines, providing the right abstractions to both ease research, development, and deployment of your ML applications.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2019/32/",
      "title": "Awesome Deep Learning Resources Updates on Aug 12 - Aug 18, 2019",
      "_short_title": "Aug 12 - Aug 18, 2019",
      "_slug": "2019/32/",
      "summary": "2 awesome projects updated on Aug 12 - Aug 18, 2019",
      "_filepath": "/content/2019/32/README.md",
      "url": "https://www.trackawesomelist.com/2019/32/",
      "date_published": "2019-08-06T23:05:51.000Z",
      "date_modified": "2019-08-06T23:08:09.000Z",
      "content_text": "\n\n### Posts and Articles\n\n*   [Uncle Bob's Principles Of OOD](http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod) - Not only the SOLID principles are needed for doing clean code, but the furtherless known REP, CCP, CRP, ADP, SDP and SAP principles are very important for developping huge software that must be bundled in different separated packages.\n\n### Practical Resources / Librairies and Implementations\n\n*   [Neuraxle, a framwework for machine learning pipelines (⭐613)](https://github.com/Neuraxio/Neuraxle) - The best framework for structuring and deploying your machine learning projects, and which is also compatible with most framework (e.g.: Scikit-Learn, TensorFlow, PyTorch, Keras, and so forth).",
      "content_html": "<h3><p>Posts and Articles</p>\n</h3>\n<ul>\n<li><a href=\"http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod\" rel=\"noopener noreferrer\">Uncle Bob's Principles Of OOD</a> - Not only the SOLID principles are needed for doing clean code, but the furtherless known REP, CCP, CRP, ADP, SDP and SAP principles are very important for developping huge software that must be bundled in different separated packages.</li>\n</ul>\n<h3><p>Practical Resources / Librairies and Implementations</p>\n</h3>\n<ul>\n<li><a href=\"https://github.com/Neuraxio/Neuraxle\" rel=\"noopener noreferrer\">Neuraxle, a framwework for machine learning pipelines (⭐613)</a> - The best framework for structuring and deploying your machine learning projects, and which is also compatible with most framework (e.g.: Scikit-Learn, TensorFlow, PyTorch, Keras, and so forth).</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2019/7/",
      "title": "Awesome Deep Learning Resources Updates on Feb 18 - Feb 24, 2019",
      "_short_title": "Feb 18 - Feb 24, 2019",
      "_slug": "2019/7/",
      "summary": "1 awesome projects updated on Feb 18 - Feb 24, 2019",
      "_filepath": "/content/2019/7/README.md",
      "url": "https://www.trackawesomelist.com/2019/7/",
      "date_published": "2019-02-13T19:03:36.000Z",
      "date_modified": "2019-02-13T19:03:36.000Z",
      "content_text": "\n\n### Books\n\n*   [Some other books I have read](https://books.google.ca/books?hl=en\\&as_coll=4\\&num=100\\&uid=103409002069648430166\\&source=gbs_slider_cls_metadata_4_mylibrary_title) - Some books listed here are less related to deep learning but are still somehow relevant to this list.",
      "content_html": "<h3><p>Books</p>\n</h3>\n<ul>\n<li><a href=\"https://books.google.ca/books?hl=en&amp;as_coll=4&amp;num=100&amp;uid=103409002069648430166&amp;source=gbs_slider_cls_metadata_4_mylibrary_title\" rel=\"noopener noreferrer\">Some other books I have read</a> - Some books listed here are less related to deep learning but are still somehow relevant to this list.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2019/1/",
      "title": "Awesome Deep Learning Resources Updates on Jan 07 - Jan 13, 2019",
      "_short_title": "Jan 07 - Jan 13, 2019",
      "_slug": "2019/1/",
      "summary": "13 awesome projects updated on Jan 07 - Jan 13, 2019",
      "_filepath": "/content/2019/1/README.md",
      "url": "https://www.trackawesomelist.com/2019/1/",
      "date_published": "2018-12-31T22:28:26.000Z",
      "date_modified": "2019-01-02T22:10:42.000Z",
      "content_text": "\n\n### Posts and Articles\n\n*   [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) - Good for understanding the \"Attention Is All You Need\" (AIAYN) paper.\n*   [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Also good for understanding the \"Attention Is All You Need\" (AIAYN) paper.\n*   [Improving Language Understanding with Unsupervised Learning](https://blog.openai.com/language-unsupervised/) - SOTA across many NLP tasks from unsupervised pretraining on huge corpus.\n*   [NLP's ImageNet moment has arrived](https://thegradient.pub/nlp-imagenet/) - All hail NLP's ImageNet moment.\n*   [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/) - Understand the different approaches used for NLP's ImageNet moment.\n\n### Practical Resources / Librairies and Implementations\n\n*   [Self Governing Neural Networks (SGNN): the Projection Layer (⭐22)](https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer) - With this, you can use words in your deep learning models without training nor loading embeddings.\n\n### Practical Resources / Some Datasets\n\n*   [SentEval: An Evaluation Toolkit for Universal Sentence Representations](https://arxiv.org/abs/1803.05449) - A Python framework to benchmark your sentence representations on many datasets (NLP tasks).\n*   [ParlAI: A Dialog Research Software Platform](https://arxiv.org/abs/1705.06476) - Another Python framework to benchmark your sentence representations on many datasets (NLP tasks).\n\n### Other Math Theory / Complex Numbers & Digital Signal Processing\n\n*   [Window Functions](https://en.wikipedia.org/wiki/Window_function) - Wikipedia page that lists some of the known window functions - note that the [Hann-Poisson window](https://en.wikipedia.org/wiki/Window_function#Hann%E2%80%93Poisson_window) is specially interesting for greedy hill-climbing algorithms (like gradient descent for example).\n\n### Papers / Attention Mechanisms\n\n*   [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (AIAYN) - Introducing multi-head self-attention neural networks with positional encoding to do sentence-level NLP without any RNN nor CNN - this paper is a must-read (also see [this explanation](http://nlp.seas.harvard.edu/2018/04/03/attention.html) and [this visualization](http://jalammar.github.io/illustrated-transformer/) of the paper).\n\n### Papers / Other\n\n*   [ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projections](https://arxiv.org/abs/1708.00630) - Replace word embeddings by word projections in your deep neural networks, which doesn't require a pre-extracted dictionnary nor storing embedding matrices.\n*   [Self-Governing Neural Networks for On-Device Short Text Classification](http://aclweb.org/anthology/D18-1105) - This paper is the sequel to the ProjectionNet just above. The SGNN is elaborated on the ProjectionNet, and the optimizations are detailed more in-depth (also see my [attempt to reproduce the paper in code (⭐22)](https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer) and watch [the talks' recording](https://vimeo.com/305197775)).\n*   [Matching Networks for One Shot Learning](https://arxiv.org/abs/1606.04080) - Classify a new example from a list of other examples (without definitive categories) and with low-data per classification task, but lots of data for lots of similar classification tasks - it seems better than siamese networks. To sum up: with Matching Networks, you can optimize directly for a cosine similarity between examples (like a self-attention product would match) which is passed to the softmax directly. I guess that Matching Networks could probably be used as with negative-sampling softmax training in word2vec's CBOW or Skip-gram without having to do any context embedding lookups.",
      "content_html": "<h3><p>Posts and Articles</p>\n</h3>\n<ul>\n<li><a href=\"http://nlp.seas.harvard.edu/2018/04/03/attention.html\" rel=\"noopener noreferrer\">The Annotated Transformer</a> - Good for understanding the \"Attention Is All You Need\" (AIAYN) paper.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://jalammar.github.io/illustrated-transformer/\" rel=\"noopener noreferrer\">The Illustrated Transformer</a> - Also good for understanding the \"Attention Is All You Need\" (AIAYN) paper.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://blog.openai.com/language-unsupervised/\" rel=\"noopener noreferrer\">Improving Language Understanding with Unsupervised Learning</a> - SOTA across many NLP tasks from unsupervised pretraining on huge corpus.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://thegradient.pub/nlp-imagenet/\" rel=\"noopener noreferrer\">NLP's ImageNet moment has arrived</a> - All hail NLP's ImageNet moment.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://jalammar.github.io/illustrated-bert/\" rel=\"noopener noreferrer\">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a> - Understand the different approaches used for NLP's ImageNet moment.</li>\n</ul>\n<h3><p>Practical Resources / Librairies and Implementations</p>\n</h3>\n<ul>\n<li><a href=\"https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer\" rel=\"noopener noreferrer\">Self Governing Neural Networks (SGNN): the Projection Layer (⭐22)</a> - With this, you can use words in your deep learning models without training nor loading embeddings.</li>\n</ul>\n<h3><p>Practical Resources / Some Datasets</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1803.05449\" rel=\"noopener noreferrer\">SentEval: An Evaluation Toolkit for Universal Sentence Representations</a> - A Python framework to benchmark your sentence representations on many datasets (NLP tasks).</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1705.06476\" rel=\"noopener noreferrer\">ParlAI: A Dialog Research Software Platform</a> - Another Python framework to benchmark your sentence representations on many datasets (NLP tasks).</li>\n</ul>\n<h3><p>Other Math Theory / Complex Numbers &amp; Digital Signal Processing</p>\n</h3>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Window_function\" rel=\"noopener noreferrer\">Window Functions</a> - Wikipedia page that lists some of the known window functions - note that the <a href=\"https://en.wikipedia.org/wiki/Window_function#Hann%E2%80%93Poisson_window\" rel=\"noopener noreferrer\">Hann-Poisson window</a> is specially interesting for greedy hill-climbing algorithms (like gradient descent for example).</li>\n</ul>\n<h3><p>Papers / Attention Mechanisms</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1706.03762\" rel=\"noopener noreferrer\">Attention Is All You Need</a> (AIAYN) - Introducing multi-head self-attention neural networks with positional encoding to do sentence-level NLP without any RNN nor CNN - this paper is a must-read (also see <a href=\"http://nlp.seas.harvard.edu/2018/04/03/attention.html\" rel=\"noopener noreferrer\">this explanation</a> and <a href=\"http://jalammar.github.io/illustrated-transformer/\" rel=\"noopener noreferrer\">this visualization</a> of the paper).</li>\n</ul>\n<h3><p>Papers / Other</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1708.00630\" rel=\"noopener noreferrer\">ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projections</a> - Replace word embeddings by word projections in your deep neural networks, which doesn't require a pre-extracted dictionnary nor storing embedding matrices.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://aclweb.org/anthology/D18-1105\" rel=\"noopener noreferrer\">Self-Governing Neural Networks for On-Device Short Text Classification</a> - This paper is the sequel to the ProjectionNet just above. The SGNN is elaborated on the ProjectionNet, and the optimizations are detailed more in-depth (also see my <a href=\"https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer\" rel=\"noopener noreferrer\">attempt to reproduce the paper in code (⭐22)</a> and watch <a href=\"https://vimeo.com/305197775\" rel=\"noopener noreferrer\">the talks' recording</a>).</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/abs/1606.04080\" rel=\"noopener noreferrer\">Matching Networks for One Shot Learning</a> - Classify a new example from a list of other examples (without definitive categories) and with low-data per classification task, but lots of data for lots of similar classification tasks - it seems better than siamese networks. To sum up: with Matching Networks, you can optimize directly for a cosine similarity between examples (like a self-attention product would match) which is passed to the softmax directly. I guess that Matching Networks could probably be used as with negative-sampling softmax training in word2vec's CBOW or Skip-gram without having to do any context embedding lookups.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2018/20/",
      "title": "Awesome Deep Learning Resources Updates on May 14 - May 20, 2018",
      "_short_title": "May 14 - May 20, 2018",
      "_slug": "2018/20/",
      "summary": "2 awesome projects updated on May 14 - May 20, 2018",
      "_filepath": "/content/2018/20/README.md",
      "url": "https://www.trackawesomelist.com/2018/20/",
      "date_published": "2018-05-16T22:47:00.000Z",
      "date_modified": "2018-05-16T22:47:00.000Z",
      "content_text": "\n\n### Online Classes\n\n*   [Deep Learning Specialization by Andrew Ng on Coursera](https://www.coursera.org/specializations/deep-learning) - New series of 5 Deep Learning courses by Andrew Ng, now with Python rather than Matlab/Octave, and which leads to a [specialization certificate](https://www.coursera.org/account/accomplishments/specialization/U7VNC3ZD9YD8).\n*   [GLO-4030/7030 Apprentissage par réseaux de neurones profonds](https://ulaval-damas.github.io/glo4030/) - This is a class given by Philippe Giguère, Professor at University Laval. I especially found awesome its rare visualization of the multi-head attention mechanism, which can be contemplated at the [slide 28 of week 13's class](http://www2.ift.ulaval.ca/~pgiguere/cours/DeepLearning/09-Attention.pdf).",
      "content_html": "<h3><p>Online Classes</p>\n</h3>\n<ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" rel=\"noopener noreferrer\">Deep Learning Specialization by Andrew Ng on Coursera</a> - New series of 5 Deep Learning courses by Andrew Ng, now with Python rather than Matlab/Octave, and which leads to a <a href=\"https://www.coursera.org/account/accomplishments/specialization/U7VNC3ZD9YD8\" rel=\"noopener noreferrer\">specialization certificate</a>.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://ulaval-damas.github.io/glo4030/\" rel=\"noopener noreferrer\">GLO-4030/7030 Apprentissage par réseaux de neurones profonds</a> - This is a class given by Philippe Giguère, Professor at University Laval. I especially found awesome its rare visualization of the multi-head attention mechanism, which can be contemplated at the <a href=\"http://www2.ift.ulaval.ca/~pgiguere/cours/DeepLearning/09-Attention.pdf\" rel=\"noopener noreferrer\">slide 28 of week 13's class</a>.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2018/19/",
      "title": "Awesome Deep Learning Resources Updates on May 07 - May 13, 2018",
      "_short_title": "May 07 - May 13, 2018",
      "_slug": "2018/19/",
      "summary": "16 awesome projects updated on May 07 - May 13, 2018",
      "_filepath": "/content/2018/19/README.md",
      "url": "https://www.trackawesomelist.com/2018/19/",
      "date_published": "2018-05-12T19:30:48.000Z",
      "date_modified": "2018-05-12T21:24:59.000Z",
      "content_text": "\n\n### Books\n\n*   [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) - This book covers many of the core concepts behind neural networks and deep learning.\n\n### Papers / Recurrent Neural Networks\n\n*   [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) - Interesting overview of the subject of NMT, I mostly read part 8 about RNNs with attention as a refresher.\n*   [Adaptive Computation Time for Recurrent Neural Networks](https://arxiv.org/pdf/1603.08983v4.pdf) - Let RNNs decide how long they compute. I would love to see how well would it combines to Neural Turing Machines. Interesting interactive visualizations on the subject can be found [here](http://distill.pub/2016/augmented-rnns/).\n\n### Papers / Convolutional Neural Networks\n\n*   [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf) - The U-Net is an encoder-decoder CNN that also has skip-connections, good for image segmentation at a per-pixel level.\n*   [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/pdf/1611.09326.pdf) - Merges the ideas of the U-Net and the DenseNet, this new neural network is especially good for huge datasets in image segmentation.\n*   [Prototypical Networks for Few-shot Learning](https://arxiv.org/pdf/1703.05175.pdf) - Use a distance metric in the loss to determine to which class does an object belongs to from a few examples.\n\n### Papers / Attention Mechanisms\n\n*   [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf) - Attention mechanism for LSTMs! Mostly, figures and formulas and their explanations revealed to be useful to me. I gave a talk on that paper [here](https://www.youtube.com/watch?v=QuvRWevJMZ4).\n*   [Neural Turing Machines](https://arxiv.org/pdf/1410.5401v2.pdf) - Outstanding for letting a neural network learn an algorithm with seemingly good generalization over long time dependencies. Sequences recall problem.\n*   [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf) - LSTMs' attention mechanisms on CNNs feature maps does wonders.\n*   [Teaching Machines to Read and Comprehend](https://arxiv.org/pdf/1506.03340v3.pdf) - A very interesting and creative work about textual question answering, what a breakthrough, there is something to do with that.\n*   [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf) - Exploring different approaches to attention mechanisms.\n*   [Matching Networks for One Shot Learning](https://arxiv.org/pdf/1606.04080.pdf) - Interesting way of doing one-shot learning with low-data by using an attention mechanism and a query to compare an image to other images for classification.\n*   [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf) - In 2016: stacked residual LSTMs with attention mechanisms on encoder/decoder are the best for NMT (Neural Machine Translation).\n*   [Hybrid computing using a neural network with dynamic external memory](http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz) - Improvements on differentiable memory based on NTMs: now it is the Differentiable Neural Computer (DNC).\n*   [Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/pdf/1703.03906.pdf) - That yields intuition about the boundaries of what works for doing NMT within a framed seq2seq problem formulation.\n*   [Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram\n    Predictions](https://arxiv.org/pdf/1712.05884.pdf) - A [WaveNet](https://arxiv.org/pdf/1609.03499v2.pdf) used as a vocoder can be conditioned on generated Mel Spectrograms from the Tacotron 2 LSTM neural network with attention to generate neat audio from text.",
      "content_html": "<h3><p>Books</p>\n</h3>\n<ul>\n<li><a href=\"http://neuralnetworksanddeeplearning.com/index.html\" rel=\"noopener noreferrer\">Neural Networks and Deep Learning</a> - This book covers many of the core concepts behind neural networks and deep learning.</li>\n</ul>\n<h3><p>Papers / Recurrent Neural Networks</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1703.01619.pdf\" rel=\"noopener noreferrer\">Neural Machine Translation and Sequence-to-sequence Models: A Tutorial</a> - Interesting overview of the subject of NMT, I mostly read part 8 about RNNs with attention as a refresher.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1603.08983v4.pdf\" rel=\"noopener noreferrer\">Adaptive Computation Time for Recurrent Neural Networks</a> - Let RNNs decide how long they compute. I would love to see how well would it combines to Neural Turing Machines. Interesting interactive visualizations on the subject can be found <a href=\"http://distill.pub/2016/augmented-rnns/\" rel=\"noopener noreferrer\">here</a>.</li>\n</ul>\n<h3><p>Papers / Convolutional Neural Networks</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1505.04597.pdf\" rel=\"noopener noreferrer\">U-Net: Convolutional Networks for Biomedical Image Segmentation</a> - The U-Net is an encoder-decoder CNN that also has skip-connections, good for image segmentation at a per-pixel level.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1611.09326.pdf\" rel=\"noopener noreferrer\">The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation</a> - Merges the ideas of the U-Net and the DenseNet, this new neural network is especially good for huge datasets in image segmentation.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1703.05175.pdf\" rel=\"noopener noreferrer\">Prototypical Networks for Few-shot Learning</a> - Use a distance metric in the loss to determine to which class does an object belongs to from a few examples.</li>\n</ul>\n<h3><p>Papers / Attention Mechanisms</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1409.0473.pdf\" rel=\"noopener noreferrer\">Neural Machine Translation by Jointly Learning to Align and Translate</a> - Attention mechanism for LSTMs! Mostly, figures and formulas and their explanations revealed to be useful to me. I gave a talk on that paper <a href=\"https://www.youtube.com/watch?v=QuvRWevJMZ4\" rel=\"noopener noreferrer\">here</a>.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1410.5401v2.pdf\" rel=\"noopener noreferrer\">Neural Turing Machines</a> - Outstanding for letting a neural network learn an algorithm with seemingly good generalization over long time dependencies. Sequences recall problem.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1502.03044.pdf\" rel=\"noopener noreferrer\">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a> - LSTMs' attention mechanisms on CNNs feature maps does wonders.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1506.03340v3.pdf\" rel=\"noopener noreferrer\">Teaching Machines to Read and Comprehend</a> - A very interesting and creative work about textual question answering, what a breakthrough, there is something to do with that.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1508.04025.pdf\" rel=\"noopener noreferrer\">Effective Approaches to Attention-based Neural Machine Translation</a> - Exploring different approaches to attention mechanisms.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1606.04080.pdf\" rel=\"noopener noreferrer\">Matching Networks for One Shot Learning</a> - Interesting way of doing one-shot learning with low-data by using an attention mechanism and a query to compare an image to other images for classification.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1609.08144.pdf\" rel=\"noopener noreferrer\">Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</a> - In 2016: stacked residual LSTMs with attention mechanisms on encoder/decoder are the best for NMT (Neural Machine Translation).</li>\n</ul>\n\n<ul>\n<li><a href=\"http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz\" rel=\"noopener noreferrer\">Hybrid computing using a neural network with dynamic external memory</a> - Improvements on differentiable memory based on NTMs: now it is the Differentiable Neural Computer (DNC).</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1703.03906.pdf\" rel=\"noopener noreferrer\">Massive Exploration of Neural Machine Translation Architectures</a> - That yields intuition about the boundaries of what works for doing NMT within a framed seq2seq problem formulation.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1712.05884.pdf\" rel=\"noopener noreferrer\">Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram\nPredictions</a> - A <a href=\"https://arxiv.org/pdf/1609.03499v2.pdf\" rel=\"noopener noreferrer\">WaveNet</a> used as a vocoder can be conditioned on generated Mel Spectrograms from the Tacotron 2 LSTM neural network with attention to generate neat audio from text.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2017/46/",
      "title": "Awesome Deep Learning Resources Updates on Nov 13 - Nov 19, 2017",
      "_short_title": "Nov 13 - Nov 19, 2017",
      "_slug": "2017/46/",
      "summary": "1 awesome projects updated on Nov 13 - Nov 19, 2017",
      "_filepath": "/content/2017/46/README.md",
      "url": "https://www.trackawesomelist.com/2017/46/",
      "date_published": "2017-11-14T05:03:21.000Z",
      "date_modified": "2017-11-14T05:03:21.000Z",
      "content_text": "\n\n### Posts and Articles\n\n*   [Estimating an Optimal Learning Rate For a Deep Neural Network](https://medium.com/@surmenok/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0) - Clever trick to estimate an optimal learning rate prior any single full training.",
      "content_html": "<h3><p>Posts and Articles</p>\n</h3>\n<ul>\n<li><a href=\"https://medium.com/@surmenok/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\" rel=\"noopener noreferrer\">Estimating an Optimal Learning Rate For a Deep Neural Network</a> - Clever trick to estimate an optimal learning rate prior any single full training.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2017/44/",
      "title": "Awesome Deep Learning Resources Updates on Oct 30 - Nov 05, 2017",
      "_short_title": "Oct 30 - Nov 05, 2017",
      "_slug": "2017/44/",
      "summary": "2 awesome projects updated on Oct 30 - Nov 05, 2017",
      "_filepath": "/content/2017/44/README.md",
      "url": "https://www.trackawesomelist.com/2017/44/",
      "date_published": "2017-11-02T19:56:35.000Z",
      "date_modified": "2017-11-02T19:56:35.000Z",
      "content_text": "\n\n### Other Math Theory / Gradient Descent Algorithms & Optimization Theory\n\n*   [Neural Networks and Deep Learning, ch.2](http://neuralnetworksanddeeplearning.com/chap2.html) - Overview on how does the backpropagation algorithm works.\n*   [Neural Networks and Deep Learning, ch.4](http://neuralnetworksanddeeplearning.com/chap4.html) - A visual proof that neural nets can compute any function.",
      "content_html": "<h3><p>Other Math Theory / Gradient Descent Algorithms &amp; Optimization Theory</p>\n</h3>\n<ul>\n<li><a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\" rel=\"noopener noreferrer\">Neural Networks and Deep Learning, ch.2</a> - Overview on how does the backpropagation algorithm works.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://neuralnetworksanddeeplearning.com/chap4.html\" rel=\"noopener noreferrer\">Neural Networks and Deep Learning, ch.4</a> - A visual proof that neural nets can compute any function.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2017/41/",
      "title": "Awesome Deep Learning Resources Updates on Oct 09 - Oct 15, 2017",
      "_short_title": "Oct 09 - Oct 15, 2017",
      "_slug": "2017/41/",
      "summary": "1 awesome projects updated on Oct 09 - Oct 15, 2017",
      "_filepath": "/content/2017/41/README.md",
      "url": "https://www.trackawesomelist.com/2017/41/",
      "date_published": "2017-10-12T19:25:27.000Z",
      "date_modified": "2017-10-12T19:25:27.000Z",
      "content_text": "\n\n### Papers / Convolutional Neural Networks\n\n*   [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf) - Best Paper Award at CVPR 2017, yielding improvements on state-of-the-art performances on CIFAR-10, CIFAR-100 and SVHN datasets, this new neural network architecture is named DenseNet.",
      "content_html": "<h3><p>Papers / Convolutional Neural Networks</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1608.06993.pdf\" rel=\"noopener noreferrer\">Densely Connected Convolutional Networks</a> - Best Paper Award at CVPR 2017, yielding improvements on state-of-the-art performances on CIFAR-10, CIFAR-100 and SVHN datasets, this new neural network architecture is named DenseNet.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2017/38/",
      "title": "Awesome Deep Learning Resources Updates on Sep 18 - Sep 24, 2017",
      "_short_title": "Sep 18 - Sep 24, 2017",
      "_slug": "2017/38/",
      "summary": "1 awesome projects updated on Sep 18 - Sep 24, 2017",
      "_filepath": "/content/2017/38/README.md",
      "url": "https://www.trackawesomelist.com/2017/38/",
      "date_published": "2017-09-17T23:03:10.000Z",
      "date_modified": "2017-09-17T23:03:10.000Z",
      "content_text": "\n\n### YouTube and Videos / Other\n\n*   [Geoffrey Hinton interview](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/dcm5r/geoffrey-hinton-interview) - Andrew Ng interviews Geoffrey Hinton, who talks about his research and breaktroughs, and gives advice for students.",
      "content_html": "<h3><p>YouTube and Videos / Other</p>\n</h3>\n<ul>\n<li><a href=\"https://www.coursera.org/learn/neural-networks-deep-learning/lecture/dcm5r/geoffrey-hinton-interview\" rel=\"noopener noreferrer\">Geoffrey Hinton interview</a> - Andrew Ng interviews Geoffrey Hinton, who talks about his research and breaktroughs, and gives advice for students.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2017/34/",
      "title": "Awesome Deep Learning Resources Updates on Aug 21 - Aug 27, 2017",
      "_short_title": "Aug 21 - Aug 27, 2017",
      "_slug": "2017/34/",
      "summary": "4 awesome projects updated on Aug 21 - Aug 27, 2017",
      "_filepath": "/content/2017/34/README.md",
      "url": "https://www.trackawesomelist.com/2017/34/",
      "date_published": "2017-08-21T06:05:16.000Z",
      "date_modified": "2017-08-21T06:05:16.000Z",
      "content_text": "\n\n### Online Classes\n\n*   [Machine Learning by Andrew Ng on Coursera](https://www.coursera.org/learn/machine-learning) - Renown entry-level online class with [certificate](https://www.coursera.org/account/accomplishments/verify/DXPXHYFNGKG3). Taught by: Andrew Ng, Associate Professor, Stanford University; Chief Scientist, Baidu; Chairman and Co-founder, Coursera.\n*   [Deep Learning by Google](https://www.udacity.com/course/deep-learning--ud730) - Good intermediate to advanced-level course covering high-level deep learning concepts, I found it helps to get creative once the basics are acquired.\n*   [Machine Learning for Trading by Georgia Tech](https://www.udacity.com/course/machine-learning-for-trading--ud501) - Interesting class for acquiring basic knowledge of machine learning applied to trading and some AI and finance concepts. I especially liked the section on Q-Learning.\n*   [Neural networks class by Hugo Larochelle, Université de Sherbrooke](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH) - Interesting class about neural networks available online for free by Hugo Larochelle, yet I have watched a few of those videos.",
      "content_html": "<h3><p>Online Classes</p>\n</h3>\n<ul>\n<li><a href=\"https://www.coursera.org/learn/machine-learning\" rel=\"noopener noreferrer\">Machine Learning by Andrew Ng on Coursera</a> - Renown entry-level online class with <a href=\"https://www.coursera.org/account/accomplishments/verify/DXPXHYFNGKG3\" rel=\"noopener noreferrer\">certificate</a>. Taught by: Andrew Ng, Associate Professor, Stanford University; Chief Scientist, Baidu; Chairman and Co-founder, Coursera.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.udacity.com/course/deep-learning--ud730\" rel=\"noopener noreferrer\">Deep Learning by Google</a> - Good intermediate to advanced-level course covering high-level deep learning concepts, I found it helps to get creative once the basics are acquired.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.udacity.com/course/machine-learning-for-trading--ud501\" rel=\"noopener noreferrer\">Machine Learning for Trading by Georgia Tech</a> - Interesting class for acquiring basic knowledge of machine learning applied to trading and some AI and finance concepts. I especially liked the section on Q-Learning.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH\" rel=\"noopener noreferrer\">Neural networks class by Hugo Larochelle, Université de Sherbrooke</a> - Interesting class about neural networks available online for free by Hugo Larochelle, yet I have watched a few of those videos.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2017/32/",
      "title": "Awesome Deep Learning Resources Updates on Aug 07 - Aug 13, 2017",
      "_short_title": "Aug 07 - Aug 13, 2017",
      "_slug": "2017/32/",
      "summary": "1 awesome projects updated on Aug 07 - Aug 13, 2017",
      "_filepath": "/content/2017/32/README.md",
      "url": "https://www.trackawesomelist.com/2017/32/",
      "date_published": "2017-08-11T19:53:13.000Z",
      "date_modified": "2017-08-11T19:53:13.000Z",
      "content_text": "\n\n### Posts and Articles\n\n*   [Hyperopt tutorial for Optimizing Neural Networks’ Hyperparameters](http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/) - Learn to slay down hyperparameter spaces automatically rather than by hand.",
      "content_html": "<h3><p>Posts and Articles</p>\n</h3>\n<ul>\n<li><a href=\"http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/\" rel=\"noopener noreferrer\">Hyperopt tutorial for Optimizing Neural Networks’ Hyperparameters</a> - Learn to slay down hyperparameter spaces automatically rather than by hand.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2017/30/",
      "title": "Awesome Deep Learning Resources Updates on Jul 24 - Jul 30, 2017",
      "_short_title": "Jul 24 - Jul 30, 2017",
      "_slug": "2017/30/",
      "summary": "71 awesome projects updated on Jul 24 - Jul 30, 2017",
      "_filepath": "/content/2017/30/README.md",
      "url": "https://www.trackawesomelist.com/2017/30/",
      "date_published": "2017-07-28T03:11:36.000Z",
      "date_modified": "2017-07-28T03:11:36.000Z",
      "content_text": "\n\n### Posts and Articles\n\n*   [Predictions made by Ray Kurzweil](https://en.wikipedia.org/wiki/Predictions_made_by_Ray_Kurzweil) - List of mid to long term futuristic predictions made by Ray Kurzweil.\n*   [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - MUST READ post by Andrej Karpathy - this is what motivated me to learn RNNs, it demonstrates what it can achieve in the most basic form of NLP.\n*   [Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) - Fresh look on how neurons map information.\n*   [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) - Explains the LSTM cells' inner workings, plus, it has interesting links in conclusion.\n*   [Recommending music on Spotify with deep learning](http://benanne.github.io/2014/08/05/spotify-cnns.html) - Awesome for doing clustering on audio - post by an intern at Spotify.\n*   [Announcing SyntaxNet: The World’s Most Accurate Parser Goes Open Source](https://research.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html) - Parsey McParseface's birth, a neural syntax tree parser.\n*   [Improving Inception and Image Classification in TensorFlow](https://research.googleblog.com/2016/08/improving-inception-and-image.html) - Very interesting CNN architecture (e.g.: the inception-style convolutional layers is promising and efficient in terms of reducing the number of parameters).\n*   [WaveNet: A Generative Model for Raw Audio](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) - Realistic talking machines: perfect voice generation.\n*   [François Chollet's Twitter](https://twitter.com/fchollet) - Author of Keras - has interesting Twitter posts and innovative ideas.\n*   [Migrating to Git LFS for Developing Deep Learning Applications with Large Files](http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/) - Easily manage huge files in your private Git projects.\n*   [The future of deep learning](https://blog.keras.io/the-future-of-deep-learning.html) - François Chollet's thoughts on the future of deep learning.\n*   [Discover structure behind data with decision trees](http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/) - Grow decision trees and visualize them, infer the hidden logic behind data.\n\n### Practical Resources / Librairies and Implementations\n\n*   [skflow (⭐3.2k)](https://github.com/tensorflow/skflow) - TensorFlow wrapper à la scikit-learn.\n*   [carpedm20's repositories](https://github.com/carpedm20) - Many interesting neural network architectures are implemented by the Korean guy Taehoon Kim, A.K.A. carpedm20.\n*   [carpedm20/NTM-tensorflow (⭐1k)](https://github.com/carpedm20/NTM-tensorflow) - Neural Turing Machine TensorFlow implementation.\n*   [Deep learning for lazybones](http://oduerr.github.io/blog/2016/04/06/Deep-Learning_for_lazybones) - Transfer learning tutorial in TensorFlow for vision from high-level embeddings of a pretrained CNN, AlexNet 2012.\n*   [LSTM for Human Activity Recognition (HAR) (⭐3.4k)](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition) - Tutorial of mine on using LSTMs on time series for classification.\n*   [Deep stacked residual bidirectional LSTMs for HAR (⭐318)](https://github.com/guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs) - Improvements on the previous project.\n*   [Sequence to Sequence (seq2seq) Recurrent Neural Network (RNN) for Time Series Prediction (⭐1.1k)](https://github.com/guillaume-chevalier/seq2seq-signal-prediction) - Tutorial of mine on how to predict temporal sequences of numbers - that may be multichannel.\n*   [ML / DL repositories I starred](https://github.com/guillaume-chevalier?direction=desc\\&page=1\\&q=machine+OR+deep+OR+learning+OR+rnn+OR+lstm+OR+cnn\\&sort=stars\\&tab=stars\\&utf8=%E2%9C%93) - GitHub is full of nice code samples & projects.\n\n### Practical Resources / Some Datasets\n\n*   [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html) - TONS of datasets for ML.\n*   [Cornell Movie--Dialogs Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) - This could be used for a chatbot.\n*   [SQuAD The Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/) - Question answering dataset that can be explored online, and a list of models performing well on that dataset.\n*   [LibriSpeech ASR corpus](http://www.openslr.org/12/) - Huge free English speech dataset with balanced genders and speakers, that seems to be of high quality.\n*   [Awesome Public Datasets (⭐64k)](https://github.com/caesar0301/awesome-public-datasets) - An awesome list of public datasets.\n\n### Other Math Theory / Gradient Descent Algorithms & Optimization Theory\n\n*   [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.mr5wq61fb) - Exposing backprop's caveats and the importance of knowing that while training models.\n*   [Artificial Neural Networks: Mathematics of Backpropagation](http://briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4) - Picturing backprop, mathematically.\n*   [Deep Learning Lecture 12: Recurrent Neural Nets and LSTMs](https://www.youtube.com/watch?v=56TYLaQN4N8) - Unfolding of RNN graphs is explained properly, and potential problems about gradient descent algorithms are exposed.\n*   [Gradient descent algorithms in a saddle point](http://sebastianruder.com/content/images/2016/09/saddle_point_evaluation_optimizers.gif) - Visualize how different optimizers interacts with a saddle points.\n*   [Gradient descent algorithms in an almost flat landscape](https://devblogs.nvidia.com/wp-content/uploads/2015/12/NKsFHJb.gif) - Visualize how different optimizers interacts with an almost flat landscape.\n*   [Gradient Descent](https://www.youtube.com/watch?v=F6GSRDoB-Cg) - Okay, I already listed Andrew NG's Coursera class above, but this video especially is quite pertinent as an introduction and defines the gradient descent algorithm.\n*   [Gradient Descent: Intuition](https://www.youtube.com/watch?v=YovTqTY-PYY) - What follows from the previous video: now add intuition.\n*   [Gradient Descent in Practice 2: Learning Rate](https://www.youtube.com/watch?v=gX6fZHgfrow) - How to adjust the learning rate of a neural network.\n*   [The Problem of Overfitting](https://www.youtube.com/watch?v=u73PU6Qwl1I) - A good explanation of overfitting and how to address that problem.\n*   [Diagnosing Bias vs Variance](https://www.youtube.com/watch?v=ewogYw5oCAI) - Understanding bias and variance in the predictions of a neural net and how to address those problems.\n*   [Self-Normalizing Neural Networks](https://arxiv.org/pdf/1706.02515.pdf) - Appearance of the incredible SELU activation function.\n*   [Learning to learn by gradient descent by gradient descent](https://arxiv.org/pdf/1606.04474.pdf) - RNN as an optimizer: introducing the L2L optimizer, a meta-neural network.\n\n### Other Math Theory / Complex Numbers & Digital Signal Processing\n\n*   [MathBox, Tools for Thought Graphical Algebra and Fourier Analysis](https://acko.net/files/gltalks/toolsforthought/) - New look on Fourier analysis.\n*   [How to Fold a Julia Fractal](http://acko.net/blog/how-to-fold-a-julia-fractal/) - Animations dealing with complex numbers and wave equations.\n*   [Animate Your Way to Glory, Math and Physics in Motion](http://acko.net/blog/animate-your-way-to-glory/) - Convergence methods in physic engines, and applied to interaction design.\n*   [Animate Your Way to Glory - Part II, Math and Physics in Motion](http://acko.net/blog/animate-your-way-to-glory-pt2/) - Nice animations for rotation and rotation interpolation with Quaternions, a mathematical object for handling 3D rotations.\n*   [Filtering signal, plotting the STFT and the Laplace transform (⭐67)](https://github.com/guillaume-chevalier/filtering-stft-and-laplace-transform) - Simple Python demo on signal processing.\n\n### Papers / Recurrent Neural Networks\n\n*   [Deep Learning in Neural Networks: An Overview](https://arxiv.org/pdf/1404.7828v4.pdf) - You\\_Again's summary/overview of deep learning, mostly about RNNs.\n*   [Bidirectional Recurrent Neural Networks](http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf) - Better classifications with RNNs with bidirectional scanning on the time axis.\n*   [Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) - 4 stacked LSTM cells of 1000 hidden size with reversed input sentences, and with beam search, on the WMT’14 English to French dataset.\n*   [Exploring the Limits of Language Modeling](https://arxiv.org/pdf/1602.02410.pdf) - Nice recursive models using word-level LSTMs on top of a character-level CNN using an overkill amount of GPU power.\n*   [Exploring the Depths of Recurrent Neural Networks with Stochastic Residual Learning](https://cs224d.stanford.edu/reports/PradhanLongpre.pdf) - Basically, residual connections can be better than stacked RNNs in the presented case of sentiment analysis.\n*   [Pixel Recurrent Neural Networks](https://arxiv.org/pdf/1601.06759.pdf) - Nice for photoshop-like \"content aware fill\" to fill missing patches in images.\n\n### Papers / Convolutional Neural Networks\n\n*   [What is the Best Multi-Stage Architecture for Object Recognition?](http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf) - Awesome for the use of \"local contrast normalization\".\n*   [ImageNet Classification with Deep Convolutional Neural Networks](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) - AlexNet, 2012 ILSVRC, breakthrough of the ReLU activation function.\n*   [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901v3.pdf) - For the \"deconvnet layer\".\n*   [Fast and Accurate Deep Network Learning by Exponential Linear Units](https://arxiv.org/pdf/1511.07289v1.pdf) - ELU activation function for CIFAR vision tasks.\n*   [Going Deeper with Convolutions](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) - GoogLeNet: Appearance of \"Inception\" layers/modules, the idea is of parallelizing conv layers into many mini-conv of different size with \"same\" padding, concatenated on depth.\n*   [Highway Networks](https://arxiv.org/pdf/1505.00387v2.pdf) - Highway networks: residual connections.\n*   [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167v3.pdf) - Batch normalization (BN): to normalize a layer's output by also summing over the entire batch, and then performing a linear rescaling and shifting of a certain trainable amount.\n*   [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385v1.pdf) - Very deep residual layers with batch normalization layers - a.k.a. \"how to overfit any vision dataset with too many layers and make any vision model work properly at recognition given enough data\".\n*   [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/pdf/1602.07261v2.pdf) - For improving GoogLeNet with residual connections.\n*   [WaveNet: a Generative Model for Raw Audio](https://arxiv.org/pdf/1609.03499v2.pdf) - Epic raw voice/music generation with new architectures based on dilated causal convolutions to capture more audio length.\n*   [Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling](https://arxiv.org/pdf/1610.07584v2.pdf) - 3D-GANs for 3D model generation and fun 3D furniture arithmetics from embeddings (think like word2vec word arithmetics with 3D furniture representations).\n*   [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://research.fb.com/publications/ImageNet1kIn1h/) - Incredibly fast distributed training of a CNN.\n\n### YouTube and Videos / Other\n\n*   [Attention Mechanisms in Recurrent Neural Networks (RNNs) - IGGG](https://www.youtube.com/watch?v=QuvRWevJMZ4) - A talk for a reading group on attention mechanisms (Paper: Neural Machine Translation by Jointly Learning to Align and Translate).\n*   [Tensor Calculus and the Calculus of Moving Surfaces](https://www.youtube.com/playlist?list=PLlXfTHzgMRULkodlIEqfgTS-H1AY_bNtq) - Generalize properly how Tensors work, yet just watching a few videos already helps a lot to grasp the concepts.\n*   [Deep Learning & Machine Learning (Advanced topics)](https://www.youtube.com/playlist?list=PLlp-GWNOd6m4C_-9HxuHg2_ZeI2Yzwwqt) - A list of videos about deep learning that I found interesting or useful, this is a mix of a bit of everything.\n*   [Signal Processing Playlist](https://www.youtube.com/playlist?list=PLlp-GWNOd6m6gSz0wIcpvl4ixSlS-HEmr) - A YouTube playlist I composed about DFT/FFT, STFT and the Laplace transform - I was mad about my software engineering bachelor not including signal processing classes (except a bit in the quantum physics class).\n*   [Computer Science](https://www.youtube.com/playlist?list=PLlp-GWNOd6m7vLOsW20xAJ81-65C-Ys6k) - Yet another YouTube playlist I composed, this time about various CS topics.\n*   [Siraj's Channel](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A/videos?view=0\\&sort=p\\&flow=grid) - Siraj has entertaining, fast-paced video tutorials about deep learning.\n*   [Two Minute Papers' Channel](https://www.youtube.com/user/keeroyz/videos?sort=p\\&view=0\\&flow=grid) - Interesting and shallow overview of some research papers, for example about WaveNet or Neural Style Transfer.\n\n### Misc. Hubs & Links / Other\n\n*   [Hacker News](https://news.ycombinator.com/news) - Maybe how I discovered ML - Interesting trends appear on that site way before they get to be a big deal.\n*   [DataTau](http://www.datatau.com/) - This is a hub similar to Hacker News, but specific to data science.\n*   [Naver](http://www.naver.com/) - This is a Korean search engine - best used with Google Translate, ironically. Surprisingly, sometimes deep learning search results and comprehensible advanced math content shows up more easily there than on Google search.\n*   [Arxiv Sanity Preserver](http://www.arxiv-sanity.com/) - arXiv browser with TF/IDF features.",
      "content_html": "<h3><p>Posts and Articles</p>\n</h3>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Predictions_made_by_Ray_Kurzweil\" rel=\"noopener noreferrer\">Predictions made by Ray Kurzweil</a> - List of mid to long term futuristic predictions made by Ray Kurzweil.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" rel=\"noopener noreferrer\">The Unreasonable Effectiveness of Recurrent Neural Networks</a> - MUST READ post by Andrej Karpathy - this is what motivated me to learn RNNs, it demonstrates what it can achieve in the most basic form of NLP.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\" rel=\"noopener noreferrer\">Neural Networks, Manifolds, and Topology</a> - Fresh look on how neurons map information.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" rel=\"noopener noreferrer\">Understanding LSTM Networks</a> - Explains the LSTM cells' inner workings, plus, it has interesting links in conclusion.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://benanne.github.io/2014/08/05/spotify-cnns.html\" rel=\"noopener noreferrer\">Recommending music on Spotify with deep learning</a> - Awesome for doing clustering on audio - post by an intern at Spotify.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://research.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html\" rel=\"noopener noreferrer\">Announcing SyntaxNet: The World’s Most Accurate Parser Goes Open Source</a> - Parsey McParseface's birth, a neural syntax tree parser.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://research.googleblog.com/2016/08/improving-inception-and-image.html\" rel=\"noopener noreferrer\">Improving Inception and Image Classification in TensorFlow</a> - Very interesting CNN architecture (e.g.: the inception-style convolutional layers is promising and efficient in terms of reducing the number of parameters).</li>\n</ul>\n\n<ul>\n<li><a href=\"https://deepmind.com/blog/wavenet-generative-model-raw-audio/\" rel=\"noopener noreferrer\">WaveNet: A Generative Model for Raw Audio</a> - Realistic talking machines: perfect voice generation.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://twitter.com/fchollet\" rel=\"noopener noreferrer\">François Chollet's Twitter</a> - Author of Keras - has interesting Twitter posts and innovative ideas.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/\" rel=\"noopener noreferrer\">Migrating to Git LFS for Developing Deep Learning Applications with Large Files</a> - Easily manage huge files in your private Git projects.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://blog.keras.io/the-future-of-deep-learning.html\" rel=\"noopener noreferrer\">The future of deep learning</a> - François Chollet's thoughts on the future of deep learning.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/\" rel=\"noopener noreferrer\">Discover structure behind data with decision trees</a> - Grow decision trees and visualize them, infer the hidden logic behind data.</li>\n</ul>\n<h3><p>Practical Resources / Librairies and Implementations</p>\n</h3>\n<ul>\n<li><a href=\"https://github.com/tensorflow/skflow\" rel=\"noopener noreferrer\">skflow (⭐3.2k)</a> - TensorFlow wrapper à la scikit-learn.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://github.com/carpedm20\" rel=\"noopener noreferrer\">carpedm20's repositories</a> - Many interesting neural network architectures are implemented by the Korean guy Taehoon Kim, A.K.A. carpedm20.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://github.com/carpedm20/NTM-tensorflow\" rel=\"noopener noreferrer\">carpedm20/NTM-tensorflow (⭐1k)</a> - Neural Turing Machine TensorFlow implementation.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://oduerr.github.io/blog/2016/04/06/Deep-Learning_for_lazybones\" rel=\"noopener noreferrer\">Deep learning for lazybones</a> - Transfer learning tutorial in TensorFlow for vision from high-level embeddings of a pretrained CNN, AlexNet 2012.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\" rel=\"noopener noreferrer\">LSTM for Human Activity Recognition (HAR) (⭐3.4k)</a> - Tutorial of mine on using LSTMs on time series for classification.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://github.com/guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs\" rel=\"noopener noreferrer\">Deep stacked residual bidirectional LSTMs for HAR (⭐318)</a> - Improvements on the previous project.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://github.com/guillaume-chevalier/seq2seq-signal-prediction\" rel=\"noopener noreferrer\">Sequence to Sequence (seq2seq) Recurrent Neural Network (RNN) for Time Series Prediction (⭐1.1k)</a> - Tutorial of mine on how to predict temporal sequences of numbers - that may be multichannel.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://github.com/guillaume-chevalier?direction=desc&amp;page=1&amp;q=machine+OR+deep+OR+learning+OR+rnn+OR+lstm+OR+cnn&amp;sort=stars&amp;tab=stars&amp;utf8=%E2%9C%93\" rel=\"noopener noreferrer\">ML / DL repositories I starred</a> - GitHub is full of nice code samples &amp; projects.</li>\n</ul>\n<h3><p>Practical Resources / Some Datasets</p>\n</h3>\n<ul>\n<li><a href=\"https://archive.ics.uci.edu/ml/datasets.html\" rel=\"noopener noreferrer\">UCI Machine Learning Repository</a> - TONS of datasets for ML.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\" rel=\"noopener noreferrer\">Cornell Movie--Dialogs Corpus</a> - This could be used for a chatbot.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://rajpurkar.github.io/SQuAD-explorer/\" rel=\"noopener noreferrer\">SQuAD The Stanford Question Answering Dataset</a> - Question answering dataset that can be explored online, and a list of models performing well on that dataset.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://www.openslr.org/12/\" rel=\"noopener noreferrer\">LibriSpeech ASR corpus</a> - Huge free English speech dataset with balanced genders and speakers, that seems to be of high quality.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://github.com/caesar0301/awesome-public-datasets\" rel=\"noopener noreferrer\">Awesome Public Datasets (⭐64k)</a> - An awesome list of public datasets.</li>\n</ul>\n<h3><p>Other Math Theory / Gradient Descent Algorithms &amp; Optimization Theory</p>\n</h3>\n<ul>\n<li><a href=\"https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.mr5wq61fb\" rel=\"noopener noreferrer\">Yes you should understand backprop</a> - Exposing backprop's caveats and the importance of knowing that while training models.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4\" rel=\"noopener noreferrer\">Artificial Neural Networks: Mathematics of Backpropagation</a> - Picturing backprop, mathematically.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=56TYLaQN4N8\" rel=\"noopener noreferrer\">Deep Learning Lecture 12: Recurrent Neural Nets and LSTMs</a> - Unfolding of RNN graphs is explained properly, and potential problems about gradient descent algorithms are exposed.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://sebastianruder.com/content/images/2016/09/saddle_point_evaluation_optimizers.gif\" rel=\"noopener noreferrer\">Gradient descent algorithms in a saddle point</a> - Visualize how different optimizers interacts with a saddle points.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://devblogs.nvidia.com/wp-content/uploads/2015/12/NKsFHJb.gif\" rel=\"noopener noreferrer\">Gradient descent algorithms in an almost flat landscape</a> - Visualize how different optimizers interacts with an almost flat landscape.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=F6GSRDoB-Cg\" rel=\"noopener noreferrer\">Gradient Descent</a> - Okay, I already listed Andrew NG's Coursera class above, but this video especially is quite pertinent as an introduction and defines the gradient descent algorithm.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=YovTqTY-PYY\" rel=\"noopener noreferrer\">Gradient Descent: Intuition</a> - What follows from the previous video: now add intuition.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=gX6fZHgfrow\" rel=\"noopener noreferrer\">Gradient Descent in Practice 2: Learning Rate</a> - How to adjust the learning rate of a neural network.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=u73PU6Qwl1I\" rel=\"noopener noreferrer\">The Problem of Overfitting</a> - A good explanation of overfitting and how to address that problem.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=ewogYw5oCAI\" rel=\"noopener noreferrer\">Diagnosing Bias vs Variance</a> - Understanding bias and variance in the predictions of a neural net and how to address those problems.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1706.02515.pdf\" rel=\"noopener noreferrer\">Self-Normalizing Neural Networks</a> - Appearance of the incredible SELU activation function.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1606.04474.pdf\" rel=\"noopener noreferrer\">Learning to learn by gradient descent by gradient descent</a> - RNN as an optimizer: introducing the L2L optimizer, a meta-neural network.</li>\n</ul>\n<h3><p>Other Math Theory / Complex Numbers &amp; Digital Signal Processing</p>\n</h3>\n<ul>\n<li><a href=\"https://acko.net/files/gltalks/toolsforthought/\" rel=\"noopener noreferrer\">MathBox, Tools for Thought Graphical Algebra and Fourier Analysis</a> - New look on Fourier analysis.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://acko.net/blog/how-to-fold-a-julia-fractal/\" rel=\"noopener noreferrer\">How to Fold a Julia Fractal</a> - Animations dealing with complex numbers and wave equations.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://acko.net/blog/animate-your-way-to-glory/\" rel=\"noopener noreferrer\">Animate Your Way to Glory, Math and Physics in Motion</a> - Convergence methods in physic engines, and applied to interaction design.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://acko.net/blog/animate-your-way-to-glory-pt2/\" rel=\"noopener noreferrer\">Animate Your Way to Glory - Part II, Math and Physics in Motion</a> - Nice animations for rotation and rotation interpolation with Quaternions, a mathematical object for handling 3D rotations.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://github.com/guillaume-chevalier/filtering-stft-and-laplace-transform\" rel=\"noopener noreferrer\">Filtering signal, plotting the STFT and the Laplace transform (⭐67)</a> - Simple Python demo on signal processing.</li>\n</ul>\n<h3><p>Papers / Recurrent Neural Networks</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1404.7828v4.pdf\" rel=\"noopener noreferrer\">Deep Learning in Neural Networks: An Overview</a> - You_Again's summary/overview of deep learning, mostly about RNNs.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf\" rel=\"noopener noreferrer\">Bidirectional Recurrent Neural Networks</a> - Better classifications with RNNs with bidirectional scanning on the time axis.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\" rel=\"noopener noreferrer\">Sequence to Sequence Learning with Neural Networks</a> - 4 stacked LSTM cells of 1000 hidden size with reversed input sentences, and with beam search, on the WMT’14 English to French dataset.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1602.02410.pdf\" rel=\"noopener noreferrer\">Exploring the Limits of Language Modeling</a> - Nice recursive models using word-level LSTMs on top of a character-level CNN using an overkill amount of GPU power.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://cs224d.stanford.edu/reports/PradhanLongpre.pdf\" rel=\"noopener noreferrer\">Exploring the Depths of Recurrent Neural Networks with Stochastic Residual Learning</a> - Basically, residual connections can be better than stacked RNNs in the presented case of sentiment analysis.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1601.06759.pdf\" rel=\"noopener noreferrer\">Pixel Recurrent Neural Networks</a> - Nice for photoshop-like \"content aware fill\" to fill missing patches in images.</li>\n</ul>\n<h3><p>Papers / Convolutional Neural Networks</p>\n</h3>\n<ul>\n<li><a href=\"http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf\" rel=\"noopener noreferrer\">What is the Best Multi-Stage Architecture for Object Recognition?</a> - Awesome for the use of \"local contrast normalization\".</li>\n</ul>\n\n<ul>\n<li><a href=\"http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf\" rel=\"noopener noreferrer\">ImageNet Classification with Deep Convolutional Neural Networks</a> - AlexNet, 2012 ILSVRC, breakthrough of the ReLU activation function.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1311.2901v3.pdf\" rel=\"noopener noreferrer\">Visualizing and Understanding Convolutional Networks</a> - For the \"deconvnet layer\".</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1511.07289v1.pdf\" rel=\"noopener noreferrer\">Fast and Accurate Deep Network Learning by Exponential Linear Units</a> - ELU activation function for CIFAR vision tasks.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf\" rel=\"noopener noreferrer\">Going Deeper with Convolutions</a> - GoogLeNet: Appearance of \"Inception\" layers/modules, the idea is of parallelizing conv layers into many mini-conv of different size with \"same\" padding, concatenated on depth.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1505.00387v2.pdf\" rel=\"noopener noreferrer\">Highway Networks</a> - Highway networks: residual connections.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1502.03167v3.pdf\" rel=\"noopener noreferrer\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> - Batch normalization (BN): to normalize a layer's output by also summing over the entire batch, and then performing a linear rescaling and shifting of a certain trainable amount.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1512.03385v1.pdf\" rel=\"noopener noreferrer\">Deep Residual Learning for Image Recognition</a> - Very deep residual layers with batch normalization layers - a.k.a. \"how to overfit any vision dataset with too many layers and make any vision model work properly at recognition given enough data\".</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1602.07261v2.pdf\" rel=\"noopener noreferrer\">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a> - For improving GoogLeNet with residual connections.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1609.03499v2.pdf\" rel=\"noopener noreferrer\">WaveNet: a Generative Model for Raw Audio</a> - Epic raw voice/music generation with new architectures based on dilated causal convolutions to capture more audio length.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1610.07584v2.pdf\" rel=\"noopener noreferrer\">Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling</a> - 3D-GANs for 3D model generation and fun 3D furniture arithmetics from embeddings (think like word2vec word arithmetics with 3D furniture representations).</li>\n</ul>\n\n<ul>\n<li><a href=\"https://research.fb.com/publications/ImageNet1kIn1h/\" rel=\"noopener noreferrer\">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a> - Incredibly fast distributed training of a CNN.</li>\n</ul>\n<h3><p>YouTube and Videos / Other</p>\n</h3>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=QuvRWevJMZ4\" rel=\"noopener noreferrer\">Attention Mechanisms in Recurrent Neural Networks (RNNs) - IGGG</a> - A talk for a reading group on attention mechanisms (Paper: Neural Machine Translation by Jointly Learning to Align and Translate).</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/playlist?list=PLlXfTHzgMRULkodlIEqfgTS-H1AY_bNtq\" rel=\"noopener noreferrer\">Tensor Calculus and the Calculus of Moving Surfaces</a> - Generalize properly how Tensors work, yet just watching a few videos already helps a lot to grasp the concepts.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/playlist?list=PLlp-GWNOd6m4C_-9HxuHg2_ZeI2Yzwwqt\" rel=\"noopener noreferrer\">Deep Learning &amp; Machine Learning (Advanced topics)</a> - A list of videos about deep learning that I found interesting or useful, this is a mix of a bit of everything.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/playlist?list=PLlp-GWNOd6m6gSz0wIcpvl4ixSlS-HEmr\" rel=\"noopener noreferrer\">Signal Processing Playlist</a> - A YouTube playlist I composed about DFT/FFT, STFT and the Laplace transform - I was mad about my software engineering bachelor not including signal processing classes (except a bit in the quantum physics class).</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/playlist?list=PLlp-GWNOd6m7vLOsW20xAJ81-65C-Ys6k\" rel=\"noopener noreferrer\">Computer Science</a> - Yet another YouTube playlist I composed, this time about various CS topics.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A/videos?view=0&amp;sort=p&amp;flow=grid\" rel=\"noopener noreferrer\">Siraj's Channel</a> - Siraj has entertaining, fast-paced video tutorials about deep learning.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.youtube.com/user/keeroyz/videos?sort=p&amp;view=0&amp;flow=grid\" rel=\"noopener noreferrer\">Two Minute Papers' Channel</a> - Interesting and shallow overview of some research papers, for example about WaveNet or Neural Style Transfer.</li>\n</ul>\n<h3><p>Misc. Hubs &amp; Links / Other</p>\n</h3>\n<ul>\n<li><a href=\"https://news.ycombinator.com/news\" rel=\"noopener noreferrer\">Hacker News</a> - Maybe how I discovered ML - Interesting trends appear on that site way before they get to be a big deal.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://www.datatau.com/\" rel=\"noopener noreferrer\">DataTau</a> - This is a hub similar to Hacker News, but specific to data science.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://www.naver.com/\" rel=\"noopener noreferrer\">Naver</a> - This is a Korean search engine - best used with Google Translate, ironically. Surprisingly, sometimes deep learning search results and comprehensible advanced math content shows up more easily there than on Google search.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://www.arxiv-sanity.com/\" rel=\"noopener noreferrer\">Arxiv Sanity Preserver</a> - arXiv browser with TF/IDF features.</li>\n</ul>\n"
    },
    {
      "id": "https://www.trackawesomelist.com/2017/19/",
      "title": "Awesome Deep Learning Resources Updates on May 08 - May 14, 2017",
      "_short_title": "May 08 - May 14, 2017",
      "_slug": "2017/19/",
      "summary": "8 awesome projects updated on May 08 - May 14, 2017",
      "_filepath": "/content/2017/19/README.md",
      "url": "https://www.trackawesomelist.com/2017/19/",
      "date_published": "2017-05-09T22:43:47.000Z",
      "date_modified": "2017-05-13T04:52:58.000Z",
      "content_text": "\n\n### Books\n\n*   [How to Create a Mind](https://www.amazon.com/How-Create-Mind-Thought-Revealed/dp/B009VSFXZ4) - The audio version is nice to listen to while commuting. This book is motivating about reverse-engineering the mind and thinking on how to code AI.\n*   [Deep Learning - An MIT Press book](http://www.deeplearningbook.org/) - Yet halfway through the book, it contains satisfying math content on how to think about actual deep learning.\n\n### Posts and Articles\n\n*   [Attention and Augmented Recurrent Neural Networks](http://distill.pub/2016/augmented-rnns/) - Interesting for visual animations, it is a nice intro to attention mechanisms as an example.\n*   [Neuralink and the Brain’s Magical Future](http://waitbutwhy.com/2017/04/neuralink.html) - Thought provoking article about the future of the brain and brain-computer interfaces.\n\n### Practical Resources / Librairies and Implementations\n\n*   [TensorFlow's GitHub repository (⭐191k)](https://github.com/tensorflow/tensorflow) - Most known deep learning framework, both high-level and low-level while staying flexible.\n*   [Keras](https://keras.io/) - Keras is another intersting deep learning framework like TensorFlow, it is mostly high-level.\n\n### Papers / Recurrent Neural Networks\n\n*   [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078v3.pdf) - Two networks in one combined into a seq2seq (sequence to sequence) Encoder-Decoder architecture. RNN Encoder–Decoder with 1000 hidden units. Adadelta optimizer.\n\n### Papers / Convolutional Neural Networks\n\n*   [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556v6.pdf) - Interesting idea of stacking multiple 3x3 conv+ReLU before pooling for a bigger filter size with just a few parameters. There is also a nice table for \"ConvNet Configuration\".",
      "content_html": "<h3><p>Books</p>\n</h3>\n<ul>\n<li><a href=\"https://www.amazon.com/How-Create-Mind-Thought-Revealed/dp/B009VSFXZ4\" rel=\"noopener noreferrer\">How to Create a Mind</a> - The audio version is nice to listen to while commuting. This book is motivating about reverse-engineering the mind and thinking on how to code AI.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://www.deeplearningbook.org/\" rel=\"noopener noreferrer\">Deep Learning - An MIT Press book</a> - Yet halfway through the book, it contains satisfying math content on how to think about actual deep learning.</li>\n</ul>\n<h3><p>Posts and Articles</p>\n</h3>\n<ul>\n<li><a href=\"http://distill.pub/2016/augmented-rnns/\" rel=\"noopener noreferrer\">Attention and Augmented Recurrent Neural Networks</a> - Interesting for visual animations, it is a nice intro to attention mechanisms as an example.</li>\n</ul>\n\n<ul>\n<li><a href=\"http://waitbutwhy.com/2017/04/neuralink.html\" rel=\"noopener noreferrer\">Neuralink and the Brain’s Magical Future</a> - Thought provoking article about the future of the brain and brain-computer interfaces.</li>\n</ul>\n<h3><p>Practical Resources / Librairies and Implementations</p>\n</h3>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow\" rel=\"noopener noreferrer\">TensorFlow's GitHub repository (⭐191k)</a> - Most known deep learning framework, both high-level and low-level while staying flexible.</li>\n</ul>\n\n<ul>\n<li><a href=\"https://keras.io/\" rel=\"noopener noreferrer\">Keras</a> - Keras is another intersting deep learning framework like TensorFlow, it is mostly high-level.</li>\n</ul>\n<h3><p>Papers / Recurrent Neural Networks</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1406.1078v3.pdf\" rel=\"noopener noreferrer\">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a> - Two networks in one combined into a seq2seq (sequence to sequence) Encoder-Decoder architecture. RNN Encoder–Decoder with 1000 hidden units. Adadelta optimizer.</li>\n</ul>\n<h3><p>Papers / Convolutional Neural Networks</p>\n</h3>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1409.1556v6.pdf\" rel=\"noopener noreferrer\">Very Deep Convolutional Networks for Large-Scale Image Recognition</a> - Interesting idea of stacking multiple 3x3 conv+ReLU before pooling for a bigger filter size with just a few parameters. There is also a nice table for \"ConvNet Configuration\".</li>\n</ul>\n"
    }
  ]
}
